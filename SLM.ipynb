{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "be0e94a1-6dd4-4b08-9afe-5a6e8fcd707f",
      "metadata": {
        "id": "be0e94a1-6dd4-4b08-9afe-5a6e8fcd707f"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9011c248-84f9-4f6a-9478-00ff3da57d18",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593,
          "referenced_widgets": [
            "318ba5803c444918843396e1d862efab",
            "88cb8ccd006b41f2ab7870e1af9af5bb",
            "8799db0d5af645a58a882c47a1725215",
            "c4d5e39c5c3d451596854417389bbac2",
            "8ae40a98e0ff44e49b0f21371eb301a7",
            "399b4054eb6b4b9498aacf354d61cb7a",
            "93b581441d7d4c00895584325a5d3fd3",
            "22327cc90b594961ac357f8da9c6cdbc",
            "4b0c8f874c164bfe88700f4c1555cd15",
            "a601d7045f3b4cedb62837c42977b4f3",
            "7ad6b7a787974d42961b38ec0b1447c3",
            "7145e038e782409d9bc115733ae5f558",
            "037bdbadaf80470ca8dab585edddeda3",
            "3e1e943df06b473d9d411cb47be03114",
            "24b854cabad44c17b79d1a1ceebb40f7",
            "cbcd78dc7d14461e9a6ce684c6cd21b2",
            "28bdba2546604fbeae77e80023640faf",
            "35e68646796346df9fe39c38076b1423",
            "b7554fcf983547c4ad8a5455302b80da",
            "8f24ae8009e94eb5afb5d78018f34fac",
            "a66f33f5c88d4db0b3e23580acf22225",
            "931383f0a1524a4a88fe9e9907137ee2",
            "40be7e8bd798497589d381172eb513b3",
            "950bb92ba2ab49cebf2eb5c460ad3550",
            "923dd9d58615428b9ae9db9b38af1192",
            "f8cb4dfead8944f790c5bd7a44182520",
            "5a107eea203c4818b5fb2bb8f1ae8e21",
            "15b8df6033d949af8943679e2c2ab4ba",
            "fe61682d3b514c63a5d3bcccc97a43b5",
            "3dbaa12acf604d33856b11779e97ffc2",
            "2a8a840936704700a1896affb196dc38",
            "4458eb3d07344153903ea36fb5e82854",
            "b515488bdb384e088d8048f373e667ce",
            "d9f14023ff9a4199acdb387cc8eeb5c5",
            "f98b27b028b249a8b5a4e83f218dcf48",
            "f04d5cdf90e64a1db19f1b1b05136255",
            "6c4302357d124b4384f79ae242a2e037",
            "2f3ad4c440da4ca3a9ab15d2438c84d0",
            "110c6d262a85498caa800d315b62a33b",
            "7df4b6bdb9564efa9d8319850f7d267b",
            "e4f4b4a1be0a41958ce80ee0767c97a4",
            "84eab848d90a43fdb8e1ab8208306b89",
            "b1fd9db6e13c4edd9977069c6d30aa22",
            "092f62791e0e402aa93ac552708537ab",
            "f3dc342ce66e46748cf55224971e95bc",
            "e987ae48e6f04b79a22a13850dcca16d",
            "d4a94bbebcec40deb9b30836725190e4",
            "f49b1f154d74423291fb6da78d9b7e0f",
            "87945c400a7d41979956d2c8c495e10a",
            "5a223550ac7949418f0d556bde6d5772",
            "3c77fce10db941b7bcd64699ce046e11",
            "5c1e60c7fc46431cada0c786933b943d",
            "9f8ffc7745a4431fa69738172aad0a9e",
            "a2d660cfaedb45a6b7903a61eaa68580",
            "4937e0744c1b4217a5c3388845eab12d",
            "f2078d447256411e909b97ea66a5e09d",
            "c0f76ab7173e4ca7a4e1eebf5a9ae94d",
            "cc877444a1ab425595bbc32ff1540ed6",
            "9369826ab1bf4ad69d5e04a8f2efbf97",
            "4d3386173c0c49f59a49b33b2897673e",
            "32c59e41b88c4b1091cb938f9fbf848d",
            "a97fcb0c621449bf922be517c0f9772e",
            "d73971b7b14b45749e7b4b348b1d2784",
            "708f5efb83b84844912d786e951f7ebf",
            "e64b0f0846f445be9badef9a257ea1b1",
            "8b1802bf76e345869e5839a0ff1f70e9",
            "3edc1b90025e4a13880ffd5ee47be4e8",
            "ac9f6cda36fd41939d461ed56a43d028",
            "dc388d2f08a3440aa824ba4760e9960d",
            "b264e006fd06424a9431cded7877e6a1",
            "2024baa94a32472aa05ed75523c9bbcc",
            "cc35ed92cff4492a837e75b7bbafcae1",
            "36720ace92df4e86b24f3e8323238f64",
            "e1c0314eb7944b4696efb82b91f8709b",
            "4786a106ea2d432087d14ac2b5b8598a",
            "391b880485864c7ea43807851f819448",
            "309c2cd947ba4265bd1001788c46fe66",
            "66fcc680014c450ea95e8d2ce848d095",
            "17b54df3f7914697bb093c4d7273e77a",
            "17685bef3f5b4493a6536083eed61326",
            "1e48ef3ccc14417d90bf31ef7c31ae4c",
            "9d99e83ba6e545d4922ef29e0edc443b",
            "1219acae00564da4b5b45f76cd8dfa6e",
            "7e242442e32b4ddc8413f6b6a8b41670",
            "37f2df9413ab48f6b0ac0f7c9143a2bc",
            "0457a80dd0774594a5caaa0390723976",
            "637ef2f4d6c6479a858e954bd8b9ea5d",
            "184fb1335c34477eab0ce3ab228c46b2"
          ]
        },
        "id": "9011c248-84f9-4f6a-9478-00ff3da57d18",
        "outputId": "f1332fdf-b58a-4435-c896-fb1eca6fd3ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "318ba5803c444918843396e1d862efab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00004-2d5a1467fff108(…):   0%|          | 0.00/249M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7145e038e782409d9bc115733ae5f558"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00001-of-00004-5852b56a2bd28f(…):   0%|          | 0.00/248M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40be7e8bd798497589d381172eb513b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00002-of-00004-a26307300439e9(…):   0%|          | 0.00/246M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9f14023ff9a4199acdb387cc8eeb5c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00003-of-00004-d243063613e5a0(…):   0%|          | 0.00/248M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3dc342ce66e46748cf55224971e95bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/validation-00000-of-00001-869c898b5(…):   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2078d447256411e909b97ea66a5e09d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3edc1b90025e4a13880ffd5ee47be4e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66fcc680014c450ea95e8d2ce848d095"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "ds = load_dataset(\"roneneldan/TinyStories\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "78072822-b865-4254-b645-094e60e31e5a",
      "metadata": {
        "id": "78072822-b865-4254-b645-094e60e31e5a"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def process(example):\n",
        "    ids = enc.encode_ordinary(example['text'])\n",
        "    out = {'ids' : ids, 'len' : len(ids)}\n",
        "\n",
        "    return out\n",
        "\n",
        "if not os.path.exists(\"train.bin\"):\n",
        "    tokenized = ds.map(\n",
        "        process,\n",
        "        remove_columns=['text'],\n",
        "        desc= \"tokenizing the splits\",\n",
        "        num_proc = 8\n",
        "    )\n",
        "    for split, dset in tokenized.items():\n",
        "        arr_len  = np.sum(dset['len'], dtype=np.uint64)\n",
        "        filename = f'{split}.bin'\n",
        "        dtype = np.uint16\n",
        "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
        "        total_batches = 1024\n",
        "\n",
        "        idx = 0\n",
        "\n",
        "        for batch_idx  in tqdm(range(total_batches) ,desc=f'writing {filename}'):\n",
        "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
        "            arr_batch = np.concatenate(batch['ids'])\n",
        "\n",
        "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
        "            idx += len(arr_batch)\n",
        "        arr.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fbc37050-7824-4e4c-aaa8-c6e44fbb2d78",
      "metadata": {
        "id": "fbc37050-7824-4e4c-aaa8-c6e44fbb2d78"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    if split == 'train':\n",
        "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
        "\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
        "    x = torch.stack([torch.from_numpy((data[i : i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1 : i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "\n",
        "    if device_type == 'cuda':\n",
        "        x,y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "\n",
        "    return x,y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1ffe56fe-f6de-4add-a34f-9d0856485047",
      "metadata": {
        "id": "1ffe56fe-f6de-4add-a34f-9d0856485047"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embed % config.n_head == 0\n",
        "\n",
        "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embed, config.n_embed, bias=config.bias)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embed = config.n_embed\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"bias\",\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "            .view(1, 1, config.block_size, config.block_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        q, k, v = self.c_attn(x).split(self.n_embed, dim=2)\n",
        "\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(config.n_embed, 4 * config.n_embed, bias=config.bias)\n",
        "        self.proj = nn.Linear(4 * config.n_embed, config.n_embed, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.gelu(self.fc(x))\n",
        "        x = self.proj(x)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config.n_embed, config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = LayerNorm(config.n_embed, config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int\n",
        "    vocab_size: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_embed: int\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(config.vocab_size, config.n_embed),\n",
        "            wpe=nn.Embedding(config.block_size, config.n_embed),\n",
        "            drop=nn.Dropout(config.dropout),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f=LayerNorm(config.n_embed, config.bias),\n",
        "        ))\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size\n",
        "\n",
        "        pos = torch.arange(0, T, device=idx.device)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                targets.view(-1),\n",
        "                ignore_index=-1\n",
        "            )\n",
        "            return logits, loss\n",
        "\n",
        "        return logits, None\n",
        "\n",
        "    @torch.no_grad()\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0,top_k=None):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits , _  = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                v,_ = torch.topk(logits, min(top_k,logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = float(\"-Inf\")\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs,num_samples=1)\n",
        "            idx = torch.cat((idx,idx_next),dim=1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3564bdb4-d6c8-48d5-abe4-9e059e34ed3e",
      "metadata": {
        "id": "3564bdb4-d6c8-48d5-abe4-9e059e34ed3e"
      },
      "outputs": [],
      "source": [
        "config = GPTConfig(\n",
        "    block_size=64,\n",
        "    vocab_size=50257,\n",
        "    n_layer=4,\n",
        "    n_head=4,\n",
        "    n_embed=256,\n",
        "    dropout=0.1,\n",
        "    bias=True\n",
        ")\n",
        "model = GPT(config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "\n",
        "    for split in ['train', 'val']:\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for _ in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "            with ctx:\n",
        "                _, loss = model(X, Y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        out[split] = total_loss / eval_iters\n",
        "\n",
        "    model.train()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "WcSY1e8X0rFX"
      },
      "id": "WcSY1e8X0rFX",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "50184caf-31a8-4338-b244-0f26a023665b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50184caf-31a8-4338-b244-0f26a023665b",
        "outputId": "a401b991-8719-40d3-cc6a-1fd2b322b2bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b2589f3b6b0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "learning_rate = 1e-4\n",
        "max_iters = 20000\n",
        "warmup_steps = 1000\n",
        "min_lr = 5e-5\n",
        "batch_size = 8\n",
        "block_size = 64\n",
        "\n",
        "gradient_accumulation_steps = 8\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_type = 'cuda' if device.type == 'cuda' else 'cpu'\n",
        "\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "ptdtype = torch.bfloat16 if dtype == 'bfloat16' else torch.float16\n",
        "\n",
        "\n",
        "ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "torch.set_default_device(device)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e5622bc4-dbdd-403b-bada-e244513c354b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5622bc4-dbdd-403b-bada-e244513c354b",
        "outputId": "83f6a28b-a1ce-4f2a-f579-cd6f173cdab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4099649570.py:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device_type == 'cuda'))\n"
          ]
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    betas=(0.9, 0.95),\n",
        "    weight_decay=0.1\n",
        ")\n",
        "\n",
        "scheduler_warmup = LinearLR(\n",
        "    optimizer,\n",
        "    start_factor=1e-3,\n",
        "    end_factor=1.0,\n",
        "    total_iters=warmup_steps\n",
        ")\n",
        "\n",
        "scheduler_decay = CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=max_iters - warmup_steps,\n",
        "    eta_min=min_lr\n",
        ")\n",
        "\n",
        "scheduler = SequentialLR(\n",
        "    optimizer,\n",
        "    schedulers=[scheduler_warmup, scheduler_decay],\n",
        "    milestones=[warmup_steps]\n",
        ")\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device_type == 'cuda'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d0af0ded-485f-4dcd-9102-b053e8f99065",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b4f6d71d7dd246bca78dd95d6981c278",
            "2c6b3a1d7a814a46a73bcc4996eb0590",
            "dd92dbfd62404c5287f6b0cef4b5ac04",
            "84054ee4a98641bda1587923578712d0",
            "69debe46d10b4b8c9b778269c3515a9b",
            "7f6f930590794f58932eaeb63d08f3f2",
            "5926a0fd9ace4f3e923cdabfdddf1110",
            "564661d5bf684c979e7fe379253c8c95",
            "229fba1167714102b4c59262c1a2d55e",
            "59247793cfec4d88971c594f75cc2181",
            "41caf9f628de4fefb03f9c53ab4b472f"
          ]
        },
        "collapsed": true,
        "id": "d0af0ded-485f-4dcd-9102-b053e8f99065",
        "outputId": "454967bb-5c5d-422f-edcb-6834a09640cc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4f6d71d7dd246bca78dd95d6981c278"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 17500 train loss 4.6255 val loss 5.2648\n",
            "The current learning rate 0.00005\n",
            "Epoch 17501 train loss 4.9465 val loss 4.7594\n",
            "The current learning rate 0.00005\n",
            "Epoch 17502 train loss 4.4738 val loss 4.7983\n",
            "The current learning rate 0.00005\n",
            "Epoch 17503 train loss 4.5687 val loss 4.8175\n",
            "The current learning rate 0.00005\n",
            "Epoch 17504 train loss 4.5459 val loss 4.6596\n",
            "The current learning rate 0.00005\n",
            "Epoch 17505 train loss 4.6735 val loss 4.6026\n",
            "The current learning rate 0.00005\n",
            "Epoch 17506 train loss 4.6483 val loss 4.4318\n",
            "The current learning rate 0.00005\n",
            "Epoch 17507 train loss 4.5145 val loss 4.4557\n",
            "The current learning rate 0.00005\n",
            "Epoch 17508 train loss 4.8965 val loss 4.6392\n",
            "The current learning rate 0.00005\n",
            "Epoch 17509 train loss 4.4942 val loss 4.5186\n",
            "The current learning rate 0.00005\n",
            "Epoch 17510 train loss 4.8240 val loss 4.3409\n",
            "The current learning rate 0.00005\n",
            "Epoch 17511 train loss 4.2234 val loss 4.5663\n",
            "The current learning rate 0.00005\n",
            "Epoch 17512 train loss 4.4145 val loss 4.6142\n",
            "The current learning rate 0.00005\n",
            "Epoch 17513 train loss 4.7074 val loss 4.5074\n",
            "The current learning rate 0.00005\n",
            "Epoch 17514 train loss 4.9719 val loss 4.2853\n",
            "The current learning rate 0.00005\n",
            "Epoch 17515 train loss 4.8175 val loss 4.5258\n",
            "The current learning rate 0.00005\n",
            "Epoch 17516 train loss 4.3381 val loss 4.7560\n",
            "The current learning rate 0.00005\n",
            "Epoch 17517 train loss 4.5915 val loss 4.8333\n",
            "The current learning rate 0.00005\n",
            "Epoch 17518 train loss 4.9212 val loss 4.5188\n",
            "The current learning rate 0.00005\n",
            "Epoch 17519 train loss 4.5695 val loss 5.0736\n",
            "The current learning rate 0.00005\n",
            "Epoch 17520 train loss 4.3487 val loss 4.3461\n",
            "The current learning rate 0.00005\n",
            "Epoch 17521 train loss 4.9743 val loss 4.7781\n",
            "The current learning rate 0.00005\n",
            "Epoch 17522 train loss 4.4538 val loss 5.0448\n",
            "The current learning rate 0.00005\n",
            "Epoch 17523 train loss 4.5026 val loss 4.4212\n",
            "The current learning rate 0.00005\n",
            "Epoch 17524 train loss 4.7226 val loss 4.8206\n",
            "The current learning rate 0.00005\n",
            "Epoch 17525 train loss 4.7423 val loss 5.0304\n",
            "The current learning rate 0.00005\n",
            "Epoch 17526 train loss 4.7789 val loss 4.6344\n",
            "The current learning rate 0.00005\n",
            "Epoch 17527 train loss 4.3429 val loss 4.5039\n",
            "The current learning rate 0.00005\n",
            "Epoch 17528 train loss 4.6775 val loss 4.5939\n",
            "The current learning rate 0.00005\n",
            "Epoch 17529 train loss 4.4291 val loss 4.3333\n",
            "The current learning rate 0.00005\n",
            "Epoch 17530 train loss 4.8579 val loss 4.8034\n",
            "The current learning rate 0.00005\n",
            "Epoch 17531 train loss 4.8430 val loss 4.6623\n",
            "The current learning rate 0.00005\n",
            "Epoch 17532 train loss 4.5568 val loss 4.7016\n",
            "The current learning rate 0.00005\n",
            "Epoch 17533 train loss 4.6781 val loss 4.7331\n",
            "The current learning rate 0.00005\n",
            "Epoch 17534 train loss 4.5109 val loss 4.6995\n",
            "The current learning rate 0.00005\n",
            "Epoch 17535 train loss 4.9354 val loss 4.4573\n",
            "The current learning rate 0.00005\n",
            "Epoch 17536 train loss 4.5595 val loss 4.8358\n",
            "The current learning rate 0.00005\n",
            "Epoch 17537 train loss 4.6898 val loss 4.4661\n",
            "The current learning rate 0.00005\n",
            "Epoch 17538 train loss 4.8042 val loss 4.4837\n",
            "The current learning rate 0.00005\n",
            "Epoch 17539 train loss 4.6833 val loss 4.5819\n",
            "The current learning rate 0.00005\n",
            "Epoch 17540 train loss 4.7949 val loss 4.8565\n",
            "The current learning rate 0.00005\n",
            "Epoch 17541 train loss 4.8771 val loss 4.7458\n",
            "The current learning rate 0.00005\n",
            "Epoch 17542 train loss 4.4125 val loss 4.4665\n",
            "The current learning rate 0.00005\n",
            "Epoch 17543 train loss 4.2414 val loss 5.0123\n",
            "The current learning rate 0.00005\n",
            "Epoch 17544 train loss 4.3569 val loss 4.7819\n",
            "The current learning rate 0.00005\n",
            "Epoch 17545 train loss 4.5570 val loss 4.8435\n",
            "The current learning rate 0.00005\n",
            "Epoch 17546 train loss 4.8759 val loss 4.7851\n",
            "The current learning rate 0.00005\n",
            "Epoch 17547 train loss 4.9059 val loss 4.7804\n",
            "The current learning rate 0.00005\n",
            "Epoch 17548 train loss 4.4489 val loss 4.6756\n",
            "The current learning rate 0.00005\n",
            "Epoch 17549 train loss 4.7679 val loss 4.6995\n",
            "The current learning rate 0.00005\n",
            "Epoch 17550 train loss 4.6947 val loss 4.7825\n",
            "The current learning rate 0.00005\n",
            "Epoch 17551 train loss 4.4748 val loss 4.6437\n",
            "The current learning rate 0.00005\n",
            "Epoch 17552 train loss 4.9217 val loss 4.7259\n",
            "The current learning rate 0.00005\n",
            "Epoch 17553 train loss 4.6378 val loss 4.5798\n",
            "The current learning rate 0.00005\n",
            "Epoch 17554 train loss 4.4493 val loss 5.0390\n",
            "The current learning rate 0.00005\n",
            "Epoch 17555 train loss 4.6144 val loss 4.2681\n",
            "The current learning rate 0.00005\n",
            "Epoch 17556 train loss 4.8741 val loss 4.7804\n",
            "The current learning rate 0.00005\n",
            "Epoch 17557 train loss 4.8783 val loss 4.3774\n",
            "The current learning rate 0.00005\n",
            "Epoch 17558 train loss 4.7442 val loss 4.7018\n",
            "The current learning rate 0.00005\n",
            "Epoch 17559 train loss 4.7245 val loss 4.6977\n",
            "The current learning rate 0.00005\n",
            "Epoch 17560 train loss 4.7498 val loss 4.6853\n",
            "The current learning rate 0.00005\n",
            "Epoch 17561 train loss 4.9201 val loss 4.6640\n",
            "The current learning rate 0.00005\n",
            "Epoch 17562 train loss 4.5274 val loss 4.4878\n",
            "The current learning rate 0.00005\n",
            "Epoch 17563 train loss 4.5417 val loss 4.9046\n",
            "The current learning rate 0.00005\n",
            "Epoch 17564 train loss 4.3869 val loss 4.3821\n",
            "The current learning rate 0.00005\n",
            "Epoch 17565 train loss 4.3022 val loss 4.5357\n",
            "The current learning rate 0.00005\n",
            "Epoch 17566 train loss 4.6684 val loss 4.6264\n",
            "The current learning rate 0.00005\n",
            "Epoch 17567 train loss 4.6586 val loss 4.7359\n",
            "The current learning rate 0.00005\n",
            "Epoch 17568 train loss 4.6039 val loss 4.7419\n",
            "The current learning rate 0.00005\n",
            "Epoch 17569 train loss 4.5634 val loss 4.6503\n",
            "The current learning rate 0.00005\n",
            "Epoch 17570 train loss 4.5178 val loss 5.1878\n",
            "The current learning rate 0.00005\n",
            "Epoch 17571 train loss 4.3992 val loss 4.3732\n",
            "The current learning rate 0.00005\n",
            "Epoch 17572 train loss 4.8424 val loss 4.1594\n",
            "The current learning rate 0.00005\n",
            "Epoch 17573 train loss 4.8062 val loss 4.6146\n",
            "The current learning rate 0.00005\n",
            "Epoch 17574 train loss 4.6070 val loss 4.5795\n",
            "The current learning rate 0.00005\n",
            "Epoch 17575 train loss 4.5172 val loss 4.7634\n",
            "The current learning rate 0.00005\n",
            "Epoch 17576 train loss 4.9601 val loss 4.7384\n",
            "The current learning rate 0.00005\n",
            "Epoch 17577 train loss 4.5554 val loss 4.8269\n",
            "The current learning rate 0.00005\n",
            "Epoch 17578 train loss 4.7415 val loss 4.5351\n",
            "The current learning rate 0.00005\n",
            "Epoch 17579 train loss 4.4601 val loss 4.5162\n",
            "The current learning rate 0.00005\n",
            "Epoch 17580 train loss 4.4720 val loss 4.8776\n",
            "The current learning rate 0.00005\n",
            "Epoch 17581 train loss 4.4778 val loss 4.5798\n",
            "The current learning rate 0.00005\n",
            "Epoch 17582 train loss 4.9291 val loss 4.6086\n",
            "The current learning rate 0.00005\n",
            "Epoch 17583 train loss 4.8799 val loss 5.0886\n",
            "The current learning rate 0.00005\n",
            "Epoch 17584 train loss 4.7119 val loss 4.4955\n",
            "The current learning rate 0.00005\n",
            "Epoch 17585 train loss 4.5195 val loss 5.0056\n",
            "The current learning rate 0.00005\n",
            "Epoch 17586 train loss 4.7364 val loss 4.7482\n",
            "The current learning rate 0.00005\n",
            "Epoch 17587 train loss 4.4178 val loss 4.8591\n",
            "The current learning rate 0.00005\n",
            "Epoch 17588 train loss 4.3641 val loss 4.3649\n",
            "The current learning rate 0.00005\n",
            "Epoch 17589 train loss 4.7516 val loss 4.7693\n",
            "The current learning rate 0.00005\n",
            "Epoch 17590 train loss 4.4537 val loss 4.5148\n",
            "The current learning rate 0.00005\n",
            "Epoch 17591 train loss 4.7157 val loss 4.2346\n",
            "The current learning rate 0.00005\n",
            "Epoch 17592 train loss 4.4967 val loss 4.9728\n",
            "The current learning rate 0.00005\n",
            "Epoch 17593 train loss 4.7914 val loss 4.7170\n",
            "The current learning rate 0.00005\n",
            "Epoch 17594 train loss 4.8515 val loss 4.6716\n",
            "The current learning rate 0.00005\n",
            "Epoch 17595 train loss 4.9480 val loss 5.0584\n",
            "The current learning rate 0.00005\n",
            "Epoch 17596 train loss 4.5156 val loss 4.4884\n",
            "The current learning rate 0.00005\n",
            "Epoch 17597 train loss 4.7460 val loss 4.3912\n",
            "The current learning rate 0.00005\n",
            "Epoch 17598 train loss 4.7574 val loss 4.6670\n",
            "The current learning rate 0.00005\n",
            "Epoch 17599 train loss 4.8162 val loss 4.6612\n",
            "The current learning rate 0.00005\n",
            "Epoch 17600 train loss 4.7463 val loss 4.4151\n",
            "The current learning rate 0.00005\n",
            "Epoch 17601 train loss 4.3014 val loss 4.5877\n",
            "The current learning rate 0.00005\n",
            "Epoch 17602 train loss 4.6499 val loss 4.7320\n",
            "The current learning rate 0.00005\n",
            "Epoch 17603 train loss 4.8914 val loss 4.9780\n",
            "The current learning rate 0.00005\n",
            "Epoch 17604 train loss 4.6335 val loss 4.8466\n",
            "The current learning rate 0.00005\n",
            "Epoch 17605 train loss 4.2222 val loss 4.9150\n",
            "The current learning rate 0.00005\n",
            "Epoch 17606 train loss 4.8839 val loss 4.6956\n",
            "The current learning rate 0.00005\n",
            "Epoch 17607 train loss 4.5773 val loss 4.6912\n",
            "The current learning rate 0.00005\n",
            "Epoch 17608 train loss 4.7128 val loss 4.9110\n",
            "The current learning rate 0.00005\n",
            "Epoch 17609 train loss 5.1032 val loss 4.2676\n",
            "The current learning rate 0.00005\n",
            "Epoch 17610 train loss 5.0447 val loss 4.7413\n",
            "The current learning rate 0.00005\n",
            "Epoch 17611 train loss 4.4359 val loss 4.6496\n",
            "The current learning rate 0.00005\n",
            "Epoch 17612 train loss 4.4356 val loss 4.9467\n",
            "The current learning rate 0.00005\n",
            "Epoch 17613 train loss 5.1102 val loss 4.3105\n",
            "The current learning rate 0.00005\n",
            "Epoch 17614 train loss 4.6844 val loss 5.0239\n",
            "The current learning rate 0.00005\n",
            "Epoch 17615 train loss 4.8672 val loss 4.8078\n",
            "The current learning rate 0.00005\n",
            "Epoch 17616 train loss 4.6048 val loss 4.3686\n",
            "The current learning rate 0.00005\n",
            "Epoch 17617 train loss 4.9879 val loss 4.6818\n",
            "The current learning rate 0.00005\n",
            "Epoch 17618 train loss 5.0732 val loss 4.3745\n",
            "The current learning rate 0.00005\n",
            "Epoch 17619 train loss 4.6392 val loss 4.4018\n",
            "The current learning rate 0.00005\n",
            "Epoch 17620 train loss 4.3113 val loss 4.7177\n",
            "The current learning rate 0.00005\n",
            "Epoch 17621 train loss 4.7411 val loss 4.4011\n",
            "The current learning rate 0.00005\n",
            "Epoch 17622 train loss 4.8349 val loss 4.7871\n",
            "The current learning rate 0.00005\n",
            "Epoch 17623 train loss 4.7298 val loss 4.4764\n",
            "The current learning rate 0.00005\n",
            "Epoch 17624 train loss 4.6616 val loss 4.5513\n",
            "The current learning rate 0.00005\n",
            "Epoch 17625 train loss 4.9122 val loss 4.7089\n",
            "The current learning rate 0.00005\n",
            "Epoch 17626 train loss 4.7382 val loss 4.6978\n",
            "The current learning rate 0.00005\n",
            "Epoch 17627 train loss 4.4468 val loss 4.5687\n",
            "The current learning rate 0.00005\n",
            "Epoch 17628 train loss 4.3732 val loss 4.4180\n",
            "The current learning rate 0.00005\n",
            "Epoch 17629 train loss 4.7110 val loss 4.6233\n",
            "The current learning rate 0.00005\n",
            "Epoch 17630 train loss 4.4945 val loss 4.5347\n",
            "The current learning rate 0.00005\n",
            "Epoch 17631 train loss 4.8063 val loss 4.3740\n",
            "The current learning rate 0.00005\n",
            "Epoch 17632 train loss 5.0487 val loss 4.6486\n",
            "The current learning rate 0.00005\n",
            "Epoch 17633 train loss 5.0676 val loss 4.9445\n",
            "The current learning rate 0.00005\n",
            "Epoch 17634 train loss 4.6937 val loss 4.5870\n",
            "The current learning rate 0.00005\n",
            "Epoch 17635 train loss 4.7938 val loss 4.4744\n",
            "The current learning rate 0.00005\n",
            "Epoch 17636 train loss 4.5293 val loss 4.8156\n",
            "The current learning rate 0.00005\n",
            "Epoch 17637 train loss 4.5761 val loss 4.9508\n",
            "The current learning rate 0.00005\n",
            "Epoch 17638 train loss 4.9063 val loss 4.5891\n",
            "The current learning rate 0.00005\n",
            "Epoch 17639 train loss 4.6164 val loss 4.8332\n",
            "The current learning rate 0.00005\n",
            "Epoch 17640 train loss 4.5445 val loss 4.5793\n",
            "The current learning rate 0.00005\n",
            "Epoch 17641 train loss 4.7165 val loss 4.8405\n",
            "The current learning rate 0.00005\n",
            "Epoch 17642 train loss 4.7646 val loss 4.9760\n",
            "The current learning rate 0.00005\n",
            "Epoch 17643 train loss 4.6433 val loss 4.5012\n",
            "The current learning rate 0.00005\n",
            "Epoch 17644 train loss 5.1024 val loss 4.5878\n",
            "The current learning rate 0.00005\n",
            "Epoch 17645 train loss 4.9101 val loss 4.8022\n",
            "The current learning rate 0.00005\n",
            "Epoch 17646 train loss 4.7765 val loss 4.7917\n",
            "The current learning rate 0.00005\n",
            "Epoch 17647 train loss 4.5170 val loss 5.0426\n",
            "The current learning rate 0.00005\n",
            "Epoch 17648 train loss 4.6487 val loss 4.7838\n",
            "The current learning rate 0.00005\n",
            "Epoch 17649 train loss 4.6195 val loss 4.7113\n",
            "The current learning rate 0.00005\n",
            "Epoch 17650 train loss 5.1268 val loss 4.4939\n",
            "The current learning rate 0.00005\n",
            "Epoch 17651 train loss 4.5142 val loss 4.5564\n",
            "The current learning rate 0.00005\n",
            "Epoch 17652 train loss 4.8029 val loss 4.7965\n",
            "The current learning rate 0.00005\n",
            "Epoch 17653 train loss 4.7741 val loss 4.4717\n",
            "The current learning rate 0.00005\n",
            "Epoch 17654 train loss 5.1666 val loss 4.3935\n",
            "The current learning rate 0.00005\n",
            "Epoch 17655 train loss 5.0406 val loss 4.1414\n",
            "The current learning rate 0.00005\n",
            "Epoch 17656 train loss 4.5846 val loss 4.3663\n",
            "The current learning rate 0.00005\n",
            "Epoch 17657 train loss 4.6998 val loss 4.6468\n",
            "The current learning rate 0.00005\n",
            "Epoch 17658 train loss 4.4281 val loss 4.3094\n",
            "The current learning rate 0.00005\n",
            "Epoch 17659 train loss 4.6883 val loss 4.6389\n",
            "The current learning rate 0.00005\n",
            "Epoch 17660 train loss 4.7898 val loss 4.7530\n",
            "The current learning rate 0.00005\n",
            "Epoch 17661 train loss 4.7261 val loss 4.7575\n",
            "The current learning rate 0.00005\n",
            "Epoch 17662 train loss 4.6918 val loss 4.9166\n",
            "The current learning rate 0.00005\n",
            "Epoch 17663 train loss 4.8095 val loss 4.7315\n",
            "The current learning rate 0.00005\n",
            "Epoch 17664 train loss 4.8002 val loss 4.7195\n",
            "The current learning rate 0.00005\n",
            "Epoch 17665 train loss 4.7359 val loss 4.7454\n",
            "The current learning rate 0.00005\n",
            "Epoch 17666 train loss 4.5982 val loss 4.4868\n",
            "The current learning rate 0.00005\n",
            "Epoch 17667 train loss 4.7111 val loss 5.0986\n",
            "The current learning rate 0.00005\n",
            "Epoch 17668 train loss 4.9397 val loss 4.5081\n",
            "The current learning rate 0.00005\n",
            "Epoch 17669 train loss 4.3089 val loss 4.9429\n",
            "The current learning rate 0.00005\n",
            "Epoch 17670 train loss 4.6466 val loss 4.8927\n",
            "The current learning rate 0.00005\n",
            "Epoch 17671 train loss 4.5692 val loss 4.4794\n",
            "The current learning rate 0.00005\n",
            "Epoch 17672 train loss 4.1282 val loss 4.4744\n",
            "The current learning rate 0.00005\n",
            "Epoch 17673 train loss 4.3613 val loss 4.3747\n",
            "The current learning rate 0.00005\n",
            "Epoch 17674 train loss 4.5605 val loss 4.4831\n",
            "The current learning rate 0.00005\n",
            "Epoch 17675 train loss 4.7706 val loss 4.7670\n",
            "The current learning rate 0.00005\n",
            "Epoch 17676 train loss 4.4286 val loss 5.1029\n",
            "The current learning rate 0.00005\n",
            "Epoch 17677 train loss 5.0260 val loss 4.3544\n",
            "The current learning rate 0.00005\n",
            "Epoch 17678 train loss 4.6688 val loss 4.7994\n",
            "The current learning rate 0.00005\n",
            "Epoch 17679 train loss 4.4846 val loss 4.4476\n",
            "The current learning rate 0.00005\n",
            "Epoch 17680 train loss 4.7330 val loss 4.8430\n",
            "The current learning rate 0.00005\n",
            "Epoch 17681 train loss 4.7553 val loss 4.4513\n",
            "The current learning rate 0.00005\n",
            "Epoch 17682 train loss 4.6883 val loss 4.7704\n",
            "The current learning rate 0.00005\n",
            "Epoch 17683 train loss 4.6319 val loss 4.5854\n",
            "The current learning rate 0.00005\n",
            "Epoch 17684 train loss 4.6515 val loss 4.6156\n",
            "The current learning rate 0.00005\n",
            "Epoch 17685 train loss 4.7268 val loss 4.6032\n",
            "The current learning rate 0.00005\n",
            "Epoch 17686 train loss 4.4033 val loss 4.6176\n",
            "The current learning rate 0.00005\n",
            "Epoch 17687 train loss 4.5642 val loss 4.5944\n",
            "The current learning rate 0.00005\n",
            "Epoch 17688 train loss 4.4821 val loss 5.0104\n",
            "The current learning rate 0.00005\n",
            "Epoch 17689 train loss 4.5309 val loss 4.6083\n",
            "The current learning rate 0.00005\n",
            "Epoch 17690 train loss 4.6447 val loss 4.5518\n",
            "The current learning rate 0.00005\n",
            "Epoch 17691 train loss 4.7605 val loss 4.8042\n",
            "The current learning rate 0.00005\n",
            "Epoch 17692 train loss 4.7286 val loss 5.1260\n",
            "The current learning rate 0.00005\n",
            "Epoch 17693 train loss 4.6679 val loss 4.9638\n",
            "The current learning rate 0.00005\n",
            "Epoch 17694 train loss 4.7681 val loss 4.2879\n",
            "The current learning rate 0.00005\n",
            "Epoch 17695 train loss 4.9584 val loss 4.6157\n",
            "The current learning rate 0.00005\n",
            "Epoch 17696 train loss 4.8636 val loss 4.5287\n",
            "The current learning rate 0.00005\n",
            "Epoch 17697 train loss 4.7768 val loss 4.2348\n",
            "The current learning rate 0.00005\n",
            "Epoch 17698 train loss 4.2435 val loss 4.8136\n",
            "The current learning rate 0.00005\n",
            "Epoch 17699 train loss 4.5803 val loss 4.6046\n",
            "The current learning rate 0.00005\n",
            "Epoch 17700 train loss 4.5193 val loss 4.6444\n",
            "The current learning rate 0.00005\n",
            "Epoch 17701 train loss 4.6978 val loss 4.8021\n",
            "The current learning rate 0.00005\n",
            "Epoch 17702 train loss 4.4759 val loss 4.9924\n",
            "The current learning rate 0.00005\n",
            "Epoch 17703 train loss 4.4263 val loss 4.7877\n",
            "The current learning rate 0.00005\n",
            "Epoch 17704 train loss 4.7679 val loss 4.7728\n",
            "The current learning rate 0.00005\n",
            "Epoch 17705 train loss 4.5432 val loss 4.4954\n",
            "The current learning rate 0.00005\n",
            "Epoch 17706 train loss 4.8579 val loss 4.3858\n",
            "The current learning rate 0.00005\n",
            "Epoch 17707 train loss 4.4435 val loss 4.6491\n",
            "The current learning rate 0.00005\n",
            "Epoch 17708 train loss 4.4928 val loss 4.5410\n",
            "The current learning rate 0.00005\n",
            "Epoch 17709 train loss 4.4358 val loss 4.8874\n",
            "The current learning rate 0.00005\n",
            "Epoch 17710 train loss 4.4342 val loss 4.3580\n",
            "The current learning rate 0.00005\n",
            "Epoch 17711 train loss 4.7587 val loss 4.6814\n",
            "The current learning rate 0.00005\n",
            "Epoch 17712 train loss 4.4716 val loss 4.4817\n",
            "The current learning rate 0.00005\n",
            "Epoch 17713 train loss 4.8969 val loss 4.8861\n",
            "The current learning rate 0.00005\n",
            "Epoch 17714 train loss 4.7260 val loss 4.8551\n",
            "The current learning rate 0.00005\n",
            "Epoch 17715 train loss 4.4613 val loss 4.9244\n",
            "The current learning rate 0.00005\n",
            "Epoch 17716 train loss 4.6906 val loss 4.6636\n",
            "The current learning rate 0.00005\n",
            "Epoch 17717 train loss 4.2447 val loss 4.2204\n",
            "The current learning rate 0.00005\n",
            "Epoch 17718 train loss 4.6244 val loss 4.6912\n",
            "The current learning rate 0.00005\n",
            "Epoch 17719 train loss 4.4664 val loss 4.2881\n",
            "The current learning rate 0.00005\n",
            "Epoch 17720 train loss 4.7635 val loss 4.9495\n",
            "The current learning rate 0.00005\n",
            "Epoch 17721 train loss 4.7490 val loss 4.6934\n",
            "The current learning rate 0.00005\n",
            "Epoch 17722 train loss 4.7591 val loss 4.9427\n",
            "The current learning rate 0.00005\n",
            "Epoch 17723 train loss 4.6641 val loss 4.4023\n",
            "The current learning rate 0.00005\n",
            "Epoch 17724 train loss 4.5160 val loss 4.6817\n",
            "The current learning rate 0.00005\n",
            "Epoch 17725 train loss 5.1373 val loss 5.0622\n",
            "The current learning rate 0.00005\n",
            "Epoch 17726 train loss 4.4962 val loss 4.6061\n",
            "The current learning rate 0.00005\n",
            "Epoch 17727 train loss 4.6935 val loss 4.8296\n",
            "The current learning rate 0.00005\n",
            "Epoch 17728 train loss 4.3832 val loss 4.8822\n",
            "The current learning rate 0.00005\n",
            "Epoch 17729 train loss 4.8005 val loss 4.9176\n",
            "The current learning rate 0.00005\n",
            "Epoch 17730 train loss 4.7335 val loss 4.9137\n",
            "The current learning rate 0.00005\n",
            "Epoch 17731 train loss 4.4981 val loss 4.8897\n",
            "The current learning rate 0.00005\n",
            "Epoch 17732 train loss 4.7437 val loss 4.2664\n",
            "The current learning rate 0.00005\n",
            "Epoch 17733 train loss 4.8227 val loss 4.5173\n",
            "The current learning rate 0.00005\n",
            "Epoch 17734 train loss 4.3508 val loss 4.1055\n",
            "The current learning rate 0.00005\n",
            "Epoch 17735 train loss 4.3732 val loss 5.1978\n",
            "The current learning rate 0.00005\n",
            "Epoch 17736 train loss 4.7328 val loss 4.7969\n",
            "The current learning rate 0.00005\n",
            "Epoch 17737 train loss 4.6258 val loss 4.6905\n",
            "The current learning rate 0.00005\n",
            "Epoch 17738 train loss 4.7478 val loss 4.4437\n",
            "The current learning rate 0.00005\n",
            "Epoch 17739 train loss 4.9444 val loss 4.8572\n",
            "The current learning rate 0.00005\n",
            "Epoch 17740 train loss 4.8037 val loss 4.8488\n",
            "The current learning rate 0.00005\n",
            "Epoch 17741 train loss 4.5995 val loss 4.4193\n",
            "The current learning rate 0.00005\n",
            "Epoch 17742 train loss 4.7202 val loss 4.4755\n",
            "The current learning rate 0.00005\n",
            "Epoch 17743 train loss 4.5448 val loss 4.5913\n",
            "The current learning rate 0.00005\n",
            "Epoch 17744 train loss 4.4118 val loss 4.3822\n",
            "The current learning rate 0.00005\n",
            "Epoch 17745 train loss 4.8026 val loss 4.7747\n",
            "The current learning rate 0.00005\n",
            "Epoch 17746 train loss 4.8317 val loss 4.5563\n",
            "The current learning rate 0.00005\n",
            "Epoch 17747 train loss 4.7689 val loss 4.5123\n",
            "The current learning rate 0.00005\n",
            "Epoch 17748 train loss 4.8486 val loss 4.5908\n",
            "The current learning rate 0.00005\n",
            "Epoch 17749 train loss 4.7239 val loss 4.6617\n",
            "The current learning rate 0.00005\n",
            "Epoch 17750 train loss 4.6387 val loss 4.5781\n",
            "The current learning rate 0.00005\n",
            "Epoch 17751 train loss 4.5533 val loss 5.0880\n",
            "The current learning rate 0.00005\n",
            "Epoch 17752 train loss 4.8137 val loss 4.5519\n",
            "The current learning rate 0.00005\n",
            "Epoch 17753 train loss 4.4166 val loss 4.4881\n",
            "The current learning rate 0.00005\n",
            "Epoch 17754 train loss 4.1869 val loss 4.6110\n",
            "The current learning rate 0.00005\n",
            "Epoch 17755 train loss 5.0482 val loss 4.6884\n",
            "The current learning rate 0.00005\n",
            "Epoch 17756 train loss 4.6297 val loss 4.6211\n",
            "The current learning rate 0.00005\n",
            "Epoch 17757 train loss 4.6406 val loss 4.9317\n",
            "The current learning rate 0.00005\n",
            "Epoch 17758 train loss 4.3439 val loss 4.7315\n",
            "The current learning rate 0.00005\n",
            "Epoch 17759 train loss 4.6854 val loss 4.5040\n",
            "The current learning rate 0.00005\n",
            "Epoch 17760 train loss 4.6594 val loss 5.0480\n",
            "The current learning rate 0.00005\n",
            "Epoch 17761 train loss 4.6636 val loss 4.3851\n",
            "The current learning rate 0.00005\n",
            "Epoch 17762 train loss 4.5599 val loss 4.6355\n",
            "The current learning rate 0.00005\n",
            "Epoch 17763 train loss 4.7720 val loss 5.0619\n",
            "The current learning rate 0.00005\n",
            "Epoch 17764 train loss 4.2372 val loss 4.5129\n",
            "The current learning rate 0.00005\n",
            "Epoch 17765 train loss 4.6847 val loss 4.5178\n",
            "The current learning rate 0.00005\n",
            "Epoch 17766 train loss 4.6089 val loss 5.2263\n",
            "The current learning rate 0.00005\n",
            "Epoch 17767 train loss 4.8354 val loss 4.6815\n",
            "The current learning rate 0.00005\n",
            "Epoch 17768 train loss 4.9746 val loss 4.9901\n",
            "The current learning rate 0.00005\n",
            "Epoch 17769 train loss 4.4507 val loss 4.6176\n",
            "The current learning rate 0.00005\n",
            "Epoch 17770 train loss 5.0121 val loss 4.6602\n",
            "The current learning rate 0.00005\n",
            "Epoch 17771 train loss 4.5805 val loss 4.4436\n",
            "The current learning rate 0.00005\n",
            "Epoch 17772 train loss 4.7667 val loss 4.9220\n",
            "The current learning rate 0.00005\n",
            "Epoch 17773 train loss 5.2930 val loss 4.5598\n",
            "The current learning rate 0.00005\n",
            "Epoch 17774 train loss 4.7758 val loss 4.5971\n",
            "The current learning rate 0.00005\n",
            "Epoch 17775 train loss 4.5503 val loss 4.5158\n",
            "The current learning rate 0.00005\n",
            "Epoch 17776 train loss 4.7004 val loss 4.5809\n",
            "The current learning rate 0.00005\n",
            "Epoch 17777 train loss 4.6615 val loss 4.8621\n",
            "The current learning rate 0.00005\n",
            "Epoch 17778 train loss 4.8027 val loss 4.5557\n",
            "The current learning rate 0.00005\n",
            "Epoch 17779 train loss 4.6033 val loss 4.8940\n",
            "The current learning rate 0.00005\n",
            "Epoch 17780 train loss 4.9065 val loss 4.3784\n",
            "The current learning rate 0.00005\n",
            "Epoch 17781 train loss 4.7131 val loss 4.4160\n",
            "The current learning rate 0.00005\n",
            "Epoch 17782 train loss 4.8340 val loss 4.6191\n",
            "The current learning rate 0.00005\n",
            "Epoch 17783 train loss 4.5513 val loss 4.8734\n",
            "The current learning rate 0.00005\n",
            "Epoch 17784 train loss 4.8336 val loss 4.7743\n",
            "The current learning rate 0.00005\n",
            "Epoch 17785 train loss 4.7999 val loss 4.7883\n",
            "The current learning rate 0.00005\n",
            "Epoch 17786 train loss 5.0052 val loss 4.7678\n",
            "The current learning rate 0.00005\n",
            "Epoch 17787 train loss 4.2695 val loss 4.5714\n",
            "The current learning rate 0.00005\n",
            "Epoch 17788 train loss 4.7878 val loss 5.0815\n",
            "The current learning rate 0.00005\n",
            "Epoch 17789 train loss 4.3568 val loss 4.8848\n",
            "The current learning rate 0.00005\n",
            "Epoch 17790 train loss 4.6277 val loss 4.5907\n",
            "The current learning rate 0.00005\n",
            "Epoch 17791 train loss 4.5602 val loss 4.7077\n",
            "The current learning rate 0.00005\n",
            "Epoch 17792 train loss 4.6439 val loss 4.7908\n",
            "The current learning rate 0.00005\n",
            "Epoch 17793 train loss 4.4423 val loss 4.9215\n",
            "The current learning rate 0.00005\n",
            "Epoch 17794 train loss 4.7265 val loss 4.6408\n",
            "The current learning rate 0.00005\n",
            "Epoch 17795 train loss 4.8115 val loss 4.4896\n",
            "The current learning rate 0.00005\n",
            "Epoch 17796 train loss 4.6175 val loss 4.5475\n",
            "The current learning rate 0.00005\n",
            "Epoch 17797 train loss 4.6478 val loss 4.7572\n",
            "The current learning rate 0.00005\n",
            "Epoch 17798 train loss 4.5700 val loss 4.8344\n",
            "The current learning rate 0.00005\n",
            "Epoch 17799 train loss 4.6376 val loss 4.7134\n",
            "The current learning rate 0.00005\n",
            "Epoch 17800 train loss 4.5494 val loss 5.0178\n",
            "The current learning rate 0.00005\n",
            "Epoch 17801 train loss 4.6988 val loss 4.7649\n",
            "The current learning rate 0.00005\n",
            "Epoch 17802 train loss 4.1651 val loss 4.6516\n",
            "The current learning rate 0.00005\n",
            "Epoch 17803 train loss 4.1987 val loss 4.7363\n",
            "The current learning rate 0.00005\n",
            "Epoch 17804 train loss 4.7170 val loss 4.4617\n",
            "The current learning rate 0.00005\n",
            "Epoch 17805 train loss 4.7697 val loss 4.8629\n",
            "The current learning rate 0.00005\n",
            "Epoch 17806 train loss 4.4256 val loss 4.8585\n",
            "The current learning rate 0.00005\n",
            "Epoch 17807 train loss 4.5495 val loss 4.7759\n",
            "The current learning rate 0.00005\n",
            "Epoch 17808 train loss 4.5029 val loss 4.6798\n",
            "The current learning rate 0.00005\n",
            "Epoch 17809 train loss 4.9335 val loss 5.0313\n",
            "The current learning rate 0.00005\n",
            "Epoch 17810 train loss 4.8208 val loss 4.7314\n",
            "The current learning rate 0.00005\n",
            "Epoch 17811 train loss 4.7377 val loss 4.6534\n",
            "The current learning rate 0.00005\n",
            "Epoch 17812 train loss 4.3247 val loss 4.5757\n",
            "The current learning rate 0.00005\n",
            "Epoch 17813 train loss 4.7629 val loss 4.8344\n",
            "The current learning rate 0.00005\n",
            "Epoch 17814 train loss 4.7756 val loss 4.6985\n",
            "The current learning rate 0.00005\n",
            "Epoch 17815 train loss 5.0635 val loss 4.5487\n",
            "The current learning rate 0.00005\n",
            "Epoch 17816 train loss 4.6798 val loss 4.3986\n",
            "The current learning rate 0.00005\n",
            "Epoch 17817 train loss 4.8579 val loss 4.6427\n",
            "The current learning rate 0.00005\n",
            "Epoch 17818 train loss 4.3983 val loss 4.7208\n",
            "The current learning rate 0.00005\n",
            "Epoch 17819 train loss 4.7946 val loss 4.8897\n",
            "The current learning rate 0.00005\n",
            "Epoch 17820 train loss 4.3714 val loss 4.5569\n",
            "The current learning rate 0.00005\n",
            "Epoch 17821 train loss 4.4897 val loss 4.5803\n",
            "The current learning rate 0.00005\n",
            "Epoch 17822 train loss 4.8702 val loss 4.6665\n",
            "The current learning rate 0.00005\n",
            "Epoch 17823 train loss 4.7462 val loss 4.8502\n",
            "The current learning rate 0.00005\n",
            "Epoch 17824 train loss 4.9614 val loss 4.9428\n",
            "The current learning rate 0.00005\n",
            "Epoch 17825 train loss 4.5345 val loss 4.6882\n",
            "The current learning rate 0.00005\n",
            "Epoch 17826 train loss 4.5850 val loss 5.0194\n",
            "The current learning rate 0.00005\n",
            "Epoch 17827 train loss 4.8392 val loss 4.4817\n",
            "The current learning rate 0.00005\n",
            "Epoch 17828 train loss 4.7799 val loss 4.8498\n",
            "The current learning rate 0.00005\n",
            "Epoch 17829 train loss 4.6737 val loss 4.9822\n",
            "The current learning rate 0.00005\n",
            "Epoch 17830 train loss 4.5355 val loss 4.4184\n",
            "The current learning rate 0.00005\n",
            "Epoch 17831 train loss 4.7118 val loss 4.6629\n",
            "The current learning rate 0.00005\n",
            "Epoch 17832 train loss 4.7825 val loss 4.6077\n",
            "The current learning rate 0.00005\n",
            "Epoch 17833 train loss 5.0347 val loss 4.4447\n",
            "The current learning rate 0.00005\n",
            "Epoch 17834 train loss 4.4354 val loss 4.7718\n",
            "The current learning rate 0.00005\n",
            "Epoch 17835 train loss 4.6196 val loss 5.0286\n",
            "The current learning rate 0.00005\n",
            "Epoch 17836 train loss 4.5356 val loss 5.3155\n",
            "The current learning rate 0.00005\n",
            "Epoch 17837 train loss 4.8764 val loss 4.4389\n",
            "The current learning rate 0.00005\n",
            "Epoch 17838 train loss 4.8448 val loss 4.4607\n",
            "The current learning rate 0.00005\n",
            "Epoch 17839 train loss 4.4659 val loss 4.3403\n",
            "The current learning rate 0.00005\n",
            "Epoch 17840 train loss 4.6602 val loss 4.6368\n",
            "The current learning rate 0.00005\n",
            "Epoch 17841 train loss 4.6548 val loss 4.7005\n",
            "The current learning rate 0.00005\n",
            "Epoch 17842 train loss 4.5150 val loss 4.3764\n",
            "The current learning rate 0.00005\n",
            "Epoch 17843 train loss 4.4896 val loss 4.4364\n",
            "The current learning rate 0.00005\n",
            "Epoch 17844 train loss 4.4547 val loss 4.5715\n",
            "The current learning rate 0.00005\n",
            "Epoch 17845 train loss 4.5834 val loss 4.3394\n",
            "The current learning rate 0.00005\n",
            "Epoch 17846 train loss 4.5973 val loss 4.7827\n",
            "The current learning rate 0.00005\n",
            "Epoch 17847 train loss 4.4997 val loss 4.6155\n",
            "The current learning rate 0.00005\n",
            "Epoch 17848 train loss 5.2169 val loss 4.5077\n",
            "The current learning rate 0.00005\n",
            "Epoch 17849 train loss 4.7036 val loss 4.5942\n",
            "The current learning rate 0.00005\n",
            "Epoch 17850 train loss 4.3990 val loss 4.7731\n",
            "The current learning rate 0.00005\n",
            "Epoch 17851 train loss 4.4578 val loss 4.7445\n",
            "The current learning rate 0.00005\n",
            "Epoch 17852 train loss 4.4046 val loss 4.6247\n",
            "The current learning rate 0.00005\n",
            "Epoch 17853 train loss 4.3156 val loss 4.8321\n",
            "The current learning rate 0.00005\n",
            "Epoch 17854 train loss 4.8962 val loss 4.4370\n",
            "The current learning rate 0.00005\n",
            "Epoch 17855 train loss 4.8358 val loss 4.8220\n",
            "The current learning rate 0.00005\n",
            "Epoch 17856 train loss 4.8492 val loss 4.7250\n",
            "The current learning rate 0.00005\n",
            "Epoch 17857 train loss 5.0657 val loss 4.5720\n",
            "The current learning rate 0.00005\n",
            "Epoch 17858 train loss 4.8098 val loss 4.4320\n",
            "The current learning rate 0.00005\n",
            "Epoch 17859 train loss 4.6074 val loss 4.7717\n",
            "The current learning rate 0.00005\n",
            "Epoch 17860 train loss 4.4071 val loss 4.9262\n",
            "The current learning rate 0.00005\n",
            "Epoch 17861 train loss 4.7007 val loss 4.9867\n",
            "The current learning rate 0.00005\n",
            "Epoch 17862 train loss 4.3168 val loss 4.5315\n",
            "The current learning rate 0.00005\n",
            "Epoch 17863 train loss 4.6274 val loss 4.9352\n",
            "The current learning rate 0.00005\n",
            "Epoch 17864 train loss 4.4331 val loss 4.7066\n",
            "The current learning rate 0.00005\n",
            "Epoch 17865 train loss 4.6038 val loss 4.5031\n",
            "The current learning rate 0.00005\n",
            "Epoch 17866 train loss 4.4511 val loss 4.4999\n",
            "The current learning rate 0.00005\n",
            "Epoch 17867 train loss 4.5665 val loss 4.6635\n",
            "The current learning rate 0.00005\n",
            "Epoch 17868 train loss 4.6806 val loss 4.5873\n",
            "The current learning rate 0.00005\n",
            "Epoch 17869 train loss 5.1520 val loss 4.5202\n",
            "The current learning rate 0.00005\n",
            "Epoch 17870 train loss 4.9600 val loss 4.6144\n",
            "The current learning rate 0.00005\n",
            "Epoch 17871 train loss 4.6873 val loss 4.5392\n",
            "The current learning rate 0.00005\n",
            "Epoch 17872 train loss 4.8388 val loss 4.6520\n",
            "The current learning rate 0.00005\n",
            "Epoch 17873 train loss 4.3852 val loss 4.8892\n",
            "The current learning rate 0.00005\n",
            "Epoch 17874 train loss 4.8029 val loss 4.7517\n",
            "The current learning rate 0.00005\n",
            "Epoch 17875 train loss 5.1421 val loss 5.0868\n",
            "The current learning rate 0.00005\n",
            "Epoch 17876 train loss 4.6497 val loss 4.7677\n",
            "The current learning rate 0.00005\n",
            "Epoch 17877 train loss 5.0390 val loss 4.7776\n",
            "The current learning rate 0.00005\n",
            "Epoch 17878 train loss 4.8342 val loss 4.4661\n",
            "The current learning rate 0.00005\n",
            "Epoch 17879 train loss 5.1511 val loss 4.5268\n",
            "The current learning rate 0.00005\n",
            "Epoch 17880 train loss 4.7427 val loss 5.1496\n",
            "The current learning rate 0.00005\n",
            "Epoch 17881 train loss 4.6906 val loss 4.7121\n",
            "The current learning rate 0.00005\n",
            "Epoch 17882 train loss 4.4264 val loss 4.4004\n",
            "The current learning rate 0.00005\n",
            "Epoch 17883 train loss 4.5377 val loss 4.7410\n",
            "The current learning rate 0.00005\n",
            "Epoch 17884 train loss 4.7297 val loss 4.3633\n",
            "The current learning rate 0.00005\n",
            "Epoch 17885 train loss 4.6210 val loss 4.7247\n",
            "The current learning rate 0.00005\n",
            "Epoch 17886 train loss 4.6335 val loss 4.8868\n",
            "The current learning rate 0.00005\n",
            "Epoch 17887 train loss 4.7339 val loss 5.5151\n",
            "The current learning rate 0.00005\n",
            "Epoch 17888 train loss 4.7591 val loss 5.0440\n",
            "The current learning rate 0.00005\n",
            "Epoch 17889 train loss 4.5161 val loss 4.2875\n",
            "The current learning rate 0.00005\n",
            "Epoch 17890 train loss 4.5401 val loss 4.4840\n",
            "The current learning rate 0.00005\n",
            "Epoch 17891 train loss 5.0609 val loss 4.5546\n",
            "The current learning rate 0.00005\n",
            "Epoch 17892 train loss 4.7588 val loss 4.6530\n",
            "The current learning rate 0.00005\n",
            "Epoch 17893 train loss 4.8595 val loss 4.7304\n",
            "The current learning rate 0.00005\n",
            "Epoch 17894 train loss 5.0019 val loss 4.7016\n",
            "The current learning rate 0.00005\n",
            "Epoch 17895 train loss 4.7093 val loss 4.5249\n",
            "The current learning rate 0.00005\n",
            "Epoch 17896 train loss 4.4444 val loss 4.7751\n",
            "The current learning rate 0.00005\n",
            "Epoch 17897 train loss 4.5621 val loss 4.8291\n",
            "The current learning rate 0.00005\n",
            "Epoch 17898 train loss 4.8459 val loss 4.9472\n",
            "The current learning rate 0.00005\n",
            "Epoch 17899 train loss 5.0355 val loss 4.4728\n",
            "The current learning rate 0.00005\n",
            "Epoch 17900 train loss 4.6614 val loss 4.6792\n",
            "The current learning rate 0.00005\n",
            "Epoch 17901 train loss 4.5974 val loss 4.6135\n",
            "The current learning rate 0.00005\n",
            "Epoch 17902 train loss 4.9250 val loss 4.5102\n",
            "The current learning rate 0.00005\n",
            "Epoch 17903 train loss 4.4121 val loss 4.7172\n",
            "The current learning rate 0.00005\n",
            "Epoch 17904 train loss 4.8522 val loss 4.3789\n",
            "The current learning rate 0.00005\n",
            "Epoch 17905 train loss 4.5842 val loss 4.7041\n",
            "The current learning rate 0.00005\n",
            "Epoch 17906 train loss 4.5389 val loss 4.5573\n",
            "The current learning rate 0.00005\n",
            "Epoch 17907 train loss 4.5079 val loss 4.6092\n",
            "The current learning rate 0.00005\n",
            "Epoch 17908 train loss 4.9120 val loss 4.6510\n",
            "The current learning rate 0.00005\n",
            "Epoch 17909 train loss 4.7178 val loss 4.6957\n",
            "The current learning rate 0.00005\n",
            "Epoch 17910 train loss 4.7281 val loss 4.8436\n",
            "The current learning rate 0.00005\n",
            "Epoch 17911 train loss 4.4872 val loss 4.5700\n",
            "The current learning rate 0.00005\n",
            "Epoch 17912 train loss 4.4111 val loss 4.2939\n",
            "The current learning rate 0.00005\n",
            "Epoch 17913 train loss 4.4369 val loss 4.3050\n",
            "The current learning rate 0.00005\n",
            "Epoch 17914 train loss 4.7599 val loss 4.9605\n",
            "The current learning rate 0.00005\n",
            "Epoch 17915 train loss 4.9230 val loss 4.9523\n",
            "The current learning rate 0.00005\n",
            "Epoch 17916 train loss 4.5327 val loss 4.9860\n",
            "The current learning rate 0.00005\n",
            "Epoch 17917 train loss 4.9188 val loss 4.9423\n",
            "The current learning rate 0.00005\n",
            "Epoch 17918 train loss 4.8098 val loss 4.8546\n",
            "The current learning rate 0.00005\n",
            "Epoch 17919 train loss 4.5893 val loss 4.5100\n",
            "The current learning rate 0.00005\n",
            "Epoch 17920 train loss 4.9330 val loss 4.8645\n",
            "The current learning rate 0.00005\n",
            "Epoch 17921 train loss 4.8354 val loss 4.5924\n",
            "The current learning rate 0.00005\n",
            "Epoch 17922 train loss 4.8054 val loss 4.5095\n",
            "The current learning rate 0.00005\n",
            "Epoch 17923 train loss 4.9075 val loss 5.0973\n",
            "The current learning rate 0.00005\n",
            "Epoch 17924 train loss 4.6789 val loss 4.6084\n",
            "The current learning rate 0.00005\n",
            "Epoch 17925 train loss 5.1432 val loss 4.6300\n",
            "The current learning rate 0.00005\n",
            "Epoch 17926 train loss 4.6722 val loss 4.7773\n",
            "The current learning rate 0.00005\n",
            "Epoch 17927 train loss 4.5049 val loss 4.5772\n",
            "The current learning rate 0.00005\n",
            "Epoch 17928 train loss 4.5089 val loss 4.6651\n",
            "The current learning rate 0.00005\n",
            "Epoch 17929 train loss 4.8795 val loss 4.8356\n",
            "The current learning rate 0.00005\n",
            "Epoch 17930 train loss 4.8053 val loss 4.6116\n",
            "The current learning rate 0.00005\n",
            "Epoch 17931 train loss 4.4495 val loss 4.5565\n",
            "The current learning rate 0.00005\n",
            "Epoch 17932 train loss 4.7648 val loss 4.9933\n",
            "The current learning rate 0.00005\n",
            "Epoch 17933 train loss 4.7008 val loss 4.6773\n",
            "The current learning rate 0.00005\n",
            "Epoch 17934 train loss 4.4774 val loss 4.8461\n",
            "The current learning rate 0.00005\n",
            "Epoch 17935 train loss 4.7505 val loss 4.7787\n",
            "The current learning rate 0.00005\n",
            "Epoch 17936 train loss 4.6046 val loss 4.4260\n",
            "The current learning rate 0.00005\n",
            "Epoch 17937 train loss 4.6612 val loss 5.0897\n",
            "The current learning rate 0.00005\n",
            "Epoch 17938 train loss 4.9937 val loss 4.6089\n",
            "The current learning rate 0.00005\n",
            "Epoch 17939 train loss 4.3367 val loss 4.5796\n",
            "The current learning rate 0.00005\n",
            "Epoch 17940 train loss 4.5525 val loss 4.8690\n",
            "The current learning rate 0.00005\n",
            "Epoch 17941 train loss 4.5554 val loss 4.5148\n",
            "The current learning rate 0.00005\n",
            "Epoch 17942 train loss 4.8576 val loss 4.6706\n",
            "The current learning rate 0.00005\n",
            "Epoch 17943 train loss 4.5562 val loss 4.7725\n",
            "The current learning rate 0.00005\n",
            "Epoch 17944 train loss 4.5012 val loss 4.8677\n",
            "The current learning rate 0.00005\n",
            "Epoch 17945 train loss 4.6645 val loss 4.6192\n",
            "The current learning rate 0.00005\n",
            "Epoch 17946 train loss 4.7480 val loss 4.8912\n",
            "The current learning rate 0.00005\n",
            "Epoch 17947 train loss 4.9847 val loss 4.6750\n",
            "The current learning rate 0.00005\n",
            "Epoch 17948 train loss 4.4129 val loss 4.8646\n",
            "The current learning rate 0.00005\n",
            "Epoch 17949 train loss 4.7576 val loss 4.6737\n",
            "The current learning rate 0.00005\n",
            "Epoch 17950 train loss 4.5255 val loss 4.6896\n",
            "The current learning rate 0.00005\n",
            "Epoch 17951 train loss 4.7709 val loss 4.6935\n",
            "The current learning rate 0.00005\n",
            "Epoch 17952 train loss 4.9646 val loss 4.6161\n",
            "The current learning rate 0.00005\n",
            "Epoch 17953 train loss 4.8884 val loss 4.7965\n",
            "The current learning rate 0.00005\n",
            "Epoch 17954 train loss 4.6185 val loss 4.6051\n",
            "The current learning rate 0.00005\n",
            "Epoch 17955 train loss 4.6593 val loss 4.8344\n",
            "The current learning rate 0.00005\n",
            "Epoch 17956 train loss 5.0168 val loss 4.4538\n",
            "The current learning rate 0.00005\n",
            "Epoch 17957 train loss 4.7574 val loss 4.4595\n",
            "The current learning rate 0.00005\n",
            "Epoch 17958 train loss 4.6269 val loss 4.6833\n",
            "The current learning rate 0.00005\n",
            "Epoch 17959 train loss 4.7628 val loss 4.5097\n",
            "The current learning rate 0.00005\n",
            "Epoch 17960 train loss 4.3787 val loss 4.6792\n",
            "The current learning rate 0.00005\n",
            "Epoch 17961 train loss 4.5330 val loss 4.5549\n",
            "The current learning rate 0.00005\n",
            "Epoch 17962 train loss 4.8423 val loss 4.9534\n",
            "The current learning rate 0.00005\n",
            "Epoch 17963 train loss 4.6624 val loss 4.8659\n",
            "The current learning rate 0.00005\n",
            "Epoch 17964 train loss 4.4778 val loss 4.7865\n",
            "The current learning rate 0.00005\n",
            "Epoch 17965 train loss 4.5235 val loss 5.0372\n",
            "The current learning rate 0.00005\n",
            "Epoch 17966 train loss 4.9397 val loss 4.9061\n",
            "The current learning rate 0.00005\n",
            "Epoch 17967 train loss 4.3745 val loss 4.8518\n",
            "The current learning rate 0.00005\n",
            "Epoch 17968 train loss 4.8517 val loss 4.6891\n",
            "The current learning rate 0.00005\n",
            "Epoch 17969 train loss 4.5202 val loss 4.7886\n",
            "The current learning rate 0.00005\n",
            "Epoch 17970 train loss 4.5471 val loss 4.5212\n",
            "The current learning rate 0.00005\n",
            "Epoch 17971 train loss 4.4973 val loss 4.3922\n",
            "The current learning rate 0.00005\n",
            "Epoch 17972 train loss 4.6305 val loss 4.4671\n",
            "The current learning rate 0.00005\n",
            "Epoch 17973 train loss 4.8625 val loss 4.8195\n",
            "The current learning rate 0.00005\n",
            "Epoch 17974 train loss 4.6837 val loss 4.6294\n",
            "The current learning rate 0.00005\n",
            "Epoch 17975 train loss 4.4340 val loss 4.9731\n",
            "The current learning rate 0.00005\n",
            "Epoch 17976 train loss 4.7677 val loss 5.0019\n",
            "The current learning rate 0.00005\n",
            "Epoch 17977 train loss 5.2403 val loss 4.6599\n",
            "The current learning rate 0.00005\n",
            "Epoch 17978 train loss 4.4860 val loss 4.5405\n",
            "The current learning rate 0.00005\n",
            "Epoch 17979 train loss 4.5693 val loss 4.6260\n",
            "The current learning rate 0.00005\n",
            "Epoch 17980 train loss 4.6915 val loss 4.8288\n",
            "The current learning rate 0.00005\n",
            "Epoch 17981 train loss 4.8387 val loss 4.6418\n",
            "The current learning rate 0.00005\n",
            "Epoch 17982 train loss 4.4222 val loss 4.9896\n",
            "The current learning rate 0.00005\n",
            "Epoch 17983 train loss 4.8799 val loss 4.5633\n",
            "The current learning rate 0.00005\n",
            "Epoch 17984 train loss 4.7077 val loss 4.4376\n",
            "The current learning rate 0.00005\n",
            "Epoch 17985 train loss 4.6031 val loss 4.7576\n",
            "The current learning rate 0.00005\n",
            "Epoch 17986 train loss 4.4704 val loss 5.1300\n",
            "The current learning rate 0.00005\n",
            "Epoch 17987 train loss 4.6574 val loss 4.5744\n",
            "The current learning rate 0.00005\n",
            "Epoch 17988 train loss 4.6055 val loss 4.8225\n",
            "The current learning rate 0.00005\n",
            "Epoch 17989 train loss 4.3732 val loss 5.0656\n",
            "The current learning rate 0.00005\n",
            "Epoch 17990 train loss 4.5098 val loss 4.3652\n",
            "The current learning rate 0.00005\n",
            "Epoch 17991 train loss 4.9734 val loss 4.7401\n",
            "The current learning rate 0.00005\n",
            "Epoch 17992 train loss 4.4620 val loss 4.9431\n",
            "The current learning rate 0.00005\n",
            "Epoch 17993 train loss 4.9827 val loss 4.6359\n",
            "The current learning rate 0.00005\n",
            "Epoch 17994 train loss 4.6398 val loss 4.5481\n",
            "The current learning rate 0.00005\n",
            "Epoch 17995 train loss 4.9429 val loss 4.8178\n",
            "The current learning rate 0.00005\n",
            "Epoch 17996 train loss 4.8047 val loss 4.6467\n",
            "The current learning rate 0.00005\n",
            "Epoch 17997 train loss 4.3762 val loss 4.2578\n",
            "The current learning rate 0.00005\n",
            "Epoch 17998 train loss 4.7263 val loss 4.6083\n",
            "The current learning rate 0.00005\n",
            "Epoch 17999 train loss 4.7184 val loss 4.7535\n",
            "The current learning rate 0.00005\n",
            "Epoch 18000 train loss 4.4051 val loss 4.4262\n",
            "The current learning rate 0.00005\n",
            "Epoch 18001 train loss 4.7180 val loss 4.4452\n",
            "The current learning rate 0.00005\n",
            "Epoch 18002 train loss 4.4440 val loss 4.6096\n",
            "The current learning rate 0.00005\n",
            "Epoch 18003 train loss 4.9532 val loss 4.4056\n",
            "The current learning rate 0.00005\n",
            "Epoch 18004 train loss 4.2492 val loss 4.5125\n",
            "The current learning rate 0.00005\n",
            "Epoch 18005 train loss 4.6051 val loss 4.8746\n",
            "The current learning rate 0.00005\n",
            "Epoch 18006 train loss 4.7182 val loss 4.9062\n",
            "The current learning rate 0.00005\n",
            "Epoch 18007 train loss 4.6789 val loss 4.6611\n",
            "The current learning rate 0.00005\n",
            "Epoch 18008 train loss 4.9652 val loss 4.3690\n",
            "The current learning rate 0.00005\n",
            "Epoch 18009 train loss 4.7800 val loss 4.4369\n",
            "The current learning rate 0.00005\n",
            "Epoch 18010 train loss 4.8873 val loss 4.6878\n",
            "The current learning rate 0.00005\n",
            "Epoch 18011 train loss 4.5628 val loss 4.9566\n",
            "The current learning rate 0.00005\n",
            "Epoch 18012 train loss 4.5287 val loss 4.4117\n",
            "The current learning rate 0.00005\n",
            "Epoch 18013 train loss 4.5102 val loss 4.6276\n",
            "The current learning rate 0.00005\n",
            "Epoch 18014 train loss 4.4123 val loss 4.6716\n",
            "The current learning rate 0.00005\n",
            "Epoch 18015 train loss 4.4145 val loss 4.5752\n",
            "The current learning rate 0.00005\n",
            "Epoch 18016 train loss 4.7202 val loss 4.6012\n",
            "The current learning rate 0.00005\n",
            "Epoch 18017 train loss 5.3016 val loss 4.5811\n",
            "The current learning rate 0.00005\n",
            "Epoch 18018 train loss 4.4740 val loss 4.6686\n",
            "The current learning rate 0.00005\n",
            "Epoch 18019 train loss 4.8699 val loss 4.8022\n",
            "The current learning rate 0.00005\n",
            "Epoch 18020 train loss 4.6856 val loss 4.5486\n",
            "The current learning rate 0.00005\n",
            "Epoch 18021 train loss 4.8347 val loss 4.7757\n",
            "The current learning rate 0.00005\n",
            "Epoch 18022 train loss 4.5211 val loss 4.7251\n",
            "The current learning rate 0.00005\n",
            "Epoch 18023 train loss 4.7994 val loss 4.6412\n",
            "The current learning rate 0.00005\n",
            "Epoch 18024 train loss 4.7269 val loss 4.3935\n",
            "The current learning rate 0.00005\n",
            "Epoch 18025 train loss 4.6641 val loss 4.4011\n",
            "The current learning rate 0.00005\n",
            "Epoch 18026 train loss 4.5453 val loss 4.6807\n",
            "The current learning rate 0.00005\n",
            "Epoch 18027 train loss 4.6991 val loss 4.9510\n",
            "The current learning rate 0.00005\n",
            "Epoch 18028 train loss 4.9277 val loss 4.4258\n",
            "The current learning rate 0.00005\n",
            "Epoch 18029 train loss 4.6818 val loss 4.9060\n",
            "The current learning rate 0.00005\n",
            "Epoch 18030 train loss 4.5809 val loss 4.6624\n",
            "The current learning rate 0.00005\n",
            "Epoch 18031 train loss 4.5202 val loss 4.7121\n",
            "The current learning rate 0.00005\n",
            "Epoch 18032 train loss 4.7830 val loss 4.6239\n",
            "The current learning rate 0.00005\n",
            "Epoch 18033 train loss 4.3973 val loss 4.8157\n",
            "The current learning rate 0.00005\n",
            "Epoch 18034 train loss 4.6900 val loss 4.8346\n",
            "The current learning rate 0.00005\n",
            "Epoch 18035 train loss 4.6547 val loss 4.6803\n",
            "The current learning rate 0.00005\n",
            "Epoch 18036 train loss 4.4179 val loss 4.5970\n",
            "The current learning rate 0.00005\n",
            "Epoch 18037 train loss 4.6452 val loss 4.5431\n",
            "The current learning rate 0.00005\n",
            "Epoch 18038 train loss 4.7379 val loss 4.6836\n",
            "The current learning rate 0.00005\n",
            "Epoch 18039 train loss 4.6312 val loss 4.8050\n",
            "The current learning rate 0.00005\n",
            "Epoch 18040 train loss 5.0202 val loss 4.4255\n",
            "The current learning rate 0.00005\n",
            "Epoch 18041 train loss 4.8654 val loss 4.7658\n",
            "The current learning rate 0.00005\n",
            "Epoch 18042 train loss 4.5491 val loss 4.5203\n",
            "The current learning rate 0.00005\n",
            "Epoch 18043 train loss 4.5287 val loss 4.4980\n",
            "The current learning rate 0.00005\n",
            "Epoch 18044 train loss 4.6442 val loss 4.8120\n",
            "The current learning rate 0.00005\n",
            "Epoch 18045 train loss 4.4240 val loss 4.8145\n",
            "The current learning rate 0.00005\n",
            "Epoch 18046 train loss 4.5966 val loss 4.6464\n",
            "The current learning rate 0.00005\n",
            "Epoch 18047 train loss 4.8157 val loss 4.0201\n",
            "The current learning rate 0.00005\n",
            "Epoch 18048 train loss 4.8580 val loss 4.7059\n",
            "The current learning rate 0.00005\n",
            "Epoch 18049 train loss 4.6659 val loss 4.6794\n",
            "The current learning rate 0.00005\n",
            "Epoch 18050 train loss 4.7494 val loss 4.5733\n",
            "The current learning rate 0.00005\n",
            "Epoch 18051 train loss 4.5927 val loss 4.4652\n",
            "The current learning rate 0.00005\n",
            "Epoch 18052 train loss 4.6764 val loss 4.6911\n",
            "The current learning rate 0.00005\n",
            "Epoch 18053 train loss 4.6945 val loss 4.4749\n",
            "The current learning rate 0.00005\n",
            "Epoch 18054 train loss 5.0950 val loss 4.6293\n",
            "The current learning rate 0.00005\n",
            "Epoch 18055 train loss 4.5589 val loss 4.5547\n",
            "The current learning rate 0.00005\n",
            "Epoch 18056 train loss 5.0158 val loss 4.6771\n",
            "The current learning rate 0.00005\n",
            "Epoch 18057 train loss 4.8172 val loss 4.6726\n",
            "The current learning rate 0.00005\n",
            "Epoch 18058 train loss 4.4806 val loss 4.8366\n",
            "The current learning rate 0.00005\n",
            "Epoch 18059 train loss 4.5295 val loss 4.5930\n",
            "The current learning rate 0.00005\n",
            "Epoch 18060 train loss 4.7197 val loss 4.7170\n",
            "The current learning rate 0.00005\n",
            "Epoch 18061 train loss 4.5113 val loss 4.7971\n",
            "The current learning rate 0.00005\n",
            "Epoch 18062 train loss 5.0470 val loss 4.2339\n",
            "The current learning rate 0.00005\n",
            "Epoch 18063 train loss 4.5104 val loss 4.5410\n",
            "The current learning rate 0.00005\n",
            "Epoch 18064 train loss 4.8376 val loss 4.8835\n",
            "The current learning rate 0.00005\n",
            "Epoch 18065 train loss 4.7314 val loss 4.3703\n",
            "The current learning rate 0.00005\n",
            "Epoch 18066 train loss 4.5189 val loss 4.7633\n",
            "The current learning rate 0.00005\n",
            "Epoch 18067 train loss 4.3653 val loss 4.9717\n",
            "The current learning rate 0.00005\n",
            "Epoch 18068 train loss 4.7216 val loss 4.7514\n",
            "The current learning rate 0.00005\n",
            "Epoch 18069 train loss 4.6605 val loss 4.9419\n",
            "The current learning rate 0.00005\n",
            "Epoch 18070 train loss 4.5390 val loss 4.8407\n",
            "The current learning rate 0.00005\n",
            "Epoch 18071 train loss 4.4755 val loss 4.5996\n",
            "The current learning rate 0.00005\n",
            "Epoch 18072 train loss 5.0141 val loss 4.4747\n",
            "The current learning rate 0.00005\n",
            "Epoch 18073 train loss 4.2728 val loss 5.0288\n",
            "The current learning rate 0.00005\n",
            "Epoch 18074 train loss 4.6621 val loss 4.5264\n",
            "The current learning rate 0.00005\n",
            "Epoch 18075 train loss 4.7453 val loss 4.9179\n",
            "The current learning rate 0.00005\n",
            "Epoch 18076 train loss 4.7451 val loss 4.8545\n",
            "The current learning rate 0.00005\n",
            "Epoch 18077 train loss 4.7166 val loss 4.5846\n",
            "The current learning rate 0.00005\n",
            "Epoch 18078 train loss 4.4351 val loss 4.8694\n",
            "The current learning rate 0.00005\n",
            "Epoch 18079 train loss 4.7208 val loss 4.7363\n",
            "The current learning rate 0.00005\n",
            "Epoch 18080 train loss 4.6043 val loss 4.5143\n",
            "The current learning rate 0.00005\n",
            "Epoch 18081 train loss 4.6142 val loss 4.4223\n",
            "The current learning rate 0.00005\n",
            "Epoch 18082 train loss 4.6228 val loss 4.9279\n",
            "The current learning rate 0.00005\n",
            "Epoch 18083 train loss 4.6749 val loss 4.6801\n",
            "The current learning rate 0.00005\n",
            "Epoch 18084 train loss 5.1544 val loss 4.3158\n",
            "The current learning rate 0.00005\n",
            "Epoch 18085 train loss 4.6795 val loss 4.9464\n",
            "The current learning rate 0.00005\n",
            "Epoch 18086 train loss 4.9110 val loss 4.7731\n",
            "The current learning rate 0.00005\n",
            "Epoch 18087 train loss 4.5094 val loss 4.4403\n",
            "The current learning rate 0.00005\n",
            "Epoch 18088 train loss 4.6126 val loss 4.6371\n",
            "The current learning rate 0.00005\n",
            "Epoch 18089 train loss 4.8216 val loss 4.9813\n",
            "The current learning rate 0.00005\n",
            "Epoch 18090 train loss 4.5131 val loss 4.8610\n",
            "The current learning rate 0.00005\n",
            "Epoch 18091 train loss 4.9248 val loss 4.6685\n",
            "The current learning rate 0.00005\n",
            "Epoch 18092 train loss 4.5380 val loss 4.4188\n",
            "The current learning rate 0.00005\n",
            "Epoch 18093 train loss 4.6678 val loss 4.7913\n",
            "The current learning rate 0.00005\n",
            "Epoch 18094 train loss 4.3727 val loss 4.8169\n",
            "The current learning rate 0.00005\n",
            "Epoch 18095 train loss 4.6046 val loss 4.7015\n",
            "The current learning rate 0.00005\n",
            "Epoch 18096 train loss 4.6739 val loss 4.6864\n",
            "The current learning rate 0.00005\n",
            "Epoch 18097 train loss 4.6166 val loss 4.8226\n",
            "The current learning rate 0.00005\n",
            "Epoch 18098 train loss 4.6664 val loss 4.2551\n",
            "The current learning rate 0.00005\n",
            "Epoch 18099 train loss 4.9549 val loss 4.6921\n",
            "The current learning rate 0.00005\n",
            "Epoch 18100 train loss 4.7030 val loss 4.5972\n",
            "The current learning rate 0.00005\n",
            "Epoch 18101 train loss 4.6678 val loss 4.6941\n",
            "The current learning rate 0.00005\n",
            "Epoch 18102 train loss 4.6154 val loss 4.7131\n",
            "The current learning rate 0.00005\n",
            "Epoch 18103 train loss 4.6233 val loss 4.5810\n",
            "The current learning rate 0.00005\n",
            "Epoch 18104 train loss 4.8220 val loss 4.9459\n",
            "The current learning rate 0.00005\n",
            "Epoch 18105 train loss 4.6480 val loss 4.6832\n",
            "The current learning rate 0.00005\n",
            "Epoch 18106 train loss 4.3592 val loss 4.6183\n",
            "The current learning rate 0.00005\n",
            "Epoch 18107 train loss 4.7808 val loss 4.8768\n",
            "The current learning rate 0.00005\n",
            "Epoch 18108 train loss 4.7896 val loss 4.6510\n",
            "The current learning rate 0.00005\n",
            "Epoch 18109 train loss 4.7251 val loss 4.4039\n",
            "The current learning rate 0.00005\n",
            "Epoch 18110 train loss 4.4635 val loss 4.7405\n",
            "The current learning rate 0.00005\n",
            "Epoch 18111 train loss 4.8925 val loss 4.4389\n",
            "The current learning rate 0.00005\n",
            "Epoch 18112 train loss 4.4126 val loss 4.9802\n",
            "The current learning rate 0.00005\n",
            "Epoch 18113 train loss 4.6786 val loss 4.5669\n",
            "The current learning rate 0.00005\n",
            "Epoch 18114 train loss 4.5911 val loss 4.3549\n",
            "The current learning rate 0.00005\n",
            "Epoch 18115 train loss 4.5984 val loss 4.5828\n",
            "The current learning rate 0.00005\n",
            "Epoch 18116 train loss 4.6388 val loss 4.7443\n",
            "The current learning rate 0.00005\n",
            "Epoch 18117 train loss 4.4856 val loss 4.7060\n",
            "The current learning rate 0.00005\n",
            "Epoch 18118 train loss 4.7365 val loss 4.8864\n",
            "The current learning rate 0.00005\n",
            "Epoch 18119 train loss 4.6519 val loss 4.8599\n",
            "The current learning rate 0.00005\n",
            "Epoch 18120 train loss 4.6306 val loss 4.8037\n",
            "The current learning rate 0.00005\n",
            "Epoch 18121 train loss 4.9227 val loss 4.5950\n",
            "The current learning rate 0.00005\n",
            "Epoch 18122 train loss 4.6727 val loss 4.7422\n",
            "The current learning rate 0.00005\n",
            "Epoch 18123 train loss 4.5816 val loss 4.8459\n",
            "The current learning rate 0.00005\n",
            "Epoch 18124 train loss 4.6331 val loss 4.1389\n",
            "The current learning rate 0.00005\n",
            "Epoch 18125 train loss 4.8005 val loss 4.6327\n",
            "The current learning rate 0.00005\n",
            "Epoch 18126 train loss 4.5184 val loss 4.5031\n",
            "The current learning rate 0.00005\n",
            "Epoch 18127 train loss 4.7065 val loss 4.8226\n",
            "The current learning rate 0.00005\n",
            "Epoch 18128 train loss 4.3199 val loss 4.7254\n",
            "The current learning rate 0.00005\n",
            "Epoch 18129 train loss 4.7582 val loss 4.6747\n",
            "The current learning rate 0.00005\n",
            "Epoch 18130 train loss 4.2620 val loss 4.9422\n",
            "The current learning rate 0.00005\n",
            "Epoch 18131 train loss 4.4989 val loss 4.4388\n",
            "The current learning rate 0.00005\n",
            "Epoch 18132 train loss 4.7375 val loss 4.6640\n",
            "The current learning rate 0.00005\n",
            "Epoch 18133 train loss 4.8247 val loss 4.5680\n",
            "The current learning rate 0.00005\n",
            "Epoch 18134 train loss 4.2555 val loss 4.6864\n",
            "The current learning rate 0.00005\n",
            "Epoch 18135 train loss 4.4569 val loss 4.8498\n",
            "The current learning rate 0.00005\n",
            "Epoch 18136 train loss 4.5152 val loss 4.8761\n",
            "The current learning rate 0.00005\n",
            "Epoch 18137 train loss 4.9924 val loss 4.6012\n",
            "The current learning rate 0.00005\n",
            "Epoch 18138 train loss 4.6182 val loss 4.7745\n",
            "The current learning rate 0.00005\n",
            "Epoch 18139 train loss 4.7129 val loss 4.8069\n",
            "The current learning rate 0.00005\n",
            "Epoch 18140 train loss 4.7553 val loss 4.6768\n",
            "The current learning rate 0.00005\n",
            "Epoch 18141 train loss 4.5248 val loss 4.5005\n",
            "The current learning rate 0.00005\n",
            "Epoch 18142 train loss 4.5259 val loss 5.0046\n",
            "The current learning rate 0.00005\n",
            "Epoch 18143 train loss 4.6497 val loss 4.3505\n",
            "The current learning rate 0.00005\n",
            "Epoch 18144 train loss 4.3422 val loss 4.4301\n",
            "The current learning rate 0.00005\n",
            "Epoch 18145 train loss 4.5574 val loss 4.3468\n",
            "The current learning rate 0.00005\n",
            "Epoch 18146 train loss 4.2674 val loss 4.3396\n",
            "The current learning rate 0.00005\n",
            "Epoch 18147 train loss 4.4501 val loss 4.4395\n",
            "The current learning rate 0.00005\n",
            "Epoch 18148 train loss 4.8017 val loss 4.3900\n",
            "The current learning rate 0.00005\n",
            "Epoch 18149 train loss 4.3820 val loss 4.7687\n",
            "The current learning rate 0.00005\n",
            "Epoch 18150 train loss 4.2432 val loss 4.8230\n",
            "The current learning rate 0.00005\n",
            "Epoch 18151 train loss 4.8686 val loss 4.5581\n",
            "The current learning rate 0.00005\n",
            "Epoch 18152 train loss 4.8349 val loss 4.4305\n",
            "The current learning rate 0.00005\n",
            "Epoch 18153 train loss 4.6770 val loss 4.4580\n",
            "The current learning rate 0.00005\n",
            "Epoch 18154 train loss 4.5105 val loss 4.4335\n",
            "The current learning rate 0.00005\n",
            "Epoch 18155 train loss 4.6880 val loss 4.6220\n",
            "The current learning rate 0.00005\n",
            "Epoch 18156 train loss 4.4347 val loss 5.3542\n",
            "The current learning rate 0.00005\n",
            "Epoch 18157 train loss 5.0477 val loss 4.4632\n",
            "The current learning rate 0.00005\n",
            "Epoch 18158 train loss 4.6871 val loss 4.7267\n",
            "The current learning rate 0.00005\n",
            "Epoch 18159 train loss 4.5781 val loss 4.6315\n",
            "The current learning rate 0.00005\n",
            "Epoch 18160 train loss 4.4410 val loss 4.7425\n",
            "The current learning rate 0.00005\n",
            "Epoch 18161 train loss 4.4683 val loss 4.5026\n",
            "The current learning rate 0.00005\n",
            "Epoch 18162 train loss 4.5758 val loss 4.6085\n",
            "The current learning rate 0.00005\n",
            "Epoch 18163 train loss 4.6077 val loss 4.4551\n",
            "The current learning rate 0.00005\n",
            "Epoch 18164 train loss 4.6956 val loss 4.6851\n",
            "The current learning rate 0.00005\n",
            "Epoch 18165 train loss 4.5923 val loss 4.6574\n",
            "The current learning rate 0.00005\n",
            "Epoch 18166 train loss 4.6189 val loss 4.5480\n",
            "The current learning rate 0.00005\n",
            "Epoch 18167 train loss 4.7035 val loss 5.0081\n",
            "The current learning rate 0.00005\n",
            "Epoch 18168 train loss 4.7016 val loss 4.6675\n",
            "The current learning rate 0.00005\n",
            "Epoch 18169 train loss 4.5856 val loss 4.5763\n",
            "The current learning rate 0.00005\n",
            "Epoch 18170 train loss 4.8756 val loss 4.5967\n",
            "The current learning rate 0.00005\n",
            "Epoch 18171 train loss 4.6367 val loss 4.7470\n",
            "The current learning rate 0.00005\n",
            "Epoch 18172 train loss 4.6341 val loss 4.9944\n",
            "The current learning rate 0.00005\n",
            "Epoch 18173 train loss 4.9544 val loss 4.6755\n",
            "The current learning rate 0.00005\n",
            "Epoch 18174 train loss 4.6326 val loss 5.0055\n",
            "The current learning rate 0.00005\n",
            "Epoch 18175 train loss 4.7014 val loss 4.5284\n",
            "The current learning rate 0.00005\n",
            "Epoch 18176 train loss 5.1201 val loss 4.5268\n",
            "The current learning rate 0.00005\n",
            "Epoch 18177 train loss 4.3416 val loss 4.6006\n",
            "The current learning rate 0.00005\n",
            "Epoch 18178 train loss 4.4802 val loss 4.7439\n",
            "The current learning rate 0.00005\n",
            "Epoch 18179 train loss 4.8769 val loss 4.7804\n",
            "The current learning rate 0.00005\n",
            "Epoch 18180 train loss 4.5282 val loss 4.8421\n",
            "The current learning rate 0.00005\n",
            "Epoch 18181 train loss 4.8427 val loss 4.3781\n",
            "The current learning rate 0.00005\n",
            "Epoch 18182 train loss 4.3982 val loss 4.9276\n",
            "The current learning rate 0.00005\n",
            "Epoch 18183 train loss 4.9989 val loss 4.6275\n",
            "The current learning rate 0.00005\n",
            "Epoch 18184 train loss 4.5178 val loss 4.1594\n",
            "The current learning rate 0.00005\n",
            "Epoch 18185 train loss 4.5089 val loss 4.2850\n",
            "The current learning rate 0.00005\n",
            "Epoch 18186 train loss 4.5480 val loss 4.4549\n",
            "The current learning rate 0.00005\n",
            "Epoch 18187 train loss 4.8676 val loss 4.8821\n",
            "The current learning rate 0.00005\n",
            "Epoch 18188 train loss 5.0012 val loss 4.9327\n",
            "The current learning rate 0.00005\n",
            "Epoch 18189 train loss 4.6461 val loss 4.3073\n",
            "The current learning rate 0.00005\n",
            "Epoch 18190 train loss 4.6925 val loss 4.9096\n",
            "The current learning rate 0.00005\n",
            "Epoch 18191 train loss 4.6994 val loss 4.5985\n",
            "The current learning rate 0.00005\n",
            "Epoch 18192 train loss 4.5102 val loss 4.8129\n",
            "The current learning rate 0.00005\n",
            "Epoch 18193 train loss 4.6934 val loss 4.6797\n",
            "The current learning rate 0.00005\n",
            "Epoch 18194 train loss 4.8516 val loss 4.8038\n",
            "The current learning rate 0.00005\n",
            "Epoch 18195 train loss 4.6321 val loss 4.9952\n",
            "The current learning rate 0.00005\n",
            "Epoch 18196 train loss 4.4357 val loss 4.7452\n",
            "The current learning rate 0.00005\n",
            "Epoch 18197 train loss 4.6728 val loss 4.4014\n",
            "The current learning rate 0.00005\n",
            "Epoch 18198 train loss 4.7148 val loss 4.5969\n",
            "The current learning rate 0.00005\n",
            "Epoch 18199 train loss 5.0286 val loss 4.7297\n",
            "The current learning rate 0.00005\n",
            "Epoch 18200 train loss 4.7233 val loss 4.5333\n",
            "The current learning rate 0.00005\n",
            "Epoch 18201 train loss 4.7434 val loss 4.6434\n",
            "The current learning rate 0.00005\n",
            "Epoch 18202 train loss 4.6693 val loss 4.5550\n",
            "The current learning rate 0.00005\n",
            "Epoch 18203 train loss 4.4437 val loss 5.0055\n",
            "The current learning rate 0.00005\n",
            "Epoch 18204 train loss 4.6342 val loss 4.5052\n",
            "The current learning rate 0.00005\n",
            "Epoch 18205 train loss 4.5180 val loss 4.6055\n",
            "The current learning rate 0.00005\n",
            "Epoch 18206 train loss 4.9514 val loss 4.6883\n",
            "The current learning rate 0.00005\n",
            "Epoch 18207 train loss 4.4938 val loss 4.4630\n",
            "The current learning rate 0.00005\n",
            "Epoch 18208 train loss 4.8690 val loss 5.0842\n",
            "The current learning rate 0.00005\n",
            "Epoch 18209 train loss 4.5831 val loss 4.5124\n",
            "The current learning rate 0.00005\n",
            "Epoch 18210 train loss 4.5667 val loss 4.3717\n",
            "The current learning rate 0.00005\n",
            "Epoch 18211 train loss 5.1457 val loss 4.9024\n",
            "The current learning rate 0.00005\n",
            "Epoch 18212 train loss 4.5941 val loss 4.1989\n",
            "The current learning rate 0.00005\n",
            "Epoch 18213 train loss 4.6396 val loss 4.3995\n",
            "The current learning rate 0.00005\n",
            "Epoch 18214 train loss 4.5346 val loss 4.9629\n",
            "The current learning rate 0.00005\n",
            "Epoch 18215 train loss 4.4319 val loss 4.7361\n",
            "The current learning rate 0.00005\n",
            "Epoch 18216 train loss 4.8595 val loss 4.8809\n",
            "The current learning rate 0.00005\n",
            "Epoch 18217 train loss 5.0123 val loss 4.8611\n",
            "The current learning rate 0.00005\n",
            "Epoch 18218 train loss 4.4926 val loss 5.1155\n",
            "The current learning rate 0.00005\n",
            "Epoch 18219 train loss 4.4400 val loss 4.8880\n",
            "The current learning rate 0.00005\n",
            "Epoch 18220 train loss 4.7553 val loss 4.8089\n",
            "The current learning rate 0.00005\n",
            "Epoch 18221 train loss 4.5937 val loss 4.6886\n",
            "The current learning rate 0.00005\n",
            "Epoch 18222 train loss 4.5775 val loss 4.3671\n",
            "The current learning rate 0.00005\n",
            "Epoch 18223 train loss 4.4005 val loss 4.7430\n",
            "The current learning rate 0.00005\n",
            "Epoch 18224 train loss 4.6764 val loss 4.8735\n",
            "The current learning rate 0.00005\n",
            "Epoch 18225 train loss 4.6765 val loss 4.3800\n",
            "The current learning rate 0.00005\n",
            "Epoch 18226 train loss 4.9446 val loss 4.6840\n",
            "The current learning rate 0.00005\n",
            "Epoch 18227 train loss 4.5995 val loss 4.4298\n",
            "The current learning rate 0.00005\n",
            "Epoch 18228 train loss 4.4938 val loss 4.3698\n",
            "The current learning rate 0.00005\n",
            "Epoch 18229 train loss 4.8844 val loss 4.8522\n",
            "The current learning rate 0.00005\n",
            "Epoch 18230 train loss 5.1165 val loss 4.8372\n",
            "The current learning rate 0.00005\n",
            "Epoch 18231 train loss 4.8599 val loss 4.7564\n",
            "The current learning rate 0.00005\n",
            "Epoch 18232 train loss 4.7573 val loss 5.0415\n",
            "The current learning rate 0.00005\n",
            "Epoch 18233 train loss 4.7299 val loss 4.7128\n",
            "The current learning rate 0.00005\n",
            "Epoch 18234 train loss 4.8923 val loss 5.0330\n",
            "The current learning rate 0.00005\n",
            "Epoch 18235 train loss 5.0674 val loss 4.2784\n",
            "The current learning rate 0.00005\n",
            "Epoch 18236 train loss 4.8298 val loss 4.5209\n",
            "The current learning rate 0.00005\n",
            "Epoch 18237 train loss 4.9671 val loss 4.9275\n",
            "The current learning rate 0.00005\n",
            "Epoch 18238 train loss 4.5834 val loss 4.3729\n",
            "The current learning rate 0.00005\n",
            "Epoch 18239 train loss 4.8141 val loss 4.3980\n",
            "The current learning rate 0.00005\n",
            "Epoch 18240 train loss 4.7978 val loss 4.3034\n",
            "The current learning rate 0.00005\n",
            "Epoch 18241 train loss 4.8672 val loss 4.5198\n",
            "The current learning rate 0.00005\n",
            "Epoch 18242 train loss 5.2832 val loss 4.6244\n",
            "The current learning rate 0.00005\n",
            "Epoch 18243 train loss 4.7685 val loss 4.9614\n",
            "The current learning rate 0.00005\n",
            "Epoch 18244 train loss 4.5148 val loss 4.7336\n",
            "The current learning rate 0.00005\n",
            "Epoch 18245 train loss 4.9188 val loss 4.7334\n",
            "The current learning rate 0.00005\n",
            "Epoch 18246 train loss 4.7572 val loss 4.7274\n",
            "The current learning rate 0.00005\n",
            "Epoch 18247 train loss 4.7136 val loss 4.4051\n",
            "The current learning rate 0.00005\n",
            "Epoch 18248 train loss 5.2101 val loss 4.8363\n",
            "The current learning rate 0.00005\n",
            "Epoch 18249 train loss 4.6088 val loss 4.6330\n",
            "The current learning rate 0.00005\n",
            "Epoch 18250 train loss 4.7164 val loss 4.6066\n",
            "The current learning rate 0.00005\n",
            "Epoch 18251 train loss 4.6498 val loss 4.7839\n",
            "The current learning rate 0.00005\n",
            "Epoch 18252 train loss 4.3604 val loss 4.7447\n",
            "The current learning rate 0.00005\n",
            "Epoch 18253 train loss 4.8790 val loss 4.6193\n",
            "The current learning rate 0.00005\n",
            "Epoch 18254 train loss 5.0497 val loss 4.4318\n",
            "The current learning rate 0.00005\n",
            "Epoch 18255 train loss 4.8063 val loss 4.8598\n",
            "The current learning rate 0.00005\n",
            "Epoch 18256 train loss 4.8090 val loss 4.4785\n",
            "The current learning rate 0.00005\n",
            "Epoch 18257 train loss 4.4964 val loss 4.4777\n",
            "The current learning rate 0.00005\n",
            "Epoch 18258 train loss 4.8774 val loss 4.6876\n",
            "The current learning rate 0.00005\n",
            "Epoch 18259 train loss 4.5690 val loss 4.7109\n",
            "The current learning rate 0.00005\n",
            "Epoch 18260 train loss 5.0100 val loss 4.0214\n",
            "The current learning rate 0.00005\n",
            "Epoch 18261 train loss 4.9337 val loss 4.4115\n",
            "The current learning rate 0.00005\n",
            "Epoch 18262 train loss 4.8427 val loss 4.9455\n",
            "The current learning rate 0.00005\n",
            "Epoch 18263 train loss 4.8472 val loss 4.7683\n",
            "The current learning rate 0.00005\n",
            "Epoch 18264 train loss 4.4588 val loss 4.5318\n",
            "The current learning rate 0.00005\n",
            "Epoch 18265 train loss 4.4009 val loss 4.6255\n",
            "The current learning rate 0.00005\n",
            "Epoch 18266 train loss 4.9294 val loss 5.0056\n",
            "The current learning rate 0.00005\n",
            "Epoch 18267 train loss 4.4588 val loss 4.6083\n",
            "The current learning rate 0.00005\n",
            "Epoch 18268 train loss 4.5824 val loss 4.5872\n",
            "The current learning rate 0.00005\n",
            "Epoch 18269 train loss 4.7418 val loss 4.6911\n",
            "The current learning rate 0.00005\n",
            "Epoch 18270 train loss 4.5033 val loss 5.0502\n",
            "The current learning rate 0.00005\n",
            "Epoch 18271 train loss 4.6680 val loss 4.6727\n",
            "The current learning rate 0.00005\n",
            "Epoch 18272 train loss 4.9977 val loss 4.4217\n",
            "The current learning rate 0.00005\n",
            "Epoch 18273 train loss 4.4289 val loss 4.7249\n",
            "The current learning rate 0.00005\n",
            "Epoch 18274 train loss 5.1497 val loss 4.7319\n",
            "The current learning rate 0.00005\n",
            "Epoch 18275 train loss 4.4102 val loss 4.7772\n",
            "The current learning rate 0.00005\n",
            "Epoch 18276 train loss 4.5964 val loss 4.5131\n",
            "The current learning rate 0.00005\n",
            "Epoch 18277 train loss 4.6528 val loss 4.2816\n",
            "The current learning rate 0.00005\n",
            "Epoch 18278 train loss 4.5871 val loss 4.2284\n",
            "The current learning rate 0.00005\n",
            "Epoch 18279 train loss 4.6040 val loss 4.2507\n",
            "The current learning rate 0.00005\n",
            "Epoch 18280 train loss 4.8117 val loss 4.6497\n",
            "The current learning rate 0.00005\n",
            "Epoch 18281 train loss 4.5394 val loss 4.5570\n",
            "The current learning rate 0.00005\n",
            "Epoch 18282 train loss 4.3390 val loss 4.5081\n",
            "The current learning rate 0.00005\n",
            "Epoch 18283 train loss 5.0836 val loss 4.5660\n",
            "The current learning rate 0.00005\n",
            "Epoch 18284 train loss 4.3626 val loss 4.7077\n",
            "The current learning rate 0.00005\n",
            "Epoch 18285 train loss 4.7392 val loss 4.2647\n",
            "The current learning rate 0.00005\n",
            "Epoch 18286 train loss 4.9343 val loss 4.4661\n",
            "The current learning rate 0.00005\n",
            "Epoch 18287 train loss 4.7190 val loss 5.1163\n",
            "The current learning rate 0.00005\n",
            "Epoch 18288 train loss 4.6808 val loss 4.5715\n",
            "The current learning rate 0.00005\n",
            "Epoch 18289 train loss 4.4498 val loss 4.9572\n",
            "The current learning rate 0.00005\n",
            "Epoch 18290 train loss 4.6109 val loss 4.6458\n",
            "The current learning rate 0.00005\n",
            "Epoch 18291 train loss 4.7599 val loss 4.8867\n",
            "The current learning rate 0.00005\n",
            "Epoch 18292 train loss 4.4285 val loss 4.4712\n",
            "The current learning rate 0.00005\n",
            "Epoch 18293 train loss 4.9180 val loss 4.6111\n",
            "The current learning rate 0.00005\n",
            "Epoch 18294 train loss 4.4717 val loss 4.8716\n",
            "The current learning rate 0.00005\n",
            "Epoch 18295 train loss 4.2054 val loss 4.5974\n",
            "The current learning rate 0.00005\n",
            "Epoch 18296 train loss 4.9198 val loss 4.8215\n",
            "The current learning rate 0.00005\n",
            "Epoch 18297 train loss 4.6142 val loss 4.5213\n",
            "The current learning rate 0.00005\n",
            "Epoch 18298 train loss 5.0879 val loss 4.3685\n",
            "The current learning rate 0.00005\n",
            "Epoch 18299 train loss 4.6968 val loss 4.5288\n",
            "The current learning rate 0.00005\n",
            "Epoch 18300 train loss 4.6563 val loss 4.3353\n",
            "The current learning rate 0.00005\n",
            "Epoch 18301 train loss 4.7198 val loss 4.7688\n",
            "The current learning rate 0.00005\n",
            "Epoch 18302 train loss 4.5340 val loss 4.8726\n",
            "The current learning rate 0.00005\n",
            "Epoch 18303 train loss 4.5176 val loss 4.7656\n",
            "The current learning rate 0.00005\n",
            "Epoch 18304 train loss 4.7455 val loss 4.6127\n",
            "The current learning rate 0.00005\n",
            "Epoch 18305 train loss 4.5358 val loss 4.2401\n",
            "The current learning rate 0.00005\n",
            "Epoch 18306 train loss 4.5834 val loss 4.4138\n",
            "The current learning rate 0.00005\n",
            "Epoch 18307 train loss 4.6490 val loss 4.7880\n",
            "The current learning rate 0.00005\n",
            "Epoch 18308 train loss 4.5484 val loss 4.5869\n",
            "The current learning rate 0.00005\n",
            "Epoch 18309 train loss 4.7468 val loss 4.6322\n",
            "The current learning rate 0.00005\n",
            "Epoch 18310 train loss 4.5594 val loss 4.6758\n",
            "The current learning rate 0.00005\n",
            "Epoch 18311 train loss 4.5372 val loss 4.8402\n",
            "The current learning rate 0.00005\n",
            "Epoch 18312 train loss 4.2886 val loss 4.7137\n",
            "The current learning rate 0.00005\n",
            "Epoch 18313 train loss 4.8873 val loss 4.5037\n",
            "The current learning rate 0.00005\n",
            "Epoch 18314 train loss 4.9013 val loss 4.5726\n",
            "The current learning rate 0.00005\n",
            "Epoch 18315 train loss 4.2931 val loss 4.8317\n",
            "The current learning rate 0.00005\n",
            "Epoch 18316 train loss 4.4483 val loss 4.6749\n",
            "The current learning rate 0.00005\n",
            "Epoch 18317 train loss 4.4989 val loss 4.4899\n",
            "The current learning rate 0.00005\n",
            "Epoch 18318 train loss 4.8425 val loss 5.1178\n",
            "The current learning rate 0.00005\n",
            "Epoch 18319 train loss 4.5228 val loss 4.5551\n",
            "The current learning rate 0.00005\n",
            "Epoch 18320 train loss 4.9142 val loss 4.5171\n",
            "The current learning rate 0.00005\n",
            "Epoch 18321 train loss 4.4984 val loss 4.7549\n",
            "The current learning rate 0.00005\n",
            "Epoch 18322 train loss 4.3869 val loss 4.8158\n",
            "The current learning rate 0.00005\n",
            "Epoch 18323 train loss 4.4125 val loss 4.7908\n",
            "The current learning rate 0.00005\n",
            "Epoch 18324 train loss 4.3885 val loss 4.6070\n",
            "The current learning rate 0.00005\n",
            "Epoch 18325 train loss 4.4902 val loss 4.6813\n",
            "The current learning rate 0.00005\n",
            "Epoch 18326 train loss 5.0358 val loss 4.9925\n",
            "The current learning rate 0.00005\n",
            "Epoch 18327 train loss 4.5203 val loss 4.9431\n",
            "The current learning rate 0.00005\n",
            "Epoch 18328 train loss 4.3444 val loss 4.7692\n",
            "The current learning rate 0.00005\n",
            "Epoch 18329 train loss 4.6524 val loss 4.7270\n",
            "The current learning rate 0.00005\n",
            "Epoch 18330 train loss 4.5908 val loss 4.5606\n",
            "The current learning rate 0.00005\n",
            "Epoch 18331 train loss 4.8087 val loss 4.8594\n",
            "The current learning rate 0.00005\n",
            "Epoch 18332 train loss 4.7900 val loss 4.7332\n",
            "The current learning rate 0.00005\n",
            "Epoch 18333 train loss 4.6467 val loss 4.1723\n",
            "The current learning rate 0.00005\n",
            "Epoch 18334 train loss 4.8807 val loss 4.7591\n",
            "The current learning rate 0.00005\n",
            "Epoch 18335 train loss 4.4535 val loss 4.6434\n",
            "The current learning rate 0.00005\n",
            "Epoch 18336 train loss 4.6865 val loss 4.5090\n",
            "The current learning rate 0.00005\n",
            "Epoch 18337 train loss 5.0191 val loss 4.7421\n",
            "The current learning rate 0.00005\n",
            "Epoch 18338 train loss 4.6081 val loss 4.2795\n",
            "The current learning rate 0.00005\n",
            "Epoch 18339 train loss 4.5253 val loss 4.3482\n",
            "The current learning rate 0.00005\n",
            "Epoch 18340 train loss 4.8583 val loss 4.7544\n",
            "The current learning rate 0.00005\n",
            "Epoch 18341 train loss 4.7459 val loss 4.7871\n",
            "The current learning rate 0.00005\n",
            "Epoch 18342 train loss 4.7115 val loss 4.8562\n",
            "The current learning rate 0.00005\n",
            "Epoch 18343 train loss 4.5698 val loss 4.9898\n",
            "The current learning rate 0.00005\n",
            "Epoch 18344 train loss 4.5086 val loss 4.5095\n",
            "The current learning rate 0.00005\n",
            "Epoch 18345 train loss 4.8592 val loss 4.8726\n",
            "The current learning rate 0.00005\n",
            "Epoch 18346 train loss 4.2694 val loss 4.7547\n",
            "The current learning rate 0.00005\n",
            "Epoch 18347 train loss 5.0716 val loss 4.7396\n",
            "The current learning rate 0.00005\n",
            "Epoch 18348 train loss 4.7086 val loss 4.9826\n",
            "The current learning rate 0.00005\n",
            "Epoch 18349 train loss 4.7809 val loss 4.9496\n",
            "The current learning rate 0.00005\n",
            "Epoch 18350 train loss 5.0899 val loss 4.4427\n",
            "The current learning rate 0.00005\n",
            "Epoch 18351 train loss 4.6049 val loss 4.4653\n",
            "The current learning rate 0.00005\n",
            "Epoch 18352 train loss 4.5265 val loss 4.3618\n",
            "The current learning rate 0.00005\n",
            "Epoch 18353 train loss 4.6608 val loss 4.6908\n",
            "The current learning rate 0.00005\n",
            "Epoch 18354 train loss 4.8521 val loss 4.5250\n",
            "The current learning rate 0.00005\n",
            "Epoch 18355 train loss 4.7790 val loss 4.9727\n",
            "The current learning rate 0.00005\n",
            "Epoch 18356 train loss 4.7798 val loss 4.5820\n",
            "The current learning rate 0.00005\n",
            "Epoch 18357 train loss 4.7044 val loss 4.5949\n",
            "The current learning rate 0.00005\n",
            "Epoch 18358 train loss 4.5823 val loss 4.8027\n",
            "The current learning rate 0.00005\n",
            "Epoch 18359 train loss 4.2805 val loss 4.4440\n",
            "The current learning rate 0.00005\n",
            "Epoch 18360 train loss 4.5694 val loss 4.7219\n",
            "The current learning rate 0.00005\n",
            "Epoch 18361 train loss 4.5194 val loss 4.7695\n",
            "The current learning rate 0.00005\n",
            "Epoch 18362 train loss 4.7980 val loss 4.5912\n",
            "The current learning rate 0.00005\n",
            "Epoch 18363 train loss 4.7631 val loss 4.8473\n",
            "The current learning rate 0.00005\n",
            "Epoch 18364 train loss 4.6116 val loss 4.6870\n",
            "The current learning rate 0.00005\n",
            "Epoch 18365 train loss 4.8711 val loss 4.5222\n",
            "The current learning rate 0.00005\n",
            "Epoch 18366 train loss 4.6236 val loss 4.6062\n",
            "The current learning rate 0.00005\n",
            "Epoch 18367 train loss 4.7412 val loss 4.6103\n",
            "The current learning rate 0.00005\n",
            "Epoch 18368 train loss 4.6948 val loss 4.8014\n",
            "The current learning rate 0.00005\n",
            "Epoch 18369 train loss 4.7030 val loss 4.3988\n",
            "The current learning rate 0.00005\n",
            "Epoch 18370 train loss 4.8644 val loss 4.8259\n",
            "The current learning rate 0.00005\n",
            "Epoch 18371 train loss 4.7520 val loss 4.5637\n",
            "The current learning rate 0.00005\n",
            "Epoch 18372 train loss 4.6084 val loss 4.6665\n",
            "The current learning rate 0.00005\n",
            "Epoch 18373 train loss 4.6509 val loss 4.8770\n",
            "The current learning rate 0.00005\n",
            "Epoch 18374 train loss 4.5509 val loss 4.4210\n",
            "The current learning rate 0.00005\n",
            "Epoch 18375 train loss 4.6094 val loss 4.4269\n",
            "The current learning rate 0.00005\n",
            "Epoch 18376 train loss 4.3310 val loss 4.7860\n",
            "The current learning rate 0.00005\n",
            "Epoch 18377 train loss 4.4909 val loss 4.8716\n",
            "The current learning rate 0.00005\n",
            "Epoch 18378 train loss 5.2216 val loss 4.9302\n",
            "The current learning rate 0.00005\n",
            "Epoch 18379 train loss 4.4617 val loss 4.7321\n",
            "The current learning rate 0.00005\n",
            "Epoch 18380 train loss 4.7940 val loss 4.6190\n",
            "The current learning rate 0.00005\n",
            "Epoch 18381 train loss 4.5506 val loss 4.4619\n",
            "The current learning rate 0.00005\n",
            "Epoch 18382 train loss 4.4831 val loss 4.7067\n",
            "The current learning rate 0.00005\n",
            "Epoch 18383 train loss 4.6539 val loss 4.4095\n",
            "The current learning rate 0.00005\n",
            "Epoch 18384 train loss 4.8390 val loss 4.5451\n",
            "The current learning rate 0.00005\n",
            "Epoch 18385 train loss 4.4527 val loss 4.5700\n",
            "The current learning rate 0.00005\n",
            "Epoch 18386 train loss 4.5772 val loss 4.5650\n",
            "The current learning rate 0.00005\n",
            "Epoch 18387 train loss 4.5778 val loss 4.6136\n",
            "The current learning rate 0.00005\n",
            "Epoch 18388 train loss 4.7592 val loss 4.4187\n",
            "The current learning rate 0.00005\n",
            "Epoch 18389 train loss 4.5053 val loss 4.6506\n",
            "The current learning rate 0.00005\n",
            "Epoch 18390 train loss 4.5975 val loss 4.7195\n",
            "The current learning rate 0.00005\n",
            "Epoch 18391 train loss 4.5447 val loss 4.5957\n",
            "The current learning rate 0.00005\n",
            "Epoch 18392 train loss 4.5521 val loss 4.5424\n",
            "The current learning rate 0.00005\n",
            "Epoch 18393 train loss 4.8176 val loss 4.8409\n",
            "The current learning rate 0.00005\n",
            "Epoch 18394 train loss 4.5232 val loss 4.5744\n",
            "The current learning rate 0.00005\n",
            "Epoch 18395 train loss 4.8408 val loss 4.3887\n",
            "The current learning rate 0.00005\n",
            "Epoch 18396 train loss 4.3489 val loss 4.5776\n",
            "The current learning rate 0.00005\n",
            "Epoch 18397 train loss 4.2749 val loss 4.7919\n",
            "The current learning rate 0.00005\n",
            "Epoch 18398 train loss 5.2149 val loss 4.3925\n",
            "The current learning rate 0.00005\n",
            "Epoch 18399 train loss 4.1904 val loss 4.8577\n",
            "The current learning rate 0.00005\n",
            "Epoch 18400 train loss 4.2125 val loss 4.7584\n",
            "The current learning rate 0.00005\n",
            "Epoch 18401 train loss 5.0430 val loss 4.3824\n",
            "The current learning rate 0.00005\n",
            "Epoch 18402 train loss 4.8516 val loss 4.9227\n",
            "The current learning rate 0.00005\n",
            "Epoch 18403 train loss 4.5848 val loss 4.8628\n",
            "The current learning rate 0.00005\n",
            "Epoch 18404 train loss 4.7104 val loss 4.6145\n",
            "The current learning rate 0.00005\n",
            "Epoch 18405 train loss 4.6464 val loss 4.3844\n",
            "The current learning rate 0.00005\n",
            "Epoch 18406 train loss 4.4804 val loss 4.8809\n",
            "The current learning rate 0.00005\n",
            "Epoch 18407 train loss 4.7242 val loss 4.9131\n",
            "The current learning rate 0.00005\n",
            "Epoch 18408 train loss 4.6800 val loss 4.4354\n",
            "The current learning rate 0.00005\n",
            "Epoch 18409 train loss 4.9373 val loss 4.8492\n",
            "The current learning rate 0.00005\n",
            "Epoch 18410 train loss 4.5171 val loss 4.7582\n",
            "The current learning rate 0.00005\n",
            "Epoch 18411 train loss 4.9529 val loss 5.0475\n",
            "The current learning rate 0.00005\n",
            "Epoch 18412 train loss 4.9585 val loss 4.3226\n",
            "The current learning rate 0.00005\n",
            "Epoch 18413 train loss 4.8260 val loss 4.3925\n",
            "The current learning rate 0.00005\n",
            "Epoch 18414 train loss 4.4418 val loss 4.6581\n",
            "The current learning rate 0.00005\n",
            "Epoch 18415 train loss 5.2132 val loss 4.1974\n",
            "The current learning rate 0.00005\n",
            "Epoch 18416 train loss 4.5842 val loss 4.3394\n",
            "The current learning rate 0.00005\n",
            "Epoch 18417 train loss 4.6608 val loss 4.4028\n",
            "The current learning rate 0.00005\n",
            "Epoch 18418 train loss 4.8413 val loss 4.7821\n",
            "The current learning rate 0.00005\n",
            "Epoch 18419 train loss 4.6950 val loss 4.3818\n",
            "The current learning rate 0.00005\n",
            "Epoch 18420 train loss 4.8438 val loss 4.5676\n",
            "The current learning rate 0.00005\n",
            "Epoch 18421 train loss 4.5911 val loss 5.1510\n",
            "The current learning rate 0.00005\n",
            "Epoch 18422 train loss 4.7458 val loss 4.8231\n",
            "The current learning rate 0.00005\n",
            "Epoch 18423 train loss 4.6407 val loss 4.6937\n",
            "The current learning rate 0.00005\n",
            "Epoch 18424 train loss 4.6235 val loss 4.8686\n",
            "The current learning rate 0.00005\n",
            "Epoch 18425 train loss 4.8763 val loss 4.2593\n",
            "The current learning rate 0.00005\n",
            "Epoch 18426 train loss 5.0167 val loss 4.8003\n",
            "The current learning rate 0.00005\n",
            "Epoch 18427 train loss 4.9427 val loss 4.4468\n",
            "The current learning rate 0.00005\n",
            "Epoch 18428 train loss 4.4836 val loss 4.6011\n",
            "The current learning rate 0.00005\n",
            "Epoch 18429 train loss 4.7773 val loss 4.5431\n",
            "The current learning rate 0.00005\n",
            "Epoch 18430 train loss 4.8847 val loss 4.4374\n",
            "The current learning rate 0.00005\n",
            "Epoch 18431 train loss 4.7723 val loss 4.6643\n",
            "The current learning rate 0.00005\n",
            "Epoch 18432 train loss 4.5620 val loss 5.2375\n",
            "The current learning rate 0.00005\n",
            "Epoch 18433 train loss 4.3998 val loss 4.9002\n",
            "The current learning rate 0.00005\n",
            "Epoch 18434 train loss 4.5668 val loss 4.7570\n",
            "The current learning rate 0.00005\n",
            "Epoch 18435 train loss 4.8220 val loss 4.8096\n",
            "The current learning rate 0.00005\n",
            "Epoch 18436 train loss 4.8918 val loss 4.4934\n",
            "The current learning rate 0.00005\n",
            "Epoch 18437 train loss 4.5721 val loss 4.3675\n",
            "The current learning rate 0.00005\n",
            "Epoch 18438 train loss 4.8305 val loss 4.8020\n",
            "The current learning rate 0.00005\n",
            "Epoch 18439 train loss 4.8593 val loss 5.0244\n",
            "The current learning rate 0.00005\n",
            "Epoch 18440 train loss 4.6021 val loss 4.6649\n",
            "The current learning rate 0.00005\n",
            "Epoch 18441 train loss 4.8419 val loss 4.2321\n",
            "The current learning rate 0.00005\n",
            "Epoch 18442 train loss 4.9690 val loss 4.5120\n",
            "The current learning rate 0.00005\n",
            "Epoch 18443 train loss 4.4287 val loss 4.6836\n",
            "The current learning rate 0.00005\n",
            "Epoch 18444 train loss 5.0583 val loss 4.4376\n",
            "The current learning rate 0.00005\n",
            "Epoch 18445 train loss 4.3547 val loss 4.2441\n",
            "The current learning rate 0.00005\n",
            "Epoch 18446 train loss 4.8950 val loss 4.6523\n",
            "The current learning rate 0.00005\n",
            "Epoch 18447 train loss 4.7576 val loss 4.7766\n",
            "The current learning rate 0.00005\n",
            "Epoch 18448 train loss 4.4949 val loss 4.7019\n",
            "The current learning rate 0.00005\n",
            "Epoch 18449 train loss 4.3311 val loss 4.4945\n",
            "The current learning rate 0.00005\n",
            "Epoch 18450 train loss 4.8184 val loss 4.7285\n",
            "The current learning rate 0.00005\n",
            "Epoch 18451 train loss 4.5943 val loss 4.5715\n",
            "The current learning rate 0.00005\n",
            "Epoch 18452 train loss 4.4149 val loss 4.6410\n",
            "The current learning rate 0.00005\n",
            "Epoch 18453 train loss 5.1329 val loss 4.6768\n",
            "The current learning rate 0.00005\n",
            "Epoch 18454 train loss 5.0590 val loss 5.0573\n",
            "The current learning rate 0.00005\n",
            "Epoch 18455 train loss 4.7934 val loss 4.7924\n",
            "The current learning rate 0.00005\n",
            "Epoch 18456 train loss 4.6768 val loss 5.0734\n",
            "The current learning rate 0.00005\n",
            "Epoch 18457 train loss 4.5138 val loss 4.4865\n",
            "The current learning rate 0.00005\n",
            "Epoch 18458 train loss 4.6629 val loss 4.9457\n",
            "The current learning rate 0.00005\n",
            "Epoch 18459 train loss 4.6881 val loss 4.9471\n",
            "The current learning rate 0.00005\n",
            "Epoch 18460 train loss 4.6979 val loss 4.8906\n",
            "The current learning rate 0.00005\n",
            "Epoch 18461 train loss 4.5313 val loss 4.5119\n",
            "The current learning rate 0.00005\n",
            "Epoch 18462 train loss 4.8148 val loss 4.8796\n",
            "The current learning rate 0.00005\n",
            "Epoch 18463 train loss 4.3577 val loss 4.9159\n",
            "The current learning rate 0.00005\n",
            "Epoch 18464 train loss 4.7484 val loss 4.8873\n",
            "The current learning rate 0.00005\n",
            "Epoch 18465 train loss 4.4380 val loss 4.8113\n",
            "The current learning rate 0.00005\n",
            "Epoch 18466 train loss 4.6681 val loss 4.5984\n",
            "The current learning rate 0.00005\n",
            "Epoch 18467 train loss 4.7401 val loss 4.8006\n",
            "The current learning rate 0.00005\n",
            "Epoch 18468 train loss 4.7010 val loss 4.5288\n",
            "The current learning rate 0.00005\n",
            "Epoch 18469 train loss 4.8749 val loss 5.1830\n",
            "The current learning rate 0.00005\n",
            "Epoch 18470 train loss 4.4704 val loss 4.7199\n",
            "The current learning rate 0.00005\n",
            "Epoch 18471 train loss 4.4234 val loss 4.4615\n",
            "The current learning rate 0.00005\n",
            "Epoch 18472 train loss 4.8388 val loss 4.6665\n",
            "The current learning rate 0.00005\n",
            "Epoch 18473 train loss 4.6868 val loss 4.8035\n",
            "The current learning rate 0.00005\n",
            "Epoch 18474 train loss 4.6940 val loss 4.5137\n",
            "The current learning rate 0.00005\n",
            "Epoch 18475 train loss 4.8467 val loss 4.6674\n",
            "The current learning rate 0.00005\n",
            "Epoch 18476 train loss 4.7497 val loss 4.4024\n",
            "The current learning rate 0.00005\n",
            "Epoch 18477 train loss 4.5303 val loss 4.7438\n",
            "The current learning rate 0.00005\n",
            "Epoch 18478 train loss 4.4533 val loss 5.1560\n",
            "The current learning rate 0.00005\n",
            "Epoch 18479 train loss 4.6572 val loss 4.9291\n",
            "The current learning rate 0.00005\n",
            "Epoch 18480 train loss 4.7115 val loss 4.4882\n",
            "The current learning rate 0.00005\n",
            "Epoch 18481 train loss 4.6274 val loss 5.0292\n",
            "The current learning rate 0.00005\n",
            "Epoch 18482 train loss 5.1589 val loss 4.8702\n",
            "The current learning rate 0.00005\n",
            "Epoch 18483 train loss 4.3130 val loss 4.5473\n",
            "The current learning rate 0.00005\n",
            "Epoch 18484 train loss 5.0528 val loss 4.6312\n",
            "The current learning rate 0.00005\n",
            "Epoch 18485 train loss 4.4965 val loss 4.7811\n",
            "The current learning rate 0.00005\n",
            "Epoch 18486 train loss 4.6162 val loss 4.7144\n",
            "The current learning rate 0.00005\n",
            "Epoch 18487 train loss 4.7040 val loss 4.6451\n",
            "The current learning rate 0.00005\n",
            "Epoch 18488 train loss 4.7022 val loss 4.5740\n",
            "The current learning rate 0.00005\n",
            "Epoch 18489 train loss 4.7203 val loss 4.4931\n",
            "The current learning rate 0.00005\n",
            "Epoch 18490 train loss 4.6031 val loss 4.2558\n",
            "The current learning rate 0.00005\n",
            "Epoch 18491 train loss 4.3667 val loss 4.4227\n",
            "The current learning rate 0.00005\n",
            "Epoch 18492 train loss 4.5645 val loss 4.5486\n",
            "The current learning rate 0.00005\n",
            "Epoch 18493 train loss 4.3475 val loss 4.5913\n",
            "The current learning rate 0.00005\n",
            "Epoch 18494 train loss 4.4335 val loss 4.7358\n",
            "The current learning rate 0.00005\n",
            "Epoch 18495 train loss 4.7501 val loss 4.3540\n",
            "The current learning rate 0.00005\n",
            "Epoch 18496 train loss 4.8113 val loss 4.8978\n",
            "The current learning rate 0.00005\n",
            "Epoch 18497 train loss 4.8019 val loss 4.9949\n",
            "The current learning rate 0.00005\n",
            "Epoch 18498 train loss 4.7175 val loss 4.9851\n",
            "The current learning rate 0.00005\n",
            "Epoch 18499 train loss 4.5915 val loss 4.5637\n",
            "The current learning rate 0.00005\n",
            "Epoch 18500 train loss 4.7063 val loss 4.8226\n",
            "The current learning rate 0.00005\n",
            "Epoch 18501 train loss 4.8690 val loss 4.5640\n",
            "The current learning rate 0.00005\n",
            "Epoch 18502 train loss 5.1336 val loss 4.9022\n",
            "The current learning rate 0.00005\n",
            "Epoch 18503 train loss 4.8935 val loss 4.5044\n",
            "The current learning rate 0.00005\n",
            "Epoch 18504 train loss 4.5294 val loss 4.6558\n",
            "The current learning rate 0.00005\n",
            "Epoch 18505 train loss 4.3609 val loss 4.5805\n",
            "The current learning rate 0.00005\n",
            "Epoch 18506 train loss 4.5299 val loss 4.5910\n",
            "The current learning rate 0.00005\n",
            "Epoch 18507 train loss 4.5044 val loss 5.1397\n",
            "The current learning rate 0.00005\n",
            "Epoch 18508 train loss 4.5324 val loss 4.5777\n",
            "The current learning rate 0.00005\n",
            "Epoch 18509 train loss 4.5953 val loss 4.9063\n",
            "The current learning rate 0.00005\n",
            "Epoch 18510 train loss 4.5478 val loss 4.6188\n",
            "The current learning rate 0.00005\n",
            "Epoch 18511 train loss 4.5935 val loss 4.2457\n",
            "The current learning rate 0.00005\n",
            "Epoch 18512 train loss 4.6047 val loss 4.5987\n",
            "The current learning rate 0.00005\n",
            "Epoch 18513 train loss 4.6316 val loss 4.9113\n",
            "The current learning rate 0.00005\n",
            "Epoch 18514 train loss 4.8115 val loss 4.3438\n",
            "The current learning rate 0.00005\n",
            "Epoch 18515 train loss 4.4616 val loss 4.1547\n",
            "The current learning rate 0.00005\n",
            "Epoch 18516 train loss 4.8209 val loss 4.4877\n",
            "The current learning rate 0.00005\n",
            "Epoch 18517 train loss 4.5395 val loss 5.0261\n",
            "The current learning rate 0.00005\n",
            "Epoch 18518 train loss 4.5758 val loss 4.8541\n",
            "The current learning rate 0.00005\n",
            "Epoch 18519 train loss 4.8767 val loss 4.7082\n",
            "The current learning rate 0.00005\n",
            "Epoch 18520 train loss 4.8785 val loss 4.7214\n",
            "The current learning rate 0.00005\n",
            "Epoch 18521 train loss 4.5970 val loss 4.4615\n",
            "The current learning rate 0.00005\n",
            "Epoch 18522 train loss 4.8406 val loss 4.6161\n",
            "The current learning rate 0.00005\n",
            "Epoch 18523 train loss 5.1468 val loss 4.7353\n",
            "The current learning rate 0.00005\n",
            "Epoch 18524 train loss 4.6946 val loss 4.6204\n",
            "The current learning rate 0.00005\n",
            "Epoch 18525 train loss 4.5635 val loss 4.6779\n",
            "The current learning rate 0.00005\n",
            "Epoch 18526 train loss 4.5327 val loss 4.6273\n",
            "The current learning rate 0.00005\n",
            "Epoch 18527 train loss 4.6138 val loss 4.2342\n",
            "The current learning rate 0.00005\n",
            "Epoch 18528 train loss 4.6653 val loss 4.6032\n",
            "The current learning rate 0.00005\n",
            "Epoch 18529 train loss 4.6148 val loss 4.4354\n",
            "The current learning rate 0.00005\n",
            "Epoch 18530 train loss 4.2431 val loss 4.9402\n",
            "The current learning rate 0.00005\n",
            "Epoch 18531 train loss 4.5100 val loss 4.6096\n",
            "The current learning rate 0.00005\n",
            "Epoch 18532 train loss 4.5928 val loss 4.6430\n",
            "The current learning rate 0.00005\n",
            "Epoch 18533 train loss 4.4845 val loss 4.9234\n",
            "The current learning rate 0.00005\n",
            "Epoch 18534 train loss 4.8835 val loss 4.6545\n",
            "The current learning rate 0.00005\n",
            "Epoch 18535 train loss 4.5234 val loss 4.8545\n",
            "The current learning rate 0.00005\n",
            "Epoch 18536 train loss 4.6385 val loss 4.9328\n",
            "The current learning rate 0.00005\n",
            "Epoch 18537 train loss 4.7310 val loss 4.6032\n",
            "The current learning rate 0.00005\n",
            "Epoch 18538 train loss 4.7741 val loss 4.6899\n",
            "The current learning rate 0.00005\n",
            "Epoch 18539 train loss 4.5973 val loss 4.4391\n",
            "The current learning rate 0.00005\n",
            "Epoch 18540 train loss 4.2022 val loss 4.7616\n",
            "The current learning rate 0.00005\n",
            "Epoch 18541 train loss 4.5472 val loss 5.2077\n",
            "The current learning rate 0.00005\n",
            "Epoch 18542 train loss 4.6843 val loss 4.8001\n",
            "The current learning rate 0.00005\n",
            "Epoch 18543 train loss 4.6830 val loss 4.7215\n",
            "The current learning rate 0.00005\n",
            "Epoch 18544 train loss 4.7529 val loss 5.1400\n",
            "The current learning rate 0.00005\n",
            "Epoch 18545 train loss 4.6022 val loss 4.6441\n",
            "The current learning rate 0.00005\n",
            "Epoch 18546 train loss 4.6718 val loss 4.5478\n",
            "The current learning rate 0.00005\n",
            "Epoch 18547 train loss 4.7108 val loss 4.8055\n",
            "The current learning rate 0.00005\n",
            "Epoch 18548 train loss 4.5883 val loss 4.6188\n",
            "The current learning rate 0.00005\n",
            "Epoch 18549 train loss 4.6246 val loss 4.5921\n",
            "The current learning rate 0.00005\n",
            "Epoch 18550 train loss 4.4905 val loss 4.3504\n",
            "The current learning rate 0.00005\n",
            "Epoch 18551 train loss 4.8200 val loss 4.9070\n",
            "The current learning rate 0.00005\n",
            "Epoch 18552 train loss 4.6700 val loss 4.5398\n",
            "The current learning rate 0.00005\n",
            "Epoch 18553 train loss 4.8280 val loss 4.8023\n",
            "The current learning rate 0.00005\n",
            "Epoch 18554 train loss 4.4375 val loss 4.2984\n",
            "The current learning rate 0.00005\n",
            "Epoch 18555 train loss 4.6916 val loss 4.7186\n",
            "The current learning rate 0.00005\n",
            "Epoch 18556 train loss 4.4911 val loss 4.7708\n",
            "The current learning rate 0.00005\n",
            "Epoch 18557 train loss 5.0727 val loss 5.0101\n",
            "The current learning rate 0.00005\n",
            "Epoch 18558 train loss 4.7698 val loss 4.8848\n",
            "The current learning rate 0.00005\n",
            "Epoch 18559 train loss 4.3216 val loss 4.3343\n",
            "The current learning rate 0.00005\n",
            "Epoch 18560 train loss 4.5499 val loss 4.4693\n",
            "The current learning rate 0.00005\n",
            "Epoch 18561 train loss 4.7612 val loss 4.6594\n",
            "The current learning rate 0.00005\n",
            "Epoch 18562 train loss 4.6253 val loss 4.4896\n",
            "The current learning rate 0.00005\n",
            "Epoch 18563 train loss 4.5679 val loss 4.7510\n",
            "The current learning rate 0.00005\n",
            "Epoch 18564 train loss 4.5810 val loss 4.8471\n",
            "The current learning rate 0.00005\n",
            "Epoch 18565 train loss 4.6127 val loss 4.5730\n",
            "The current learning rate 0.00005\n",
            "Epoch 18566 train loss 4.9192 val loss 4.8076\n",
            "The current learning rate 0.00005\n",
            "Epoch 18567 train loss 4.7932 val loss 4.9717\n",
            "The current learning rate 0.00005\n",
            "Epoch 18568 train loss 4.7708 val loss 4.6042\n",
            "The current learning rate 0.00005\n",
            "Epoch 18569 train loss 4.6154 val loss 4.6883\n",
            "The current learning rate 0.00005\n",
            "Epoch 18570 train loss 4.7129 val loss 4.9164\n",
            "The current learning rate 0.00005\n",
            "Epoch 18571 train loss 4.7319 val loss 4.8182\n",
            "The current learning rate 0.00005\n",
            "Epoch 18572 train loss 4.6186 val loss 4.0340\n",
            "The current learning rate 0.00005\n",
            "Epoch 18573 train loss 4.7248 val loss 4.5010\n",
            "The current learning rate 0.00005\n",
            "Epoch 18574 train loss 4.2720 val loss 4.4968\n",
            "The current learning rate 0.00005\n",
            "Epoch 18575 train loss 4.4071 val loss 4.9148\n",
            "The current learning rate 0.00005\n",
            "Epoch 18576 train loss 4.6306 val loss 4.4825\n",
            "The current learning rate 0.00005\n",
            "Epoch 18577 train loss 4.7413 val loss 4.7353\n",
            "The current learning rate 0.00005\n",
            "Epoch 18578 train loss 4.5835 val loss 5.1253\n",
            "The current learning rate 0.00005\n",
            "Epoch 18579 train loss 4.5293 val loss 4.8155\n",
            "The current learning rate 0.00005\n",
            "Epoch 18580 train loss 4.6291 val loss 4.6576\n",
            "The current learning rate 0.00005\n",
            "Epoch 18581 train loss 4.9719 val loss 4.6787\n",
            "The current learning rate 0.00005\n",
            "Epoch 18582 train loss 4.6297 val loss 4.8007\n",
            "The current learning rate 0.00005\n",
            "Epoch 18583 train loss 4.8267 val loss 5.1014\n",
            "The current learning rate 0.00005\n",
            "Epoch 18584 train loss 4.5019 val loss 4.9051\n",
            "The current learning rate 0.00005\n",
            "Epoch 18585 train loss 4.5462 val loss 4.7540\n",
            "The current learning rate 0.00005\n",
            "Epoch 18586 train loss 4.8817 val loss 4.7945\n",
            "The current learning rate 0.00005\n",
            "Epoch 18587 train loss 4.3313 val loss 4.6749\n",
            "The current learning rate 0.00005\n",
            "Epoch 18588 train loss 4.5925 val loss 4.7229\n",
            "The current learning rate 0.00005\n",
            "Epoch 18589 train loss 4.5129 val loss 5.0508\n",
            "The current learning rate 0.00005\n",
            "Epoch 18590 train loss 4.4781 val loss 4.5017\n",
            "The current learning rate 0.00005\n",
            "Epoch 18591 train loss 4.7775 val loss 4.7946\n",
            "The current learning rate 0.00005\n",
            "Epoch 18592 train loss 4.8134 val loss 4.5655\n",
            "The current learning rate 0.00005\n",
            "Epoch 18593 train loss 5.0682 val loss 5.0160\n",
            "The current learning rate 0.00005\n",
            "Epoch 18594 train loss 4.6741 val loss 4.9357\n",
            "The current learning rate 0.00005\n",
            "Epoch 18595 train loss 4.5590 val loss 4.6950\n",
            "The current learning rate 0.00005\n",
            "Epoch 18596 train loss 4.3384 val loss 4.8354\n",
            "The current learning rate 0.00005\n",
            "Epoch 18597 train loss 4.6286 val loss 4.5322\n",
            "The current learning rate 0.00005\n",
            "Epoch 18598 train loss 4.6715 val loss 4.5574\n",
            "The current learning rate 0.00005\n",
            "Epoch 18599 train loss 4.7744 val loss 4.3880\n",
            "The current learning rate 0.00005\n",
            "Epoch 18600 train loss 4.4357 val loss 4.7223\n",
            "The current learning rate 0.00005\n",
            "Epoch 18601 train loss 4.8314 val loss 4.6959\n",
            "The current learning rate 0.00005\n",
            "Epoch 18602 train loss 4.4173 val loss 4.5277\n",
            "The current learning rate 0.00005\n",
            "Epoch 18603 train loss 4.2055 val loss 4.5511\n",
            "The current learning rate 0.00005\n",
            "Epoch 18604 train loss 4.5371 val loss 4.4930\n",
            "The current learning rate 0.00005\n",
            "Epoch 18605 train loss 4.7168 val loss 4.1151\n",
            "The current learning rate 0.00005\n",
            "Epoch 18606 train loss 4.3577 val loss 4.7150\n",
            "The current learning rate 0.00005\n",
            "Epoch 18607 train loss 4.5633 val loss 4.4374\n",
            "The current learning rate 0.00005\n",
            "Epoch 18608 train loss 4.5564 val loss 4.9272\n",
            "The current learning rate 0.00005\n",
            "Epoch 18609 train loss 4.7464 val loss 4.4621\n",
            "The current learning rate 0.00005\n",
            "Epoch 18610 train loss 4.6496 val loss 4.7597\n",
            "The current learning rate 0.00005\n",
            "Epoch 18611 train loss 4.6817 val loss 4.5338\n",
            "The current learning rate 0.00005\n",
            "Epoch 18612 train loss 4.4502 val loss 4.4739\n",
            "The current learning rate 0.00005\n",
            "Epoch 18613 train loss 4.7615 val loss 4.7602\n",
            "The current learning rate 0.00005\n",
            "Epoch 18614 train loss 5.0076 val loss 4.5516\n",
            "The current learning rate 0.00005\n",
            "Epoch 18615 train loss 4.5940 val loss 4.7311\n",
            "The current learning rate 0.00005\n",
            "Epoch 18616 train loss 4.8004 val loss 4.7324\n",
            "The current learning rate 0.00005\n",
            "Epoch 18617 train loss 4.2463 val loss 4.6932\n",
            "The current learning rate 0.00005\n",
            "Epoch 18618 train loss 5.1865 val loss 4.4107\n",
            "The current learning rate 0.00005\n",
            "Epoch 18619 train loss 4.5735 val loss 4.7958\n",
            "The current learning rate 0.00005\n",
            "Epoch 18620 train loss 4.7648 val loss 4.7795\n",
            "The current learning rate 0.00005\n",
            "Epoch 18621 train loss 5.1163 val loss 5.2108\n",
            "The current learning rate 0.00005\n",
            "Epoch 18622 train loss 4.6075 val loss 4.2604\n",
            "The current learning rate 0.00005\n",
            "Epoch 18623 train loss 4.8846 val loss 4.5415\n",
            "The current learning rate 0.00005\n",
            "Epoch 18624 train loss 4.7602 val loss 4.5570\n",
            "The current learning rate 0.00005\n",
            "Epoch 18625 train loss 4.6776 val loss 4.8131\n",
            "The current learning rate 0.00005\n",
            "Epoch 18626 train loss 4.7456 val loss 4.3952\n",
            "The current learning rate 0.00005\n",
            "Epoch 18627 train loss 4.7374 val loss 4.8255\n",
            "The current learning rate 0.00005\n",
            "Epoch 18628 train loss 4.3933 val loss 4.3680\n",
            "The current learning rate 0.00005\n",
            "Epoch 18629 train loss 4.4591 val loss 4.6588\n",
            "The current learning rate 0.00005\n",
            "Epoch 18630 train loss 5.0844 val loss 4.9718\n",
            "The current learning rate 0.00005\n",
            "Epoch 18631 train loss 4.3696 val loss 4.8604\n",
            "The current learning rate 0.00005\n",
            "Epoch 18632 train loss 5.1508 val loss 4.6542\n",
            "The current learning rate 0.00005\n",
            "Epoch 18633 train loss 4.8598 val loss 4.8483\n",
            "The current learning rate 0.00005\n",
            "Epoch 18634 train loss 4.6290 val loss 4.7467\n",
            "The current learning rate 0.00005\n",
            "Epoch 18635 train loss 4.5826 val loss 4.3955\n",
            "The current learning rate 0.00005\n",
            "Epoch 18636 train loss 4.5113 val loss 4.4770\n",
            "The current learning rate 0.00005\n",
            "Epoch 18637 train loss 4.7950 val loss 4.4212\n",
            "The current learning rate 0.00005\n",
            "Epoch 18638 train loss 4.6441 val loss 4.4872\n",
            "The current learning rate 0.00005\n",
            "Epoch 18639 train loss 4.8131 val loss 4.6673\n",
            "The current learning rate 0.00005\n",
            "Epoch 18640 train loss 4.3952 val loss 4.6128\n",
            "The current learning rate 0.00005\n",
            "Epoch 18641 train loss 4.8717 val loss 4.4792\n",
            "The current learning rate 0.00005\n",
            "Epoch 18642 train loss 4.4998 val loss 4.4385\n",
            "The current learning rate 0.00005\n",
            "Epoch 18643 train loss 4.4740 val loss 4.5505\n",
            "The current learning rate 0.00005\n",
            "Epoch 18644 train loss 4.6826 val loss 4.8065\n",
            "The current learning rate 0.00005\n",
            "Epoch 18645 train loss 4.5875 val loss 4.7946\n",
            "The current learning rate 0.00005\n",
            "Epoch 18646 train loss 4.5320 val loss 4.5912\n",
            "The current learning rate 0.00005\n",
            "Epoch 18647 train loss 4.6203 val loss 4.6789\n",
            "The current learning rate 0.00005\n",
            "Epoch 18648 train loss 4.5233 val loss 4.6488\n",
            "The current learning rate 0.00005\n",
            "Epoch 18649 train loss 4.3036 val loss 4.5801\n",
            "The current learning rate 0.00005\n",
            "Epoch 18650 train loss 4.4947 val loss 4.7374\n",
            "The current learning rate 0.00005\n",
            "Epoch 18651 train loss 4.4890 val loss 4.2404\n",
            "The current learning rate 0.00005\n",
            "Epoch 18652 train loss 4.7053 val loss 4.7801\n",
            "The current learning rate 0.00005\n",
            "Epoch 18653 train loss 4.7863 val loss 4.7199\n",
            "The current learning rate 0.00005\n",
            "Epoch 18654 train loss 4.5427 val loss 4.5673\n",
            "The current learning rate 0.00005\n",
            "Epoch 18655 train loss 5.0199 val loss 4.8767\n",
            "The current learning rate 0.00005\n",
            "Epoch 18656 train loss 5.0049 val loss 4.8488\n",
            "The current learning rate 0.00005\n",
            "Epoch 18657 train loss 4.5896 val loss 4.6646\n",
            "The current learning rate 0.00005\n",
            "Epoch 18658 train loss 4.8669 val loss 4.5553\n",
            "The current learning rate 0.00005\n",
            "Epoch 18659 train loss 4.7005 val loss 4.6127\n",
            "The current learning rate 0.00005\n",
            "Epoch 18660 train loss 4.6617 val loss 4.7796\n",
            "The current learning rate 0.00005\n",
            "Epoch 18661 train loss 4.6680 val loss 4.6476\n",
            "The current learning rate 0.00005\n",
            "Epoch 18662 train loss 4.7839 val loss 4.8043\n",
            "The current learning rate 0.00005\n",
            "Epoch 18663 train loss 4.8799 val loss 4.9246\n",
            "The current learning rate 0.00005\n",
            "Epoch 18664 train loss 4.7072 val loss 4.5539\n",
            "The current learning rate 0.00005\n",
            "Epoch 18665 train loss 4.7013 val loss 5.1823\n",
            "The current learning rate 0.00005\n",
            "Epoch 18666 train loss 5.2557 val loss 5.0955\n",
            "The current learning rate 0.00005\n",
            "Epoch 18667 train loss 5.2628 val loss 4.7873\n",
            "The current learning rate 0.00005\n",
            "Epoch 18668 train loss 4.5640 val loss 4.3724\n",
            "The current learning rate 0.00005\n",
            "Epoch 18669 train loss 4.4252 val loss 4.6732\n",
            "The current learning rate 0.00005\n",
            "Epoch 18670 train loss 4.6985 val loss 4.5598\n",
            "The current learning rate 0.00005\n",
            "Epoch 18671 train loss 4.7363 val loss 4.5453\n",
            "The current learning rate 0.00005\n",
            "Epoch 18672 train loss 5.0972 val loss 4.5234\n",
            "The current learning rate 0.00005\n",
            "Epoch 18673 train loss 4.4157 val loss 4.7290\n",
            "The current learning rate 0.00005\n",
            "Epoch 18674 train loss 4.8799 val loss 4.6634\n",
            "The current learning rate 0.00005\n",
            "Epoch 18675 train loss 4.5532 val loss 4.4722\n",
            "The current learning rate 0.00005\n",
            "Epoch 18676 train loss 4.5740 val loss 4.7823\n",
            "The current learning rate 0.00005\n",
            "Epoch 18677 train loss 4.7939 val loss 4.6313\n",
            "The current learning rate 0.00005\n",
            "Epoch 18678 train loss 4.7322 val loss 4.5453\n",
            "The current learning rate 0.00005\n",
            "Epoch 18679 train loss 4.8902 val loss 5.1067\n",
            "The current learning rate 0.00005\n",
            "Epoch 18680 train loss 5.0154 val loss 4.9318\n",
            "The current learning rate 0.00005\n",
            "Epoch 18681 train loss 4.7254 val loss 4.4436\n",
            "The current learning rate 0.00005\n",
            "Epoch 18682 train loss 4.8388 val loss 4.5003\n",
            "The current learning rate 0.00005\n",
            "Epoch 18683 train loss 4.7037 val loss 4.8063\n",
            "The current learning rate 0.00005\n",
            "Epoch 18684 train loss 4.5986 val loss 4.4521\n",
            "The current learning rate 0.00005\n",
            "Epoch 18685 train loss 4.8697 val loss 4.9287\n",
            "The current learning rate 0.00005\n",
            "Epoch 18686 train loss 4.5803 val loss 4.7720\n",
            "The current learning rate 0.00005\n",
            "Epoch 18687 train loss 4.9846 val loss 4.8299\n",
            "The current learning rate 0.00005\n",
            "Epoch 18688 train loss 4.5405 val loss 4.8074\n",
            "The current learning rate 0.00005\n",
            "Epoch 18689 train loss 4.5150 val loss 4.7805\n",
            "The current learning rate 0.00005\n",
            "Epoch 18690 train loss 4.8135 val loss 4.3091\n",
            "The current learning rate 0.00005\n",
            "Epoch 18691 train loss 4.3675 val loss 4.4377\n",
            "The current learning rate 0.00005\n",
            "Epoch 18692 train loss 4.4719 val loss 4.5818\n",
            "The current learning rate 0.00005\n",
            "Epoch 18693 train loss 4.6047 val loss 4.6848\n",
            "The current learning rate 0.00005\n",
            "Epoch 18694 train loss 4.7316 val loss 4.6663\n",
            "The current learning rate 0.00005\n",
            "Epoch 18695 train loss 4.5280 val loss 4.6803\n",
            "The current learning rate 0.00005\n",
            "Epoch 18696 train loss 5.0366 val loss 4.5606\n",
            "The current learning rate 0.00005\n",
            "Epoch 18697 train loss 4.8309 val loss 4.5934\n",
            "The current learning rate 0.00005\n",
            "Epoch 18698 train loss 4.4209 val loss 5.2328\n",
            "The current learning rate 0.00005\n",
            "Epoch 18699 train loss 4.5826 val loss 4.6360\n",
            "The current learning rate 0.00005\n",
            "Epoch 18700 train loss 4.7740 val loss 4.5662\n",
            "The current learning rate 0.00005\n",
            "Epoch 18701 train loss 4.2382 val loss 4.8548\n",
            "The current learning rate 0.00005\n",
            "Epoch 18702 train loss 4.8047 val loss 4.7162\n",
            "The current learning rate 0.00005\n",
            "Epoch 18703 train loss 4.5330 val loss 4.6261\n",
            "The current learning rate 0.00005\n",
            "Epoch 18704 train loss 4.8498 val loss 4.5983\n",
            "The current learning rate 0.00005\n",
            "Epoch 18705 train loss 4.7597 val loss 4.4881\n",
            "The current learning rate 0.00005\n",
            "Epoch 18706 train loss 4.3793 val loss 4.9070\n",
            "The current learning rate 0.00005\n",
            "Epoch 18707 train loss 4.7640 val loss 4.1834\n",
            "The current learning rate 0.00005\n",
            "Epoch 18708 train loss 4.7534 val loss 4.8394\n",
            "The current learning rate 0.00005\n",
            "Epoch 18709 train loss 4.6366 val loss 4.9047\n",
            "The current learning rate 0.00005\n",
            "Epoch 18710 train loss 4.7889 val loss 4.3873\n",
            "The current learning rate 0.00005\n",
            "Epoch 18711 train loss 4.9921 val loss 4.8925\n",
            "The current learning rate 0.00005\n",
            "Epoch 18712 train loss 4.2642 val loss 4.8892\n",
            "The current learning rate 0.00005\n",
            "Epoch 18713 train loss 4.5348 val loss 4.6601\n",
            "The current learning rate 0.00005\n",
            "Epoch 18714 train loss 4.6336 val loss 4.8737\n",
            "The current learning rate 0.00005\n",
            "Epoch 18715 train loss 4.7674 val loss 4.7758\n",
            "The current learning rate 0.00005\n",
            "Epoch 18716 train loss 4.5911 val loss 4.4663\n",
            "The current learning rate 0.00005\n",
            "Epoch 18717 train loss 4.7425 val loss 4.6181\n",
            "The current learning rate 0.00005\n",
            "Epoch 18718 train loss 4.5205 val loss 4.4050\n",
            "The current learning rate 0.00005\n",
            "Epoch 18719 train loss 4.8205 val loss 4.7014\n",
            "The current learning rate 0.00005\n",
            "Epoch 18720 train loss 4.5859 val loss 4.4711\n",
            "The current learning rate 0.00005\n",
            "Epoch 18721 train loss 4.8405 val loss 4.5885\n",
            "The current learning rate 0.00005\n",
            "Epoch 18722 train loss 4.7639 val loss 5.0034\n",
            "The current learning rate 0.00005\n",
            "Epoch 18723 train loss 4.5222 val loss 4.8052\n",
            "The current learning rate 0.00005\n",
            "Epoch 18724 train loss 4.7579 val loss 4.7303\n",
            "The current learning rate 0.00005\n",
            "Epoch 18725 train loss 4.1782 val loss 4.4804\n",
            "The current learning rate 0.00005\n",
            "Epoch 18726 train loss 4.6032 val loss 4.5040\n",
            "The current learning rate 0.00005\n",
            "Epoch 18727 train loss 4.6384 val loss 4.1234\n",
            "The current learning rate 0.00005\n",
            "Epoch 18728 train loss 4.8136 val loss 4.5318\n",
            "The current learning rate 0.00005\n",
            "Epoch 18729 train loss 4.4188 val loss 4.6100\n",
            "The current learning rate 0.00005\n",
            "Epoch 18730 train loss 5.0485 val loss 4.8925\n",
            "The current learning rate 0.00005\n",
            "Epoch 18731 train loss 4.9093 val loss 4.4645\n",
            "The current learning rate 0.00005\n",
            "Epoch 18732 train loss 4.9467 val loss 4.8752\n",
            "The current learning rate 0.00005\n",
            "Epoch 18733 train loss 4.9542 val loss 4.6588\n",
            "The current learning rate 0.00005\n",
            "Epoch 18734 train loss 4.6248 val loss 4.4671\n",
            "The current learning rate 0.00005\n",
            "Epoch 18735 train loss 4.5822 val loss 4.5390\n",
            "The current learning rate 0.00005\n",
            "Epoch 18736 train loss 4.6690 val loss 4.7065\n",
            "The current learning rate 0.00005\n",
            "Epoch 18737 train loss 4.6265 val loss 4.6193\n",
            "The current learning rate 0.00005\n",
            "Epoch 18738 train loss 5.2161 val loss 4.9164\n",
            "The current learning rate 0.00005\n",
            "Epoch 18739 train loss 4.6478 val loss 4.8856\n",
            "The current learning rate 0.00005\n",
            "Epoch 18740 train loss 4.2981 val loss 4.5484\n",
            "The current learning rate 0.00005\n",
            "Epoch 18741 train loss 4.7356 val loss 4.6079\n",
            "The current learning rate 0.00005\n",
            "Epoch 18742 train loss 4.4284 val loss 4.5931\n",
            "The current learning rate 0.00005\n",
            "Epoch 18743 train loss 4.4778 val loss 4.5633\n",
            "The current learning rate 0.00005\n",
            "Epoch 18744 train loss 4.4436 val loss 4.5827\n",
            "The current learning rate 0.00005\n",
            "Epoch 18745 train loss 4.7333 val loss 4.4009\n",
            "The current learning rate 0.00005\n",
            "Epoch 18746 train loss 4.6302 val loss 5.0226\n",
            "The current learning rate 0.00005\n",
            "Epoch 18747 train loss 4.5755 val loss 4.3189\n",
            "The current learning rate 0.00005\n",
            "Epoch 18748 train loss 4.5894 val loss 4.6068\n",
            "The current learning rate 0.00005\n",
            "Epoch 18749 train loss 4.5218 val loss 4.2615\n",
            "The current learning rate 0.00005\n",
            "Epoch 18750 train loss 4.6006 val loss 4.4375\n",
            "The current learning rate 0.00005\n",
            "Epoch 18751 train loss 5.1481 val loss 4.2879\n",
            "The current learning rate 0.00005\n",
            "Epoch 18752 train loss 4.5951 val loss 4.7001\n",
            "The current learning rate 0.00005\n",
            "Epoch 18753 train loss 4.8644 val loss 4.5777\n",
            "The current learning rate 0.00005\n",
            "Epoch 18754 train loss 4.9480 val loss 4.7744\n",
            "The current learning rate 0.00005\n",
            "Epoch 18755 train loss 4.3304 val loss 4.6446\n",
            "The current learning rate 0.00005\n",
            "Epoch 18756 train loss 5.0887 val loss 4.7595\n",
            "The current learning rate 0.00005\n",
            "Epoch 18757 train loss 4.7438 val loss 4.4144\n",
            "The current learning rate 0.00005\n",
            "Epoch 18758 train loss 4.9270 val loss 4.2577\n",
            "The current learning rate 0.00005\n",
            "Epoch 18759 train loss 4.5582 val loss 4.8028\n",
            "The current learning rate 0.00005\n",
            "Epoch 18760 train loss 4.7292 val loss 4.4286\n",
            "The current learning rate 0.00005\n",
            "Epoch 18761 train loss 4.0101 val loss 4.9084\n",
            "The current learning rate 0.00005\n",
            "Epoch 18762 train loss 4.8017 val loss 4.7892\n",
            "The current learning rate 0.00005\n",
            "Epoch 18763 train loss 4.5772 val loss 4.9264\n",
            "The current learning rate 0.00005\n",
            "Epoch 18764 train loss 4.4426 val loss 4.5585\n",
            "The current learning rate 0.00005\n",
            "Epoch 18765 train loss 4.3194 val loss 4.3417\n",
            "The current learning rate 0.00005\n",
            "Epoch 18766 train loss 4.5558 val loss 4.6749\n",
            "The current learning rate 0.00005\n",
            "Epoch 18767 train loss 4.8504 val loss 4.5517\n",
            "The current learning rate 0.00005\n",
            "Epoch 18768 train loss 4.5266 val loss 4.7102\n",
            "The current learning rate 0.00005\n",
            "Epoch 18769 train loss 4.5900 val loss 4.4630\n",
            "The current learning rate 0.00005\n",
            "Epoch 18770 train loss 4.5683 val loss 4.4732\n",
            "The current learning rate 0.00005\n",
            "Epoch 18771 train loss 4.3728 val loss 4.5006\n",
            "The current learning rate 0.00005\n",
            "Epoch 18772 train loss 4.5564 val loss 4.3014\n",
            "The current learning rate 0.00005\n",
            "Epoch 18773 train loss 4.7000 val loss 4.6285\n",
            "The current learning rate 0.00005\n",
            "Epoch 18774 train loss 4.7293 val loss 4.1691\n",
            "The current learning rate 0.00005\n",
            "Epoch 18775 train loss 4.5122 val loss 4.9467\n",
            "The current learning rate 0.00005\n",
            "Epoch 18776 train loss 4.4951 val loss 4.5973\n",
            "The current learning rate 0.00005\n",
            "Epoch 18777 train loss 4.6151 val loss 4.9413\n",
            "The current learning rate 0.00005\n",
            "Epoch 18778 train loss 4.5595 val loss 4.7438\n",
            "The current learning rate 0.00005\n",
            "Epoch 18779 train loss 4.3562 val loss 4.5829\n",
            "The current learning rate 0.00005\n",
            "Epoch 18780 train loss 4.7744 val loss 4.4197\n",
            "The current learning rate 0.00005\n",
            "Epoch 18781 train loss 5.1063 val loss 4.6957\n",
            "The current learning rate 0.00005\n",
            "Epoch 18782 train loss 4.6440 val loss 5.1501\n",
            "The current learning rate 0.00005\n",
            "Epoch 18783 train loss 4.5950 val loss 4.6426\n",
            "The current learning rate 0.00005\n",
            "Epoch 18784 train loss 5.2172 val loss 4.6294\n",
            "The current learning rate 0.00005\n",
            "Epoch 18785 train loss 4.6487 val loss 4.6311\n",
            "The current learning rate 0.00005\n",
            "Epoch 18786 train loss 4.6241 val loss 4.4991\n",
            "The current learning rate 0.00005\n",
            "Epoch 18787 train loss 4.8163 val loss 4.6040\n",
            "The current learning rate 0.00005\n",
            "Epoch 18788 train loss 4.3521 val loss 4.7596\n",
            "The current learning rate 0.00005\n",
            "Epoch 18789 train loss 4.6668 val loss 4.2434\n",
            "The current learning rate 0.00005\n",
            "Epoch 18790 train loss 4.3840 val loss 3.8297\n",
            "The current learning rate 0.00005\n",
            "Epoch 18791 train loss 4.6829 val loss 4.7623\n",
            "The current learning rate 0.00005\n",
            "Epoch 18792 train loss 4.7452 val loss 4.3757\n",
            "The current learning rate 0.00005\n",
            "Epoch 18793 train loss 4.8780 val loss 4.3313\n",
            "The current learning rate 0.00005\n",
            "Epoch 18794 train loss 4.5190 val loss 5.0392\n",
            "The current learning rate 0.00005\n",
            "Epoch 18795 train loss 4.4318 val loss 4.6804\n",
            "The current learning rate 0.00005\n",
            "Epoch 18796 train loss 4.9908 val loss 5.0644\n",
            "The current learning rate 0.00005\n",
            "Epoch 18797 train loss 4.8607 val loss 4.6315\n",
            "The current learning rate 0.00005\n",
            "Epoch 18798 train loss 4.7106 val loss 4.5156\n",
            "The current learning rate 0.00005\n",
            "Epoch 18799 train loss 4.5692 val loss 4.6177\n",
            "The current learning rate 0.00005\n",
            "Epoch 18800 train loss 4.5239 val loss 5.0602\n",
            "The current learning rate 0.00005\n",
            "Epoch 18801 train loss 4.4870 val loss 4.5228\n",
            "The current learning rate 0.00005\n",
            "Epoch 18802 train loss 4.7950 val loss 4.6537\n",
            "The current learning rate 0.00005\n",
            "Epoch 18803 train loss 4.5144 val loss 5.0171\n",
            "The current learning rate 0.00005\n",
            "Epoch 18804 train loss 4.9209 val loss 4.4946\n",
            "The current learning rate 0.00005\n",
            "Epoch 18805 train loss 4.9347 val loss 4.8916\n",
            "The current learning rate 0.00005\n",
            "Epoch 18806 train loss 4.7550 val loss 5.3089\n",
            "The current learning rate 0.00005\n",
            "Epoch 18807 train loss 4.6202 val loss 4.3827\n",
            "The current learning rate 0.00005\n",
            "Epoch 18808 train loss 4.3581 val loss 4.8298\n",
            "The current learning rate 0.00005\n",
            "Epoch 18809 train loss 4.9103 val loss 4.5217\n",
            "The current learning rate 0.00005\n",
            "Epoch 18810 train loss 4.4565 val loss 4.7655\n",
            "The current learning rate 0.00005\n",
            "Epoch 18811 train loss 4.5717 val loss 4.5386\n",
            "The current learning rate 0.00005\n",
            "Epoch 18812 train loss 4.5722 val loss 4.8410\n",
            "The current learning rate 0.00005\n",
            "Epoch 18813 train loss 4.4739 val loss 4.8232\n",
            "The current learning rate 0.00005\n",
            "Epoch 18814 train loss 4.8632 val loss 4.7314\n",
            "The current learning rate 0.00005\n",
            "Epoch 18815 train loss 4.5102 val loss 4.7925\n",
            "The current learning rate 0.00005\n",
            "Epoch 18816 train loss 4.6317 val loss 4.6547\n",
            "The current learning rate 0.00005\n",
            "Epoch 18817 train loss 4.2158 val loss 4.5498\n",
            "The current learning rate 0.00005\n",
            "Epoch 18818 train loss 4.9404 val loss 4.6023\n",
            "The current learning rate 0.00005\n",
            "Epoch 18819 train loss 4.5691 val loss 4.5795\n",
            "The current learning rate 0.00005\n",
            "Epoch 18820 train loss 4.4708 val loss 4.6870\n",
            "The current learning rate 0.00005\n",
            "Epoch 18821 train loss 4.6566 val loss 5.0049\n",
            "The current learning rate 0.00005\n",
            "Epoch 18822 train loss 4.6552 val loss 4.6021\n",
            "The current learning rate 0.00005\n",
            "Epoch 18823 train loss 4.9922 val loss 4.3198\n",
            "The current learning rate 0.00005\n",
            "Epoch 18824 train loss 4.2550 val loss 4.6149\n",
            "The current learning rate 0.00005\n",
            "Epoch 18825 train loss 4.7923 val loss 4.7679\n",
            "The current learning rate 0.00005\n",
            "Epoch 18826 train loss 4.5175 val loss 4.6434\n",
            "The current learning rate 0.00005\n",
            "Epoch 18827 train loss 4.3555 val loss 4.5840\n",
            "The current learning rate 0.00005\n",
            "Epoch 18828 train loss 4.0957 val loss 4.4686\n",
            "The current learning rate 0.00005\n",
            "Epoch 18829 train loss 4.4617 val loss 5.0020\n",
            "The current learning rate 0.00005\n",
            "Epoch 18830 train loss 4.8693 val loss 4.8183\n",
            "The current learning rate 0.00005\n",
            "Epoch 18831 train loss 4.3588 val loss 4.4730\n",
            "The current learning rate 0.00005\n",
            "Epoch 18832 train loss 4.4851 val loss 4.8484\n",
            "The current learning rate 0.00005\n",
            "Epoch 18833 train loss 4.6330 val loss 4.8035\n",
            "The current learning rate 0.00005\n",
            "Epoch 18834 train loss 4.4062 val loss 4.4344\n",
            "The current learning rate 0.00005\n",
            "Epoch 18835 train loss 4.6079 val loss 4.5648\n",
            "The current learning rate 0.00005\n",
            "Epoch 18836 train loss 4.8098 val loss 4.1790\n",
            "The current learning rate 0.00005\n",
            "Epoch 18837 train loss 4.9221 val loss 4.6732\n",
            "The current learning rate 0.00005\n",
            "Epoch 18838 train loss 4.7168 val loss 4.3888\n",
            "The current learning rate 0.00005\n",
            "Epoch 18839 train loss 4.6253 val loss 4.7908\n",
            "The current learning rate 0.00005\n",
            "Epoch 18840 train loss 4.6150 val loss 4.4467\n",
            "The current learning rate 0.00005\n",
            "Epoch 18841 train loss 4.5051 val loss 4.4012\n",
            "The current learning rate 0.00005\n",
            "Epoch 18842 train loss 4.9625 val loss 4.6187\n",
            "The current learning rate 0.00005\n",
            "Epoch 18843 train loss 5.1182 val loss 4.5245\n",
            "The current learning rate 0.00005\n",
            "Epoch 18844 train loss 5.0559 val loss 4.6955\n",
            "The current learning rate 0.00005\n",
            "Epoch 18845 train loss 4.7182 val loss 4.5613\n",
            "The current learning rate 0.00005\n",
            "Epoch 18846 train loss 4.7211 val loss 4.8460\n",
            "The current learning rate 0.00005\n",
            "Epoch 18847 train loss 4.8646 val loss 4.9125\n",
            "The current learning rate 0.00005\n",
            "Epoch 18848 train loss 4.8245 val loss 4.4802\n",
            "The current learning rate 0.00005\n",
            "Epoch 18849 train loss 4.6847 val loss 4.7286\n",
            "The current learning rate 0.00005\n",
            "Epoch 18850 train loss 4.9510 val loss 4.8512\n",
            "The current learning rate 0.00005\n",
            "Epoch 18851 train loss 4.3867 val loss 4.9021\n",
            "The current learning rate 0.00005\n",
            "Epoch 18852 train loss 4.4749 val loss 4.5882\n",
            "The current learning rate 0.00005\n",
            "Epoch 18853 train loss 4.7183 val loss 4.8440\n",
            "The current learning rate 0.00005\n",
            "Epoch 18854 train loss 4.4548 val loss 4.7507\n",
            "The current learning rate 0.00005\n",
            "Epoch 18855 train loss 4.5486 val loss 4.7592\n",
            "The current learning rate 0.00005\n",
            "Epoch 18856 train loss 4.5471 val loss 4.7595\n",
            "The current learning rate 0.00005\n",
            "Epoch 18857 train loss 5.0324 val loss 4.8017\n",
            "The current learning rate 0.00005\n",
            "Epoch 18858 train loss 4.5649 val loss 4.6968\n",
            "The current learning rate 0.00005\n",
            "Epoch 18859 train loss 4.6702 val loss 4.6557\n",
            "The current learning rate 0.00005\n",
            "Epoch 18860 train loss 4.6764 val loss 4.4413\n",
            "The current learning rate 0.00005\n",
            "Epoch 18861 train loss 4.6521 val loss 4.5894\n",
            "The current learning rate 0.00005\n",
            "Epoch 18862 train loss 4.5582 val loss 4.7277\n",
            "The current learning rate 0.00005\n",
            "Epoch 18863 train loss 4.2764 val loss 4.4745\n",
            "The current learning rate 0.00005\n",
            "Epoch 18864 train loss 4.8837 val loss 5.0638\n",
            "The current learning rate 0.00005\n",
            "Epoch 18865 train loss 4.7487 val loss 4.3412\n",
            "The current learning rate 0.00005\n",
            "Epoch 18866 train loss 4.5044 val loss 4.5780\n",
            "The current learning rate 0.00005\n",
            "Epoch 18867 train loss 4.8311 val loss 4.5266\n",
            "The current learning rate 0.00005\n",
            "Epoch 18868 train loss 4.8192 val loss 4.7068\n",
            "The current learning rate 0.00005\n",
            "Epoch 18869 train loss 4.7934 val loss 4.7527\n",
            "The current learning rate 0.00005\n",
            "Epoch 18870 train loss 4.5069 val loss 4.6392\n",
            "The current learning rate 0.00005\n",
            "Epoch 18871 train loss 4.7756 val loss 4.9020\n",
            "The current learning rate 0.00005\n",
            "Epoch 18872 train loss 4.8542 val loss 4.6642\n",
            "The current learning rate 0.00005\n",
            "Epoch 18873 train loss 4.6384 val loss 4.7278\n",
            "The current learning rate 0.00005\n",
            "Epoch 18874 train loss 4.7541 val loss 5.0213\n",
            "The current learning rate 0.00005\n",
            "Epoch 18875 train loss 4.8110 val loss 4.6050\n",
            "The current learning rate 0.00005\n",
            "Epoch 18876 train loss 4.3714 val loss 4.4793\n",
            "The current learning rate 0.00005\n",
            "Epoch 18877 train loss 4.5327 val loss 4.6886\n",
            "The current learning rate 0.00005\n",
            "Epoch 18878 train loss 4.7906 val loss 4.5122\n",
            "The current learning rate 0.00005\n",
            "Epoch 18879 train loss 4.7963 val loss 4.7376\n",
            "The current learning rate 0.00005\n",
            "Epoch 18880 train loss 4.6097 val loss 4.8842\n",
            "The current learning rate 0.00005\n",
            "Epoch 18881 train loss 4.6194 val loss 4.2653\n",
            "The current learning rate 0.00005\n",
            "Epoch 18882 train loss 4.6716 val loss 4.7336\n",
            "The current learning rate 0.00005\n",
            "Epoch 18883 train loss 4.6346 val loss 4.8061\n",
            "The current learning rate 0.00005\n",
            "Epoch 18884 train loss 4.6434 val loss 4.6962\n",
            "The current learning rate 0.00005\n",
            "Epoch 18885 train loss 4.8072 val loss 4.8898\n",
            "The current learning rate 0.00005\n",
            "Epoch 18886 train loss 4.7113 val loss 4.7553\n",
            "The current learning rate 0.00005\n",
            "Epoch 18887 train loss 4.6261 val loss 4.5215\n",
            "The current learning rate 0.00005\n",
            "Epoch 18888 train loss 4.3327 val loss 4.6959\n",
            "The current learning rate 0.00005\n",
            "Epoch 18889 train loss 4.6445 val loss 5.0507\n",
            "The current learning rate 0.00005\n",
            "Epoch 18890 train loss 4.6787 val loss 5.0957\n",
            "The current learning rate 0.00005\n",
            "Epoch 18891 train loss 4.9428 val loss 4.5479\n",
            "The current learning rate 0.00005\n",
            "Epoch 18892 train loss 4.8615 val loss 4.7061\n",
            "The current learning rate 0.00005\n",
            "Epoch 18893 train loss 4.5891 val loss 4.6195\n",
            "The current learning rate 0.00005\n",
            "Epoch 18894 train loss 4.5029 val loss 4.6706\n",
            "The current learning rate 0.00005\n",
            "Epoch 18895 train loss 4.7756 val loss 4.7111\n",
            "The current learning rate 0.00005\n",
            "Epoch 18896 train loss 4.3973 val loss 4.3865\n",
            "The current learning rate 0.00005\n",
            "Epoch 18897 train loss 5.0023 val loss 4.2775\n",
            "The current learning rate 0.00005\n",
            "Epoch 18898 train loss 4.8690 val loss 4.6320\n",
            "The current learning rate 0.00005\n",
            "Epoch 18899 train loss 4.6732 val loss 4.6405\n",
            "The current learning rate 0.00005\n",
            "Epoch 18900 train loss 4.8236 val loss 4.8444\n",
            "The current learning rate 0.00005\n",
            "Epoch 18901 train loss 4.5770 val loss 4.4254\n",
            "The current learning rate 0.00005\n",
            "Epoch 18902 train loss 4.5948 val loss 4.6075\n",
            "The current learning rate 0.00005\n",
            "Epoch 18903 train loss 5.0701 val loss 4.9415\n",
            "The current learning rate 0.00005\n",
            "Epoch 18904 train loss 4.5967 val loss 3.8897\n",
            "The current learning rate 0.00005\n",
            "Epoch 18905 train loss 4.6343 val loss 4.7546\n",
            "The current learning rate 0.00005\n",
            "Epoch 18906 train loss 4.7322 val loss 4.7004\n",
            "The current learning rate 0.00005\n",
            "Epoch 18907 train loss 4.4363 val loss 4.4164\n",
            "The current learning rate 0.00005\n",
            "Epoch 18908 train loss 4.6777 val loss 4.9629\n",
            "The current learning rate 0.00005\n",
            "Epoch 18909 train loss 4.2306 val loss 4.4641\n",
            "The current learning rate 0.00005\n",
            "Epoch 18910 train loss 4.4209 val loss 4.2467\n",
            "The current learning rate 0.00005\n",
            "Epoch 18911 train loss 4.6336 val loss 4.6464\n",
            "The current learning rate 0.00005\n",
            "Epoch 18912 train loss 4.7852 val loss 4.3618\n",
            "The current learning rate 0.00005\n",
            "Epoch 18913 train loss 4.3943 val loss 4.4185\n",
            "The current learning rate 0.00005\n",
            "Epoch 18914 train loss 4.4676 val loss 4.6969\n",
            "The current learning rate 0.00005\n",
            "Epoch 18915 train loss 4.6717 val loss 4.9874\n",
            "The current learning rate 0.00005\n",
            "Epoch 18916 train loss 4.3851 val loss 4.7980\n",
            "The current learning rate 0.00005\n",
            "Epoch 18917 train loss 4.5768 val loss 4.4021\n",
            "The current learning rate 0.00005\n",
            "Epoch 18918 train loss 4.4867 val loss 4.5897\n",
            "The current learning rate 0.00005\n",
            "Epoch 18919 train loss 4.4679 val loss 4.6183\n",
            "The current learning rate 0.00005\n",
            "Epoch 18920 train loss 4.9374 val loss 4.6265\n",
            "The current learning rate 0.00005\n",
            "Epoch 18921 train loss 4.8959 val loss 4.9111\n",
            "The current learning rate 0.00005\n",
            "Epoch 18922 train loss 4.5074 val loss 4.6626\n",
            "The current learning rate 0.00005\n",
            "Epoch 18923 train loss 4.9708 val loss 4.6313\n",
            "The current learning rate 0.00005\n",
            "Epoch 18924 train loss 4.7777 val loss 4.4687\n",
            "The current learning rate 0.00005\n",
            "Epoch 18925 train loss 4.3454 val loss 4.6918\n",
            "The current learning rate 0.00005\n",
            "Epoch 18926 train loss 4.4649 val loss 4.7199\n",
            "The current learning rate 0.00005\n",
            "Epoch 18927 train loss 4.6912 val loss 4.7822\n",
            "The current learning rate 0.00005\n",
            "Epoch 18928 train loss 4.6402 val loss 4.6255\n",
            "The current learning rate 0.00005\n",
            "Epoch 18929 train loss 4.9188 val loss 4.5991\n",
            "The current learning rate 0.00005\n",
            "Epoch 18930 train loss 4.4968 val loss 4.7471\n",
            "The current learning rate 0.00005\n",
            "Epoch 18931 train loss 5.1346 val loss 4.7254\n",
            "The current learning rate 0.00005\n",
            "Epoch 18932 train loss 5.0313 val loss 4.7740\n",
            "The current learning rate 0.00005\n",
            "Epoch 18933 train loss 4.5604 val loss 4.2259\n",
            "The current learning rate 0.00005\n",
            "Epoch 18934 train loss 4.6273 val loss 4.7659\n",
            "The current learning rate 0.00005\n",
            "Epoch 18935 train loss 4.8675 val loss 4.7258\n",
            "The current learning rate 0.00005\n",
            "Epoch 18936 train loss 4.0542 val loss 4.5445\n",
            "The current learning rate 0.00005\n",
            "Epoch 18937 train loss 4.9776 val loss 4.5670\n",
            "The current learning rate 0.00005\n",
            "Epoch 18938 train loss 4.4520 val loss 4.7619\n",
            "The current learning rate 0.00005\n",
            "Epoch 18939 train loss 4.4595 val loss 4.6705\n",
            "The current learning rate 0.00005\n",
            "Epoch 18940 train loss 4.4209 val loss 4.4554\n",
            "The current learning rate 0.00005\n",
            "Epoch 18941 train loss 4.9897 val loss 4.4962\n",
            "The current learning rate 0.00005\n",
            "Epoch 18942 train loss 5.1462 val loss 4.3243\n",
            "The current learning rate 0.00005\n",
            "Epoch 18943 train loss 4.7286 val loss 4.4365\n",
            "The current learning rate 0.00005\n",
            "Epoch 18944 train loss 4.7120 val loss 4.4836\n",
            "The current learning rate 0.00005\n",
            "Epoch 18945 train loss 4.7114 val loss 4.4552\n",
            "The current learning rate 0.00005\n",
            "Epoch 18946 train loss 4.5746 val loss 4.9729\n",
            "The current learning rate 0.00005\n",
            "Epoch 18947 train loss 4.5143 val loss 4.6833\n",
            "The current learning rate 0.00005\n",
            "Epoch 18948 train loss 4.8982 val loss 4.7764\n",
            "The current learning rate 0.00005\n",
            "Epoch 18949 train loss 4.2703 val loss 4.6840\n",
            "The current learning rate 0.00005\n",
            "Epoch 18950 train loss 4.7742 val loss 4.8860\n",
            "The current learning rate 0.00005\n",
            "Epoch 18951 train loss 4.7701 val loss 4.5591\n",
            "The current learning rate 0.00005\n",
            "Epoch 18952 train loss 4.5768 val loss 4.5496\n",
            "The current learning rate 0.00005\n",
            "Epoch 18953 train loss 4.9924 val loss 4.5132\n",
            "The current learning rate 0.00005\n",
            "Epoch 18954 train loss 4.5063 val loss 4.4902\n",
            "The current learning rate 0.00005\n",
            "Epoch 18955 train loss 5.0434 val loss 4.4049\n",
            "The current learning rate 0.00005\n",
            "Epoch 18956 train loss 4.6328 val loss 4.7069\n",
            "The current learning rate 0.00005\n",
            "Epoch 18957 train loss 5.0422 val loss 4.3667\n",
            "The current learning rate 0.00005\n",
            "Epoch 18958 train loss 4.4796 val loss 4.7994\n",
            "The current learning rate 0.00005\n",
            "Epoch 18959 train loss 4.3022 val loss 4.4213\n",
            "The current learning rate 0.00005\n",
            "Epoch 18960 train loss 4.3764 val loss 4.6815\n",
            "The current learning rate 0.00005\n",
            "Epoch 18961 train loss 4.3495 val loss 4.5673\n",
            "The current learning rate 0.00005\n",
            "Epoch 18962 train loss 4.4081 val loss 4.4860\n",
            "The current learning rate 0.00005\n",
            "Epoch 18963 train loss 5.0232 val loss 4.8021\n",
            "The current learning rate 0.00005\n",
            "Epoch 18964 train loss 4.7704 val loss 4.5549\n",
            "The current learning rate 0.00005\n",
            "Epoch 18965 train loss 4.3640 val loss 4.6918\n",
            "The current learning rate 0.00005\n",
            "Epoch 18966 train loss 4.5333 val loss 4.6551\n",
            "The current learning rate 0.00005\n",
            "Epoch 18967 train loss 4.7256 val loss 4.3954\n",
            "The current learning rate 0.00005\n",
            "Epoch 18968 train loss 4.3525 val loss 4.5065\n",
            "The current learning rate 0.00005\n",
            "Epoch 18969 train loss 4.9687 val loss 5.2413\n",
            "The current learning rate 0.00005\n",
            "Epoch 18970 train loss 4.5887 val loss 4.8335\n",
            "The current learning rate 0.00005\n",
            "Epoch 18971 train loss 4.6106 val loss 4.7855\n",
            "The current learning rate 0.00005\n",
            "Epoch 18972 train loss 4.6307 val loss 4.4765\n",
            "The current learning rate 0.00005\n",
            "Epoch 18973 train loss 4.7868 val loss 4.6669\n",
            "The current learning rate 0.00005\n",
            "Epoch 18974 train loss 4.4440 val loss 4.4597\n",
            "The current learning rate 0.00005\n",
            "Epoch 18975 train loss 4.0769 val loss 4.6392\n",
            "The current learning rate 0.00005\n",
            "Epoch 18976 train loss 4.9387 val loss 4.7925\n",
            "The current learning rate 0.00005\n",
            "Epoch 18977 train loss 4.5718 val loss 4.3919\n",
            "The current learning rate 0.00005\n",
            "Epoch 18978 train loss 4.5451 val loss 4.4557\n",
            "The current learning rate 0.00005\n",
            "Epoch 18979 train loss 4.6947 val loss 4.3262\n",
            "The current learning rate 0.00005\n",
            "Epoch 18980 train loss 4.3310 val loss 4.6364\n",
            "The current learning rate 0.00005\n",
            "Epoch 18981 train loss 4.7240 val loss 4.7363\n",
            "The current learning rate 0.00005\n",
            "Epoch 18982 train loss 4.1203 val loss 5.0277\n",
            "The current learning rate 0.00005\n",
            "Epoch 18983 train loss 4.3661 val loss 4.0460\n",
            "The current learning rate 0.00005\n",
            "Epoch 18984 train loss 4.4740 val loss 5.0991\n",
            "The current learning rate 0.00005\n",
            "Epoch 18985 train loss 4.6510 val loss 4.3420\n",
            "The current learning rate 0.00005\n",
            "Epoch 18986 train loss 4.8432 val loss 4.3267\n",
            "The current learning rate 0.00005\n",
            "Epoch 18987 train loss 4.3072 val loss 4.7146\n",
            "The current learning rate 0.00005\n",
            "Epoch 18988 train loss 4.2860 val loss 5.0959\n",
            "The current learning rate 0.00005\n",
            "Epoch 18989 train loss 4.9122 val loss 4.5068\n",
            "The current learning rate 0.00005\n",
            "Epoch 18990 train loss 4.9069 val loss 4.6712\n",
            "The current learning rate 0.00005\n",
            "Epoch 18991 train loss 5.0240 val loss 5.0716\n",
            "The current learning rate 0.00005\n",
            "Epoch 18992 train loss 4.4368 val loss 4.3024\n",
            "The current learning rate 0.00005\n",
            "Epoch 18993 train loss 4.4107 val loss 4.6543\n",
            "The current learning rate 0.00005\n",
            "Epoch 18994 train loss 4.4759 val loss 4.5755\n",
            "The current learning rate 0.00005\n",
            "Epoch 18995 train loss 4.5664 val loss 4.3845\n",
            "The current learning rate 0.00005\n",
            "Epoch 18996 train loss 4.6212 val loss 4.5568\n",
            "The current learning rate 0.00005\n",
            "Epoch 18997 train loss 4.6120 val loss 4.4014\n",
            "The current learning rate 0.00005\n",
            "Epoch 18998 train loss 4.1491 val loss 4.3571\n",
            "The current learning rate 0.00005\n",
            "Epoch 18999 train loss 4.7165 val loss 4.5014\n",
            "The current learning rate 0.00005\n",
            "Epoch 19000 train loss 4.5806 val loss 4.6026\n",
            "The current learning rate 0.00005\n",
            "Epoch 19001 train loss 4.8157 val loss 5.0402\n",
            "The current learning rate 0.00005\n",
            "Epoch 19002 train loss 4.6046 val loss 4.7929\n",
            "The current learning rate 0.00005\n",
            "Epoch 19003 train loss 4.0651 val loss 4.4563\n",
            "The current learning rate 0.00005\n",
            "Epoch 19004 train loss 4.5608 val loss 4.7318\n",
            "The current learning rate 0.00005\n",
            "Epoch 19005 train loss 4.5131 val loss 4.4610\n",
            "The current learning rate 0.00005\n",
            "Epoch 19006 train loss 4.7882 val loss 4.3777\n",
            "The current learning rate 0.00005\n",
            "Epoch 19007 train loss 4.7449 val loss 4.7047\n",
            "The current learning rate 0.00005\n",
            "Epoch 19008 train loss 4.5025 val loss 4.3153\n",
            "The current learning rate 0.00005\n",
            "Epoch 19009 train loss 4.2446 val loss 4.7796\n",
            "The current learning rate 0.00005\n",
            "Epoch 19010 train loss 4.7082 val loss 4.6345\n",
            "The current learning rate 0.00005\n",
            "Epoch 19011 train loss 4.4880 val loss 4.9027\n",
            "The current learning rate 0.00005\n",
            "Epoch 19012 train loss 4.2303 val loss 4.7966\n",
            "The current learning rate 0.00005\n",
            "Epoch 19013 train loss 4.4947 val loss 5.0943\n",
            "The current learning rate 0.00005\n",
            "Epoch 19014 train loss 4.5592 val loss 4.4224\n",
            "The current learning rate 0.00005\n",
            "Epoch 19015 train loss 4.8157 val loss 4.7238\n",
            "The current learning rate 0.00005\n",
            "Epoch 19016 train loss 4.7348 val loss 4.5051\n",
            "The current learning rate 0.00005\n",
            "Epoch 19017 train loss 4.7286 val loss 4.3890\n",
            "The current learning rate 0.00005\n",
            "Epoch 19018 train loss 4.6801 val loss 4.3471\n",
            "The current learning rate 0.00005\n",
            "Epoch 19019 train loss 4.6931 val loss 4.3865\n",
            "The current learning rate 0.00005\n",
            "Epoch 19020 train loss 4.6639 val loss 4.5538\n",
            "The current learning rate 0.00005\n",
            "Epoch 19021 train loss 4.5951 val loss 4.4724\n",
            "The current learning rate 0.00005\n",
            "Epoch 19022 train loss 4.6608 val loss 4.3289\n",
            "The current learning rate 0.00005\n",
            "Epoch 19023 train loss 4.6807 val loss 4.5286\n",
            "The current learning rate 0.00005\n",
            "Epoch 19024 train loss 5.0912 val loss 4.3868\n",
            "The current learning rate 0.00005\n",
            "Epoch 19025 train loss 4.7217 val loss 4.9092\n",
            "The current learning rate 0.00005\n",
            "Epoch 19026 train loss 4.3163 val loss 4.5557\n",
            "The current learning rate 0.00005\n",
            "Epoch 19027 train loss 4.9426 val loss 4.6968\n",
            "The current learning rate 0.00005\n",
            "Epoch 19028 train loss 4.6806 val loss 4.8392\n",
            "The current learning rate 0.00005\n",
            "Epoch 19029 train loss 4.6331 val loss 4.2850\n",
            "The current learning rate 0.00005\n",
            "Epoch 19030 train loss 4.4547 val loss 4.3099\n",
            "The current learning rate 0.00005\n",
            "Epoch 19031 train loss 4.8419 val loss 4.7443\n",
            "The current learning rate 0.00005\n",
            "Epoch 19032 train loss 4.6878 val loss 4.6947\n",
            "The current learning rate 0.00005\n",
            "Epoch 19033 train loss 4.5076 val loss 4.5541\n",
            "The current learning rate 0.00005\n",
            "Epoch 19034 train loss 4.6442 val loss 4.3029\n",
            "The current learning rate 0.00005\n",
            "Epoch 19035 train loss 4.5652 val loss 4.9887\n",
            "The current learning rate 0.00005\n",
            "Epoch 19036 train loss 4.7827 val loss 4.2435\n",
            "The current learning rate 0.00005\n",
            "Epoch 19037 train loss 4.3754 val loss 4.8168\n",
            "The current learning rate 0.00005\n",
            "Epoch 19038 train loss 4.7459 val loss 4.6637\n",
            "The current learning rate 0.00005\n",
            "Epoch 19039 train loss 4.5781 val loss 4.8118\n",
            "The current learning rate 0.00005\n",
            "Epoch 19040 train loss 4.5870 val loss 4.7495\n",
            "The current learning rate 0.00005\n",
            "Epoch 19041 train loss 4.3568 val loss 4.7676\n",
            "The current learning rate 0.00005\n",
            "Epoch 19042 train loss 4.4176 val loss 4.7297\n",
            "The current learning rate 0.00005\n",
            "Epoch 19043 train loss 4.5727 val loss 4.4670\n",
            "The current learning rate 0.00005\n",
            "Epoch 19044 train loss 4.5698 val loss 4.6659\n",
            "The current learning rate 0.00005\n",
            "Epoch 19045 train loss 4.3096 val loss 4.2809\n",
            "The current learning rate 0.00005\n",
            "Epoch 19046 train loss 4.9868 val loss 4.7153\n",
            "The current learning rate 0.00005\n",
            "Epoch 19047 train loss 4.6817 val loss 4.6644\n",
            "The current learning rate 0.00005\n",
            "Epoch 19048 train loss 4.6069 val loss 4.5807\n",
            "The current learning rate 0.00005\n",
            "Epoch 19049 train loss 4.7415 val loss 4.4732\n",
            "The current learning rate 0.00005\n",
            "Epoch 19050 train loss 4.6856 val loss 5.1661\n",
            "The current learning rate 0.00005\n",
            "Epoch 19051 train loss 4.3775 val loss 4.4144\n",
            "The current learning rate 0.00005\n",
            "Epoch 19052 train loss 4.5622 val loss 4.8903\n",
            "The current learning rate 0.00005\n",
            "Epoch 19053 train loss 4.7684 val loss 4.8819\n",
            "The current learning rate 0.00005\n",
            "Epoch 19054 train loss 4.5336 val loss 4.6155\n",
            "The current learning rate 0.00005\n",
            "Epoch 19055 train loss 4.5574 val loss 4.7131\n",
            "The current learning rate 0.00005\n",
            "Epoch 19056 train loss 4.5795 val loss 4.5541\n",
            "The current learning rate 0.00005\n",
            "Epoch 19057 train loss 4.8242 val loss 4.7472\n",
            "The current learning rate 0.00005\n",
            "Epoch 19058 train loss 4.4784 val loss 4.5293\n",
            "The current learning rate 0.00005\n",
            "Epoch 19059 train loss 4.7638 val loss 4.6626\n",
            "The current learning rate 0.00005\n",
            "Epoch 19060 train loss 4.8816 val loss 5.1358\n",
            "The current learning rate 0.00005\n",
            "Epoch 19061 train loss 4.2590 val loss 4.8226\n",
            "The current learning rate 0.00005\n",
            "Epoch 19062 train loss 4.7961 val loss 4.4605\n",
            "The current learning rate 0.00005\n",
            "Epoch 19063 train loss 4.5687 val loss 4.7503\n",
            "The current learning rate 0.00005\n",
            "Epoch 19064 train loss 4.7689 val loss 4.7143\n",
            "The current learning rate 0.00005\n",
            "Epoch 19065 train loss 5.2819 val loss 4.6929\n",
            "The current learning rate 0.00005\n",
            "Epoch 19066 train loss 4.6452 val loss 4.9760\n",
            "The current learning rate 0.00005\n",
            "Epoch 19067 train loss 4.8977 val loss 4.7854\n",
            "The current learning rate 0.00005\n",
            "Epoch 19068 train loss 4.3951 val loss 4.3403\n",
            "The current learning rate 0.00005\n",
            "Epoch 19069 train loss 4.5689 val loss 4.6030\n",
            "The current learning rate 0.00005\n",
            "Epoch 19070 train loss 4.5138 val loss 4.5369\n",
            "The current learning rate 0.00005\n",
            "Epoch 19071 train loss 4.8633 val loss 4.3457\n",
            "The current learning rate 0.00005\n",
            "Epoch 19072 train loss 4.3650 val loss 4.7649\n",
            "The current learning rate 0.00005\n",
            "Epoch 19073 train loss 4.5277 val loss 4.5520\n",
            "The current learning rate 0.00005\n",
            "Epoch 19074 train loss 4.7274 val loss 4.4664\n",
            "The current learning rate 0.00005\n",
            "Epoch 19075 train loss 4.8656 val loss 4.3783\n",
            "The current learning rate 0.00005\n",
            "Epoch 19076 train loss 4.3263 val loss 4.5930\n",
            "The current learning rate 0.00005\n",
            "Epoch 19077 train loss 4.7779 val loss 4.6485\n",
            "The current learning rate 0.00005\n",
            "Epoch 19078 train loss 4.4799 val loss 4.4110\n",
            "The current learning rate 0.00005\n",
            "Epoch 19079 train loss 4.9433 val loss 4.1976\n",
            "The current learning rate 0.00005\n",
            "Epoch 19080 train loss 4.9169 val loss 4.4141\n",
            "The current learning rate 0.00005\n",
            "Epoch 19081 train loss 4.8499 val loss 4.7132\n",
            "The current learning rate 0.00005\n",
            "Epoch 19082 train loss 4.2805 val loss 4.8097\n",
            "The current learning rate 0.00005\n",
            "Epoch 19083 train loss 4.8133 val loss 4.5471\n",
            "The current learning rate 0.00005\n",
            "Epoch 19084 train loss 4.6457 val loss 4.7122\n",
            "The current learning rate 0.00005\n",
            "Epoch 19085 train loss 4.4415 val loss 4.4406\n",
            "The current learning rate 0.00005\n",
            "Epoch 19086 train loss 4.6670 val loss 4.6873\n",
            "The current learning rate 0.00005\n",
            "Epoch 19087 train loss 4.9191 val loss 4.3908\n",
            "The current learning rate 0.00005\n",
            "Epoch 19088 train loss 4.7064 val loss 4.8985\n",
            "The current learning rate 0.00005\n",
            "Epoch 19089 train loss 4.7006 val loss 4.4933\n",
            "The current learning rate 0.00005\n",
            "Epoch 19090 train loss 4.4726 val loss 4.4013\n",
            "The current learning rate 0.00005\n",
            "Epoch 19091 train loss 4.6074 val loss 5.0406\n",
            "The current learning rate 0.00005\n",
            "Epoch 19092 train loss 4.3592 val loss 4.7911\n",
            "The current learning rate 0.00005\n",
            "Epoch 19093 train loss 4.5640 val loss 4.3759\n",
            "The current learning rate 0.00005\n",
            "Epoch 19094 train loss 4.5292 val loss 4.4422\n",
            "The current learning rate 0.00005\n",
            "Epoch 19095 train loss 4.2900 val loss 4.9192\n",
            "The current learning rate 0.00005\n",
            "Epoch 19096 train loss 4.8194 val loss 4.7117\n",
            "The current learning rate 0.00005\n",
            "Epoch 19097 train loss 4.5887 val loss 4.5863\n",
            "The current learning rate 0.00005\n",
            "Epoch 19098 train loss 4.3942 val loss 4.8171\n",
            "The current learning rate 0.00005\n",
            "Epoch 19099 train loss 4.3115 val loss 4.2749\n",
            "The current learning rate 0.00005\n",
            "Epoch 19100 train loss 4.3484 val loss 4.6842\n",
            "The current learning rate 0.00005\n",
            "Epoch 19101 train loss 4.8340 val loss 4.8958\n",
            "The current learning rate 0.00005\n",
            "Epoch 19102 train loss 4.8534 val loss 4.5890\n",
            "The current learning rate 0.00005\n",
            "Epoch 19103 train loss 4.6195 val loss 4.5090\n",
            "The current learning rate 0.00005\n",
            "Epoch 19104 train loss 4.4345 val loss 4.0631\n",
            "The current learning rate 0.00005\n",
            "Epoch 19105 train loss 4.6964 val loss 5.1346\n",
            "The current learning rate 0.00005\n",
            "Epoch 19106 train loss 4.5826 val loss 4.5260\n",
            "The current learning rate 0.00005\n",
            "Epoch 19107 train loss 4.6804 val loss 4.1654\n",
            "The current learning rate 0.00005\n",
            "Epoch 19108 train loss 4.4403 val loss 4.5717\n",
            "The current learning rate 0.00005\n",
            "Epoch 19109 train loss 4.5365 val loss 4.6788\n",
            "The current learning rate 0.00005\n",
            "Epoch 19110 train loss 4.9891 val loss 4.5438\n",
            "The current learning rate 0.00005\n",
            "Epoch 19111 train loss 5.0323 val loss 4.6899\n",
            "The current learning rate 0.00005\n",
            "Epoch 19112 train loss 4.4205 val loss 4.5745\n",
            "The current learning rate 0.00005\n",
            "Epoch 19113 train loss 4.3419 val loss 4.6024\n",
            "The current learning rate 0.00005\n",
            "Epoch 19114 train loss 4.5034 val loss 4.6153\n",
            "The current learning rate 0.00005\n",
            "Epoch 19115 train loss 4.1808 val loss 4.7172\n",
            "The current learning rate 0.00005\n",
            "Epoch 19116 train loss 4.6316 val loss 4.5141\n",
            "The current learning rate 0.00005\n",
            "Epoch 19117 train loss 4.9897 val loss 4.5418\n",
            "The current learning rate 0.00005\n",
            "Epoch 19118 train loss 4.5448 val loss 4.5874\n",
            "The current learning rate 0.00005\n",
            "Epoch 19119 train loss 4.8181 val loss 4.6322\n",
            "The current learning rate 0.00005\n",
            "Epoch 19120 train loss 4.3746 val loss 4.6451\n",
            "The current learning rate 0.00005\n",
            "Epoch 19121 train loss 4.4850 val loss 4.2941\n",
            "The current learning rate 0.00005\n",
            "Epoch 19122 train loss 4.4570 val loss 4.9726\n",
            "The current learning rate 0.00005\n",
            "Epoch 19123 train loss 4.6311 val loss 4.5364\n",
            "The current learning rate 0.00005\n",
            "Epoch 19124 train loss 4.4804 val loss 4.4612\n",
            "The current learning rate 0.00005\n",
            "Epoch 19125 train loss 4.4192 val loss 4.8779\n",
            "The current learning rate 0.00005\n",
            "Epoch 19126 train loss 4.9281 val loss 4.6507\n",
            "The current learning rate 0.00005\n",
            "Epoch 19127 train loss 4.3247 val loss 5.0706\n",
            "The current learning rate 0.00005\n",
            "Epoch 19128 train loss 4.4490 val loss 5.2884\n",
            "The current learning rate 0.00005\n",
            "Epoch 19129 train loss 4.4732 val loss 4.8001\n",
            "The current learning rate 0.00005\n",
            "Epoch 19130 train loss 4.4437 val loss 4.3689\n",
            "The current learning rate 0.00005\n",
            "Epoch 19131 train loss 4.4121 val loss 4.2356\n",
            "The current learning rate 0.00005\n",
            "Epoch 19132 train loss 4.6946 val loss 4.8096\n",
            "The current learning rate 0.00005\n",
            "Epoch 19133 train loss 4.8170 val loss 4.5625\n",
            "The current learning rate 0.00005\n",
            "Epoch 19134 train loss 4.6970 val loss 4.7505\n",
            "The current learning rate 0.00005\n",
            "Epoch 19135 train loss 4.4212 val loss 4.2228\n",
            "The current learning rate 0.00005\n",
            "Epoch 19136 train loss 4.4251 val loss 4.5837\n",
            "The current learning rate 0.00005\n",
            "Epoch 19137 train loss 4.2351 val loss 5.0117\n",
            "The current learning rate 0.00005\n",
            "Epoch 19138 train loss 4.8237 val loss 4.7686\n",
            "The current learning rate 0.00005\n",
            "Epoch 19139 train loss 4.6141 val loss 4.4483\n",
            "The current learning rate 0.00005\n",
            "Epoch 19140 train loss 4.5559 val loss 4.9255\n",
            "The current learning rate 0.00005\n",
            "Epoch 19141 train loss 4.6803 val loss 4.9761\n",
            "The current learning rate 0.00005\n",
            "Epoch 19142 train loss 4.3131 val loss 4.6554\n",
            "The current learning rate 0.00005\n",
            "Epoch 19143 train loss 4.6714 val loss 4.5436\n",
            "The current learning rate 0.00005\n",
            "Epoch 19144 train loss 4.4306 val loss 4.8685\n",
            "The current learning rate 0.00005\n",
            "Epoch 19145 train loss 4.5632 val loss 4.7223\n",
            "The current learning rate 0.00005\n",
            "Epoch 19146 train loss 4.7035 val loss 4.8720\n",
            "The current learning rate 0.00005\n",
            "Epoch 19147 train loss 4.8181 val loss 4.3940\n",
            "The current learning rate 0.00005\n",
            "Epoch 19148 train loss 4.6794 val loss 4.6751\n",
            "The current learning rate 0.00005\n",
            "Epoch 19149 train loss 4.6823 val loss 4.3255\n",
            "The current learning rate 0.00005\n",
            "Epoch 19150 train loss 4.3033 val loss 4.5982\n",
            "The current learning rate 0.00005\n",
            "Epoch 19151 train loss 4.3733 val loss 4.7570\n",
            "The current learning rate 0.00005\n",
            "Epoch 19152 train loss 4.6475 val loss 4.5553\n",
            "The current learning rate 0.00005\n",
            "Epoch 19153 train loss 4.8680 val loss 4.2956\n",
            "The current learning rate 0.00005\n",
            "Epoch 19154 train loss 4.5355 val loss 4.4160\n",
            "The current learning rate 0.00005\n",
            "Epoch 19155 train loss 4.6250 val loss 4.5829\n",
            "The current learning rate 0.00005\n",
            "Epoch 19156 train loss 4.4344 val loss 4.4935\n",
            "The current learning rate 0.00005\n",
            "Epoch 19157 train loss 4.3707 val loss 4.7969\n",
            "The current learning rate 0.00005\n",
            "Epoch 19158 train loss 4.7479 val loss 4.4209\n",
            "The current learning rate 0.00005\n",
            "Epoch 19159 train loss 4.7239 val loss 4.6082\n",
            "The current learning rate 0.00005\n",
            "Epoch 19160 train loss 5.0105 val loss 4.8803\n",
            "The current learning rate 0.00005\n",
            "Epoch 19161 train loss 4.7405 val loss 4.4496\n",
            "The current learning rate 0.00005\n",
            "Epoch 19162 train loss 4.7749 val loss 4.8219\n",
            "The current learning rate 0.00005\n",
            "Epoch 19163 train loss 4.5564 val loss 4.8093\n",
            "The current learning rate 0.00005\n",
            "Epoch 19164 train loss 4.9973 val loss 4.6553\n",
            "The current learning rate 0.00005\n",
            "Epoch 19165 train loss 4.7517 val loss 4.6437\n",
            "The current learning rate 0.00005\n",
            "Epoch 19166 train loss 4.4945 val loss 5.0208\n",
            "The current learning rate 0.00005\n",
            "Epoch 19167 train loss 5.0599 val loss 4.5690\n",
            "The current learning rate 0.00005\n",
            "Epoch 19168 train loss 4.6946 val loss 4.6059\n",
            "The current learning rate 0.00005\n",
            "Epoch 19169 train loss 4.4000 val loss 4.8826\n",
            "The current learning rate 0.00005\n",
            "Epoch 19170 train loss 4.4386 val loss 4.7892\n",
            "The current learning rate 0.00005\n",
            "Epoch 19171 train loss 4.4140 val loss 4.8292\n",
            "The current learning rate 0.00005\n",
            "Epoch 19172 train loss 4.9017 val loss 4.0728\n",
            "The current learning rate 0.00005\n",
            "Epoch 19173 train loss 4.6575 val loss 4.7044\n",
            "The current learning rate 0.00005\n",
            "Epoch 19174 train loss 4.8748 val loss 4.3977\n",
            "The current learning rate 0.00005\n",
            "Epoch 19175 train loss 4.6937 val loss 4.5719\n",
            "The current learning rate 0.00005\n",
            "Epoch 19176 train loss 4.7278 val loss 4.5819\n",
            "The current learning rate 0.00005\n",
            "Epoch 19177 train loss 4.6909 val loss 4.7470\n",
            "The current learning rate 0.00005\n",
            "Epoch 19178 train loss 4.6198 val loss 4.7896\n",
            "The current learning rate 0.00005\n",
            "Epoch 19179 train loss 4.4514 val loss 4.4163\n",
            "The current learning rate 0.00005\n",
            "Epoch 19180 train loss 4.8914 val loss 4.6278\n",
            "The current learning rate 0.00005\n",
            "Epoch 19181 train loss 4.8506 val loss 4.5496\n",
            "The current learning rate 0.00005\n",
            "Epoch 19182 train loss 4.5413 val loss 4.5572\n",
            "The current learning rate 0.00005\n",
            "Epoch 19183 train loss 4.7033 val loss 4.8837\n",
            "The current learning rate 0.00005\n",
            "Epoch 19184 train loss 4.9770 val loss 4.7194\n",
            "The current learning rate 0.00005\n",
            "Epoch 19185 train loss 5.0008 val loss 5.0632\n",
            "The current learning rate 0.00005\n",
            "Epoch 19186 train loss 4.6170 val loss 4.6266\n",
            "The current learning rate 0.00005\n",
            "Epoch 19187 train loss 4.6105 val loss 4.8898\n",
            "The current learning rate 0.00005\n",
            "Epoch 19188 train loss 4.3961 val loss 4.9646\n",
            "The current learning rate 0.00005\n",
            "Epoch 19189 train loss 4.4234 val loss 4.3659\n",
            "The current learning rate 0.00005\n",
            "Epoch 19190 train loss 4.6490 val loss 4.4542\n",
            "The current learning rate 0.00005\n",
            "Epoch 19191 train loss 4.7617 val loss 4.6016\n",
            "The current learning rate 0.00005\n",
            "Epoch 19192 train loss 4.5132 val loss 4.5208\n",
            "The current learning rate 0.00005\n",
            "Epoch 19193 train loss 4.8293 val loss 4.5704\n",
            "The current learning rate 0.00005\n",
            "Epoch 19194 train loss 4.6066 val loss 4.6616\n",
            "The current learning rate 0.00005\n",
            "Epoch 19195 train loss 4.5900 val loss 4.4187\n",
            "The current learning rate 0.00005\n",
            "Epoch 19196 train loss 5.3733 val loss 4.7820\n",
            "The current learning rate 0.00005\n",
            "Epoch 19197 train loss 4.8929 val loss 4.5517\n",
            "The current learning rate 0.00005\n",
            "Epoch 19198 train loss 4.5743 val loss 4.3963\n",
            "The current learning rate 0.00005\n",
            "Epoch 19199 train loss 4.8396 val loss 4.6096\n",
            "The current learning rate 0.00005\n",
            "Epoch 19200 train loss 5.2434 val loss 4.6720\n",
            "The current learning rate 0.00005\n",
            "Epoch 19201 train loss 4.5460 val loss 4.1963\n",
            "The current learning rate 0.00005\n",
            "Epoch 19202 train loss 4.5680 val loss 5.1034\n",
            "The current learning rate 0.00005\n",
            "Epoch 19203 train loss 4.5414 val loss 4.7898\n",
            "The current learning rate 0.00005\n",
            "Epoch 19204 train loss 5.0096 val loss 4.6826\n",
            "The current learning rate 0.00005\n",
            "Epoch 19205 train loss 4.7839 val loss 4.5049\n",
            "The current learning rate 0.00005\n",
            "Epoch 19206 train loss 4.5628 val loss 4.4607\n",
            "The current learning rate 0.00005\n",
            "Epoch 19207 train loss 4.7486 val loss 4.5195\n",
            "The current learning rate 0.00005\n",
            "Epoch 19208 train loss 4.9300 val loss 4.6934\n",
            "The current learning rate 0.00005\n",
            "Epoch 19209 train loss 5.0648 val loss 4.5228\n",
            "The current learning rate 0.00005\n",
            "Epoch 19210 train loss 4.6078 val loss 4.8619\n",
            "The current learning rate 0.00005\n",
            "Epoch 19211 train loss 4.6405 val loss 4.7792\n",
            "The current learning rate 0.00005\n",
            "Epoch 19212 train loss 5.0441 val loss 4.2418\n",
            "The current learning rate 0.00005\n",
            "Epoch 19213 train loss 4.3591 val loss 4.5281\n",
            "The current learning rate 0.00005\n",
            "Epoch 19214 train loss 4.5112 val loss 4.6844\n",
            "The current learning rate 0.00005\n",
            "Epoch 19215 train loss 4.4996 val loss 4.6781\n",
            "The current learning rate 0.00005\n",
            "Epoch 19216 train loss 4.4996 val loss 4.8986\n",
            "The current learning rate 0.00005\n",
            "Epoch 19217 train loss 4.5855 val loss 4.4916\n",
            "The current learning rate 0.00005\n",
            "Epoch 19218 train loss 4.7941 val loss 4.6494\n",
            "The current learning rate 0.00005\n",
            "Epoch 19219 train loss 4.4942 val loss 4.6542\n",
            "The current learning rate 0.00005\n",
            "Epoch 19220 train loss 4.5614 val loss 4.2282\n",
            "The current learning rate 0.00005\n",
            "Epoch 19221 train loss 4.8292 val loss 4.4622\n",
            "The current learning rate 0.00005\n",
            "Epoch 19222 train loss 4.6772 val loss 4.3652\n",
            "The current learning rate 0.00005\n",
            "Epoch 19223 train loss 4.5963 val loss 4.5624\n",
            "The current learning rate 0.00005\n",
            "Epoch 19224 train loss 4.6345 val loss 4.8643\n",
            "The current learning rate 0.00005\n",
            "Epoch 19225 train loss 4.9372 val loss 4.6981\n",
            "The current learning rate 0.00005\n",
            "Epoch 19226 train loss 4.4610 val loss 4.3962\n",
            "The current learning rate 0.00005\n",
            "Epoch 19227 train loss 5.0728 val loss 4.7298\n",
            "The current learning rate 0.00005\n",
            "Epoch 19228 train loss 4.6910 val loss 4.4774\n",
            "The current learning rate 0.00005\n",
            "Epoch 19229 train loss 4.7040 val loss 4.6708\n",
            "The current learning rate 0.00005\n",
            "Epoch 19230 train loss 4.5483 val loss 4.7359\n",
            "The current learning rate 0.00005\n",
            "Epoch 19231 train loss 4.5785 val loss 4.4998\n",
            "The current learning rate 0.00005\n",
            "Epoch 19232 train loss 4.5006 val loss 4.7610\n",
            "The current learning rate 0.00005\n",
            "Epoch 19233 train loss 4.5981 val loss 4.6460\n",
            "The current learning rate 0.00005\n",
            "Epoch 19234 train loss 4.2264 val loss 4.8517\n",
            "The current learning rate 0.00005\n",
            "Epoch 19235 train loss 4.6941 val loss 4.8453\n",
            "The current learning rate 0.00005\n",
            "Epoch 19236 train loss 4.4322 val loss 4.5636\n",
            "The current learning rate 0.00005\n",
            "Epoch 19237 train loss 5.2118 val loss 4.6773\n",
            "The current learning rate 0.00005\n",
            "Epoch 19238 train loss 4.8288 val loss 4.5820\n",
            "The current learning rate 0.00005\n",
            "Epoch 19239 train loss 4.5182 val loss 4.6193\n",
            "The current learning rate 0.00005\n",
            "Epoch 19240 train loss 4.9782 val loss 4.6094\n",
            "The current learning rate 0.00005\n",
            "Epoch 19241 train loss 4.5665 val loss 4.4173\n",
            "The current learning rate 0.00005\n",
            "Epoch 19242 train loss 4.7862 val loss 4.5634\n",
            "The current learning rate 0.00005\n",
            "Epoch 19243 train loss 4.5953 val loss 4.8365\n",
            "The current learning rate 0.00005\n",
            "Epoch 19244 train loss 4.7736 val loss 4.2223\n",
            "The current learning rate 0.00005\n",
            "Epoch 19245 train loss 4.7191 val loss 4.7766\n",
            "The current learning rate 0.00005\n",
            "Epoch 19246 train loss 4.5823 val loss 4.6462\n",
            "The current learning rate 0.00005\n",
            "Epoch 19247 train loss 4.5253 val loss 4.7373\n",
            "The current learning rate 0.00005\n",
            "Epoch 19248 train loss 4.7083 val loss 4.7518\n",
            "The current learning rate 0.00005\n",
            "Epoch 19249 train loss 4.3601 val loss 4.6253\n",
            "The current learning rate 0.00005\n",
            "Epoch 19250 train loss 4.6929 val loss 4.5926\n",
            "The current learning rate 0.00005\n",
            "Epoch 19251 train loss 4.5240 val loss 4.1809\n",
            "The current learning rate 0.00005\n",
            "Epoch 19252 train loss 4.5970 val loss 4.5830\n",
            "The current learning rate 0.00005\n",
            "Epoch 19253 train loss 4.6501 val loss 4.5371\n",
            "The current learning rate 0.00005\n",
            "Epoch 19254 train loss 4.3939 val loss 4.7210\n",
            "The current learning rate 0.00005\n",
            "Epoch 19255 train loss 4.5306 val loss 4.6220\n",
            "The current learning rate 0.00005\n",
            "Epoch 19256 train loss 4.5304 val loss 4.4314\n",
            "The current learning rate 0.00005\n",
            "Epoch 19257 train loss 4.4229 val loss 4.6313\n",
            "The current learning rate 0.00005\n",
            "Epoch 19258 train loss 4.5474 val loss 4.4098\n",
            "The current learning rate 0.00005\n",
            "Epoch 19259 train loss 4.8811 val loss 4.6757\n",
            "The current learning rate 0.00005\n",
            "Epoch 19260 train loss 4.5847 val loss 4.5878\n",
            "The current learning rate 0.00005\n",
            "Epoch 19261 train loss 4.7138 val loss 4.5359\n",
            "The current learning rate 0.00005\n",
            "Epoch 19262 train loss 4.3296 val loss 4.7791\n",
            "The current learning rate 0.00005\n",
            "Epoch 19263 train loss 4.4445 val loss 4.3372\n",
            "The current learning rate 0.00005\n",
            "Epoch 19264 train loss 4.6282 val loss 4.2690\n",
            "The current learning rate 0.00005\n",
            "Epoch 19265 train loss 4.8461 val loss 4.6287\n",
            "The current learning rate 0.00005\n",
            "Epoch 19266 train loss 4.3692 val loss 4.7492\n",
            "The current learning rate 0.00005\n",
            "Epoch 19267 train loss 4.9084 val loss 4.6914\n",
            "The current learning rate 0.00005\n",
            "Epoch 19268 train loss 4.6452 val loss 4.0719\n",
            "The current learning rate 0.00005\n",
            "Epoch 19269 train loss 4.6783 val loss 4.3850\n",
            "The current learning rate 0.00005\n",
            "Epoch 19270 train loss 4.9961 val loss 4.6715\n",
            "The current learning rate 0.00005\n",
            "Epoch 19271 train loss 4.5875 val loss 4.7844\n",
            "The current learning rate 0.00005\n",
            "Epoch 19272 train loss 5.1240 val loss 4.6310\n",
            "The current learning rate 0.00005\n",
            "Epoch 19273 train loss 4.6726 val loss 4.5561\n",
            "The current learning rate 0.00005\n",
            "Epoch 19274 train loss 4.8781 val loss 4.5067\n",
            "The current learning rate 0.00005\n",
            "Epoch 19275 train loss 4.5950 val loss 4.7041\n",
            "The current learning rate 0.00005\n",
            "Epoch 19276 train loss 4.2814 val loss 4.4270\n",
            "The current learning rate 0.00005\n",
            "Epoch 19277 train loss 4.4928 val loss 4.8242\n",
            "The current learning rate 0.00005\n",
            "Epoch 19278 train loss 4.4042 val loss 4.5183\n",
            "The current learning rate 0.00005\n",
            "Epoch 19279 train loss 4.3990 val loss 5.0099\n",
            "The current learning rate 0.00005\n",
            "Epoch 19280 train loss 4.7278 val loss 4.8815\n",
            "The current learning rate 0.00005\n",
            "Epoch 19281 train loss 4.6301 val loss 5.0325\n",
            "The current learning rate 0.00005\n",
            "Epoch 19282 train loss 4.8732 val loss 4.6286\n",
            "The current learning rate 0.00005\n",
            "Epoch 19283 train loss 5.0266 val loss 4.3561\n",
            "The current learning rate 0.00005\n",
            "Epoch 19284 train loss 4.5164 val loss 4.9591\n",
            "The current learning rate 0.00005\n",
            "Epoch 19285 train loss 4.8181 val loss 4.9767\n",
            "The current learning rate 0.00005\n",
            "Epoch 19286 train loss 4.5415 val loss 4.6088\n",
            "The current learning rate 0.00005\n",
            "Epoch 19287 train loss 4.9934 val loss 5.0460\n",
            "The current learning rate 0.00005\n",
            "Epoch 19288 train loss 4.5404 val loss 4.7731\n",
            "The current learning rate 0.00005\n",
            "Epoch 19289 train loss 4.7589 val loss 4.6487\n",
            "The current learning rate 0.00005\n",
            "Epoch 19290 train loss 4.6659 val loss 4.8804\n",
            "The current learning rate 0.00005\n",
            "Epoch 19291 train loss 4.4243 val loss 4.8349\n",
            "The current learning rate 0.00005\n",
            "Epoch 19292 train loss 4.6123 val loss 4.4782\n",
            "The current learning rate 0.00005\n",
            "Epoch 19293 train loss 4.8630 val loss 4.8710\n",
            "The current learning rate 0.00005\n",
            "Epoch 19294 train loss 4.3481 val loss 4.4478\n",
            "The current learning rate 0.00005\n",
            "Epoch 19295 train loss 4.5768 val loss 4.8795\n",
            "The current learning rate 0.00005\n",
            "Epoch 19296 train loss 4.6122 val loss 4.5776\n",
            "The current learning rate 0.00005\n",
            "Epoch 19297 train loss 4.3608 val loss 4.7554\n",
            "The current learning rate 0.00005\n",
            "Epoch 19298 train loss 4.3544 val loss 4.8102\n",
            "The current learning rate 0.00005\n",
            "Epoch 19299 train loss 4.6923 val loss 4.5043\n",
            "The current learning rate 0.00005\n",
            "Epoch 19300 train loss 4.7102 val loss 4.9008\n",
            "The current learning rate 0.00005\n",
            "Epoch 19301 train loss 4.4948 val loss 4.6807\n",
            "The current learning rate 0.00005\n",
            "Epoch 19302 train loss 4.8773 val loss 4.4200\n",
            "The current learning rate 0.00005\n",
            "Epoch 19303 train loss 4.8498 val loss 4.7351\n",
            "The current learning rate 0.00005\n",
            "Epoch 19304 train loss 4.3429 val loss 4.3651\n",
            "The current learning rate 0.00005\n",
            "Epoch 19305 train loss 4.7057 val loss 4.2790\n",
            "The current learning rate 0.00005\n",
            "Epoch 19306 train loss 4.4864 val loss 5.0559\n",
            "The current learning rate 0.00005\n",
            "Epoch 19307 train loss 4.5617 val loss 4.6982\n",
            "The current learning rate 0.00005\n",
            "Epoch 19308 train loss 4.9276 val loss 4.4218\n",
            "The current learning rate 0.00005\n",
            "Epoch 19309 train loss 4.9002 val loss 4.5891\n",
            "The current learning rate 0.00005\n",
            "Epoch 19310 train loss 4.6216 val loss 4.9577\n",
            "The current learning rate 0.00005\n",
            "Epoch 19311 train loss 4.5418 val loss 4.6415\n",
            "The current learning rate 0.00005\n",
            "Epoch 19312 train loss 4.7175 val loss 4.8507\n",
            "The current learning rate 0.00005\n",
            "Epoch 19313 train loss 4.6756 val loss 4.9796\n",
            "The current learning rate 0.00005\n",
            "Epoch 19314 train loss 4.6779 val loss 4.8484\n",
            "The current learning rate 0.00005\n",
            "Epoch 19315 train loss 4.8312 val loss 4.5859\n",
            "The current learning rate 0.00005\n",
            "Epoch 19316 train loss 4.4661 val loss 5.0777\n",
            "The current learning rate 0.00005\n",
            "Epoch 19317 train loss 4.7012 val loss 4.4888\n",
            "The current learning rate 0.00005\n",
            "Epoch 19318 train loss 4.7426 val loss 4.4470\n",
            "The current learning rate 0.00005\n",
            "Epoch 19319 train loss 4.3891 val loss 4.6819\n",
            "The current learning rate 0.00005\n",
            "Epoch 19320 train loss 4.4058 val loss 4.6725\n",
            "The current learning rate 0.00005\n",
            "Epoch 19321 train loss 4.3663 val loss 4.8303\n",
            "The current learning rate 0.00005\n",
            "Epoch 19322 train loss 4.6566 val loss 4.6969\n",
            "The current learning rate 0.00005\n",
            "Epoch 19323 train loss 4.8970 val loss 4.7674\n",
            "The current learning rate 0.00005\n",
            "Epoch 19324 train loss 4.4924 val loss 4.5764\n",
            "The current learning rate 0.00005\n",
            "Epoch 19325 train loss 4.4345 val loss 4.9145\n",
            "The current learning rate 0.00005\n",
            "Epoch 19326 train loss 4.3335 val loss 4.8371\n",
            "The current learning rate 0.00005\n",
            "Epoch 19327 train loss 4.6976 val loss 4.4518\n",
            "The current learning rate 0.00005\n",
            "Epoch 19328 train loss 4.6383 val loss 4.6333\n",
            "The current learning rate 0.00005\n",
            "Epoch 19329 train loss 4.4265 val loss 4.9624\n",
            "The current learning rate 0.00005\n",
            "Epoch 19330 train loss 4.1922 val loss 4.7834\n",
            "The current learning rate 0.00005\n",
            "Epoch 19331 train loss 4.9895 val loss 4.5041\n",
            "The current learning rate 0.00005\n",
            "Epoch 19332 train loss 4.7314 val loss 4.5806\n",
            "The current learning rate 0.00005\n",
            "Epoch 19333 train loss 4.6242 val loss 4.9489\n",
            "The current learning rate 0.00005\n",
            "Epoch 19334 train loss 4.4786 val loss 4.8406\n",
            "The current learning rate 0.00005\n",
            "Epoch 19335 train loss 5.0564 val loss 4.4547\n",
            "The current learning rate 0.00005\n",
            "Epoch 19336 train loss 4.6117 val loss 4.5400\n",
            "The current learning rate 0.00005\n",
            "Epoch 19337 train loss 4.6279 val loss 4.8264\n",
            "The current learning rate 0.00005\n",
            "Epoch 19338 train loss 4.6282 val loss 4.7636\n",
            "The current learning rate 0.00005\n",
            "Epoch 19339 train loss 4.4705 val loss 4.8599\n",
            "The current learning rate 0.00005\n",
            "Epoch 19340 train loss 4.5946 val loss 4.6393\n",
            "The current learning rate 0.00005\n",
            "Epoch 19341 train loss 4.7349 val loss 4.7512\n",
            "The current learning rate 0.00005\n",
            "Epoch 19342 train loss 4.7717 val loss 5.1107\n",
            "The current learning rate 0.00005\n",
            "Epoch 19343 train loss 4.6686 val loss 4.5293\n",
            "The current learning rate 0.00005\n",
            "Epoch 19344 train loss 5.1702 val loss 4.5384\n",
            "The current learning rate 0.00005\n",
            "Epoch 19345 train loss 4.8414 val loss 5.0917\n",
            "The current learning rate 0.00005\n",
            "Epoch 19346 train loss 5.0053 val loss 4.9860\n",
            "The current learning rate 0.00005\n",
            "Epoch 19347 train loss 4.3467 val loss 5.2945\n",
            "The current learning rate 0.00005\n",
            "Epoch 19348 train loss 4.5547 val loss 4.5356\n",
            "The current learning rate 0.00005\n",
            "Epoch 19349 train loss 4.6879 val loss 4.7191\n",
            "The current learning rate 0.00005\n",
            "Epoch 19350 train loss 4.5501 val loss 4.9269\n",
            "The current learning rate 0.00005\n",
            "Epoch 19351 train loss 4.7696 val loss 4.3688\n",
            "The current learning rate 0.00005\n",
            "Epoch 19352 train loss 5.0987 val loss 4.8974\n",
            "The current learning rate 0.00005\n",
            "Epoch 19353 train loss 4.4885 val loss 5.1130\n",
            "The current learning rate 0.00005\n",
            "Epoch 19354 train loss 4.4684 val loss 4.5174\n",
            "The current learning rate 0.00005\n",
            "Epoch 19355 train loss 4.6954 val loss 4.2392\n",
            "The current learning rate 0.00005\n",
            "Epoch 19356 train loss 4.6162 val loss 4.8956\n",
            "The current learning rate 0.00005\n",
            "Epoch 19357 train loss 4.2985 val loss 4.6871\n",
            "The current learning rate 0.00005\n",
            "Epoch 19358 train loss 4.4168 val loss 4.7512\n",
            "The current learning rate 0.00005\n",
            "Epoch 19359 train loss 4.4895 val loss 4.8762\n",
            "The current learning rate 0.00005\n",
            "Epoch 19360 train loss 4.6919 val loss 4.7049\n",
            "The current learning rate 0.00005\n",
            "Epoch 19361 train loss 4.4433 val loss 4.8132\n",
            "The current learning rate 0.00005\n",
            "Epoch 19362 train loss 4.7922 val loss 4.4522\n",
            "The current learning rate 0.00005\n",
            "Epoch 19363 train loss 5.0125 val loss 5.1051\n",
            "The current learning rate 0.00005\n",
            "Epoch 19364 train loss 4.8082 val loss 4.5219\n",
            "The current learning rate 0.00005\n",
            "Epoch 19365 train loss 4.6667 val loss 5.1027\n",
            "The current learning rate 0.00005\n",
            "Epoch 19366 train loss 4.6529 val loss 4.7226\n",
            "The current learning rate 0.00005\n",
            "Epoch 19367 train loss 4.7904 val loss 4.7090\n",
            "The current learning rate 0.00005\n",
            "Epoch 19368 train loss 4.4499 val loss 4.2293\n",
            "The current learning rate 0.00005\n",
            "Epoch 19369 train loss 4.8149 val loss 4.6369\n",
            "The current learning rate 0.00005\n",
            "Epoch 19370 train loss 4.6261 val loss 4.5024\n",
            "The current learning rate 0.00005\n",
            "Epoch 19371 train loss 4.6037 val loss 4.7412\n",
            "The current learning rate 0.00005\n",
            "Epoch 19372 train loss 4.4218 val loss 4.7585\n",
            "The current learning rate 0.00005\n",
            "Epoch 19373 train loss 4.5464 val loss 4.6168\n",
            "The current learning rate 0.00005\n",
            "Epoch 19374 train loss 4.6660 val loss 4.8213\n",
            "The current learning rate 0.00005\n",
            "Epoch 19375 train loss 4.3491 val loss 4.3424\n",
            "The current learning rate 0.00005\n",
            "Epoch 19376 train loss 4.7790 val loss 4.6154\n",
            "The current learning rate 0.00005\n",
            "Epoch 19377 train loss 4.7624 val loss 4.5199\n",
            "The current learning rate 0.00005\n",
            "Epoch 19378 train loss 4.8593 val loss 4.2080\n",
            "The current learning rate 0.00005\n",
            "Epoch 19379 train loss 4.6751 val loss 4.6433\n",
            "The current learning rate 0.00005\n",
            "Epoch 19380 train loss 4.7032 val loss 4.6968\n",
            "The current learning rate 0.00005\n",
            "Epoch 19381 train loss 4.3572 val loss 4.9704\n",
            "The current learning rate 0.00005\n",
            "Epoch 19382 train loss 4.3823 val loss 4.9331\n",
            "The current learning rate 0.00005\n",
            "Epoch 19383 train loss 4.3309 val loss 4.6734\n",
            "The current learning rate 0.00005\n",
            "Epoch 19384 train loss 4.8269 val loss 4.6175\n",
            "The current learning rate 0.00005\n",
            "Epoch 19385 train loss 4.6423 val loss 4.5256\n",
            "The current learning rate 0.00005\n",
            "Epoch 19386 train loss 4.5774 val loss 4.6147\n",
            "The current learning rate 0.00005\n",
            "Epoch 19387 train loss 4.4075 val loss 4.4420\n",
            "The current learning rate 0.00005\n",
            "Epoch 19388 train loss 4.7758 val loss 4.4325\n",
            "The current learning rate 0.00005\n",
            "Epoch 19389 train loss 5.2946 val loss 4.5337\n",
            "The current learning rate 0.00005\n",
            "Epoch 19390 train loss 4.5638 val loss 4.6325\n",
            "The current learning rate 0.00005\n",
            "Epoch 19391 train loss 4.2920 val loss 4.5028\n",
            "The current learning rate 0.00005\n",
            "Epoch 19392 train loss 4.4618 val loss 4.9795\n",
            "The current learning rate 0.00005\n",
            "Epoch 19393 train loss 4.5786 val loss 4.6397\n",
            "The current learning rate 0.00005\n",
            "Epoch 19394 train loss 4.4206 val loss 5.0324\n",
            "The current learning rate 0.00005\n",
            "Epoch 19395 train loss 4.3631 val loss 4.6819\n",
            "The current learning rate 0.00005\n",
            "Epoch 19396 train loss 4.5006 val loss 4.7018\n",
            "The current learning rate 0.00005\n",
            "Epoch 19397 train loss 4.6618 val loss 4.6351\n",
            "The current learning rate 0.00005\n",
            "Epoch 19398 train loss 4.7974 val loss 4.4308\n",
            "The current learning rate 0.00005\n",
            "Epoch 19399 train loss 5.0303 val loss 4.5781\n",
            "The current learning rate 0.00005\n",
            "Epoch 19400 train loss 4.5521 val loss 4.8196\n",
            "The current learning rate 0.00005\n",
            "Epoch 19401 train loss 4.5807 val loss 4.7548\n",
            "The current learning rate 0.00005\n",
            "Epoch 19402 train loss 4.4322 val loss 4.3586\n",
            "The current learning rate 0.00005\n",
            "Epoch 19403 train loss 4.5223 val loss 4.7894\n",
            "The current learning rate 0.00005\n",
            "Epoch 19404 train loss 4.4109 val loss 4.6398\n",
            "The current learning rate 0.00005\n",
            "Epoch 19405 train loss 4.6550 val loss 4.8162\n",
            "The current learning rate 0.00005\n",
            "Epoch 19406 train loss 4.8577 val loss 4.4561\n",
            "The current learning rate 0.00005\n",
            "Epoch 19407 train loss 4.8554 val loss 4.3567\n",
            "The current learning rate 0.00005\n",
            "Epoch 19408 train loss 4.5767 val loss 4.6784\n",
            "The current learning rate 0.00005\n",
            "Epoch 19409 train loss 4.6648 val loss 4.5587\n",
            "The current learning rate 0.00005\n",
            "Epoch 19410 train loss 4.6140 val loss 4.6429\n",
            "The current learning rate 0.00005\n",
            "Epoch 19411 train loss 4.7288 val loss 4.8306\n",
            "The current learning rate 0.00005\n",
            "Epoch 19412 train loss 4.5294 val loss 4.9062\n",
            "The current learning rate 0.00005\n",
            "Epoch 19413 train loss 4.6609 val loss 4.5600\n",
            "The current learning rate 0.00005\n",
            "Epoch 19414 train loss 4.7114 val loss 4.6280\n",
            "The current learning rate 0.00005\n",
            "Epoch 19415 train loss 4.8796 val loss 4.4857\n",
            "The current learning rate 0.00005\n",
            "Epoch 19416 train loss 4.8161 val loss 4.9977\n",
            "The current learning rate 0.00005\n",
            "Epoch 19417 train loss 4.4910 val loss 4.6997\n",
            "The current learning rate 0.00005\n",
            "Epoch 19418 train loss 4.7790 val loss 4.3663\n",
            "The current learning rate 0.00005\n",
            "Epoch 19419 train loss 4.9290 val loss 4.8192\n",
            "The current learning rate 0.00005\n",
            "Epoch 19420 train loss 4.5376 val loss 4.2195\n",
            "The current learning rate 0.00005\n",
            "Epoch 19421 train loss 4.6643 val loss 4.3481\n",
            "The current learning rate 0.00005\n",
            "Epoch 19422 train loss 4.4259 val loss 4.5104\n",
            "The current learning rate 0.00005\n",
            "Epoch 19423 train loss 4.6949 val loss 4.7365\n",
            "The current learning rate 0.00005\n",
            "Epoch 19424 train loss 4.3704 val loss 4.8917\n",
            "The current learning rate 0.00005\n",
            "Epoch 19425 train loss 4.3465 val loss 4.3740\n",
            "The current learning rate 0.00005\n",
            "Epoch 19426 train loss 4.5264 val loss 4.6770\n",
            "The current learning rate 0.00005\n",
            "Epoch 19427 train loss 4.3938 val loss 4.4138\n",
            "The current learning rate 0.00005\n",
            "Epoch 19428 train loss 4.8098 val loss 4.4376\n",
            "The current learning rate 0.00005\n",
            "Epoch 19429 train loss 4.9484 val loss 4.8163\n",
            "The current learning rate 0.00005\n",
            "Epoch 19430 train loss 4.7341 val loss 4.3604\n",
            "The current learning rate 0.00005\n",
            "Epoch 19431 train loss 5.0267 val loss 4.6741\n",
            "The current learning rate 0.00005\n",
            "Epoch 19432 train loss 4.6838 val loss 4.6016\n",
            "The current learning rate 0.00005\n",
            "Epoch 19433 train loss 4.3575 val loss 4.4484\n",
            "The current learning rate 0.00005\n",
            "Epoch 19434 train loss 4.6456 val loss 5.0875\n",
            "The current learning rate 0.00005\n",
            "Epoch 19435 train loss 4.4483 val loss 4.6732\n",
            "The current learning rate 0.00005\n",
            "Epoch 19436 train loss 4.1347 val loss 4.6460\n",
            "The current learning rate 0.00005\n",
            "Epoch 19437 train loss 4.7427 val loss 4.6494\n",
            "The current learning rate 0.00005\n",
            "Epoch 19438 train loss 4.6059 val loss 4.6192\n",
            "The current learning rate 0.00005\n",
            "Epoch 19439 train loss 4.4842 val loss 4.8089\n",
            "The current learning rate 0.00005\n",
            "Epoch 19440 train loss 4.6466 val loss 4.7764\n",
            "The current learning rate 0.00005\n",
            "Epoch 19441 train loss 4.4617 val loss 4.6266\n",
            "The current learning rate 0.00005\n",
            "Epoch 19442 train loss 4.6891 val loss 4.8581\n",
            "The current learning rate 0.00005\n",
            "Epoch 19443 train loss 4.5689 val loss 5.2228\n",
            "The current learning rate 0.00005\n",
            "Epoch 19444 train loss 4.2520 val loss 4.6974\n",
            "The current learning rate 0.00005\n",
            "Epoch 19445 train loss 4.5753 val loss 4.5253\n",
            "The current learning rate 0.00005\n",
            "Epoch 19446 train loss 4.8009 val loss 4.7287\n",
            "The current learning rate 0.00005\n",
            "Epoch 19447 train loss 4.6251 val loss 4.6784\n",
            "The current learning rate 0.00005\n",
            "Epoch 19448 train loss 4.8371 val loss 4.1780\n",
            "The current learning rate 0.00005\n",
            "Epoch 19449 train loss 4.6034 val loss 4.4539\n",
            "The current learning rate 0.00005\n",
            "Epoch 19450 train loss 4.7636 val loss 4.7377\n",
            "The current learning rate 0.00005\n",
            "Epoch 19451 train loss 4.4739 val loss 4.6330\n",
            "The current learning rate 0.00005\n",
            "Epoch 19452 train loss 4.8307 val loss 4.5049\n",
            "The current learning rate 0.00005\n",
            "Epoch 19453 train loss 4.4270 val loss 4.5244\n",
            "The current learning rate 0.00005\n",
            "Epoch 19454 train loss 4.7610 val loss 4.7756\n",
            "The current learning rate 0.00005\n",
            "Epoch 19455 train loss 4.7874 val loss 4.4120\n",
            "The current learning rate 0.00005\n",
            "Epoch 19456 train loss 4.3473 val loss 4.5254\n",
            "The current learning rate 0.00005\n",
            "Epoch 19457 train loss 4.4024 val loss 4.6301\n",
            "The current learning rate 0.00005\n",
            "Epoch 19458 train loss 4.6795 val loss 4.4706\n",
            "The current learning rate 0.00005\n",
            "Epoch 19459 train loss 4.5176 val loss 4.5242\n",
            "The current learning rate 0.00005\n",
            "Epoch 19460 train loss 4.7155 val loss 4.4242\n",
            "The current learning rate 0.00005\n",
            "Epoch 19461 train loss 4.5233 val loss 4.3374\n",
            "The current learning rate 0.00005\n",
            "Epoch 19462 train loss 4.8490 val loss 4.8199\n",
            "The current learning rate 0.00005\n",
            "Epoch 19463 train loss 4.6507 val loss 4.4306\n",
            "The current learning rate 0.00005\n",
            "Epoch 19464 train loss 4.6864 val loss 4.5375\n",
            "The current learning rate 0.00005\n",
            "Epoch 19465 train loss 4.9195 val loss 4.4874\n",
            "The current learning rate 0.00005\n",
            "Epoch 19466 train loss 4.7884 val loss 4.6988\n",
            "The current learning rate 0.00005\n",
            "Epoch 19467 train loss 4.3122 val loss 4.6772\n",
            "The current learning rate 0.00005\n",
            "Epoch 19468 train loss 4.8123 val loss 4.4021\n",
            "The current learning rate 0.00005\n",
            "Epoch 19469 train loss 4.9190 val loss 4.5610\n",
            "The current learning rate 0.00005\n",
            "Epoch 19470 train loss 4.5700 val loss 4.5372\n",
            "The current learning rate 0.00005\n",
            "Epoch 19471 train loss 4.1184 val loss 4.4535\n",
            "The current learning rate 0.00005\n",
            "Epoch 19472 train loss 4.6014 val loss 4.6314\n",
            "The current learning rate 0.00005\n",
            "Epoch 19473 train loss 4.7596 val loss 4.6201\n",
            "The current learning rate 0.00005\n",
            "Epoch 19474 train loss 4.4587 val loss 4.2042\n",
            "The current learning rate 0.00005\n",
            "Epoch 19475 train loss 4.6204 val loss 4.6217\n",
            "The current learning rate 0.00005\n",
            "Epoch 19476 train loss 4.4376 val loss 4.7465\n",
            "The current learning rate 0.00005\n",
            "Epoch 19477 train loss 4.4022 val loss 4.6576\n",
            "The current learning rate 0.00005\n",
            "Epoch 19478 train loss 4.4730 val loss 4.5080\n",
            "The current learning rate 0.00005\n",
            "Epoch 19479 train loss 4.7062 val loss 4.6530\n",
            "The current learning rate 0.00005\n",
            "Epoch 19480 train loss 4.7535 val loss 5.1006\n",
            "The current learning rate 0.00005\n",
            "Epoch 19481 train loss 4.9483 val loss 4.6769\n",
            "The current learning rate 0.00005\n",
            "Epoch 19482 train loss 4.8971 val loss 4.5823\n",
            "The current learning rate 0.00005\n",
            "Epoch 19483 train loss 5.0097 val loss 4.4368\n",
            "The current learning rate 0.00005\n",
            "Epoch 19484 train loss 5.0728 val loss 4.8840\n",
            "The current learning rate 0.00005\n",
            "Epoch 19485 train loss 4.5086 val loss 4.4169\n",
            "The current learning rate 0.00005\n",
            "Epoch 19486 train loss 4.3795 val loss 4.7170\n",
            "The current learning rate 0.00005\n",
            "Epoch 19487 train loss 4.6822 val loss 4.7970\n",
            "The current learning rate 0.00005\n",
            "Epoch 19488 train loss 4.1721 val loss 4.7310\n",
            "The current learning rate 0.00005\n",
            "Epoch 19489 train loss 4.2226 val loss 4.4256\n",
            "The current learning rate 0.00005\n",
            "Epoch 19490 train loss 4.5323 val loss 4.3633\n",
            "The current learning rate 0.00005\n",
            "Epoch 19491 train loss 4.4276 val loss 4.7705\n",
            "The current learning rate 0.00005\n",
            "Epoch 19492 train loss 4.6118 val loss 4.2091\n",
            "The current learning rate 0.00005\n",
            "Epoch 19493 train loss 4.5434 val loss 4.7881\n",
            "The current learning rate 0.00005\n",
            "Epoch 19494 train loss 4.7454 val loss 4.7092\n",
            "The current learning rate 0.00005\n",
            "Epoch 19495 train loss 4.5927 val loss 4.7783\n",
            "The current learning rate 0.00005\n",
            "Epoch 19496 train loss 4.6476 val loss 4.5697\n",
            "The current learning rate 0.00005\n",
            "Epoch 19497 train loss 4.5197 val loss 4.6778\n",
            "The current learning rate 0.00005\n",
            "Epoch 19498 train loss 4.7300 val loss 4.7440\n",
            "The current learning rate 0.00005\n",
            "Epoch 19499 train loss 4.8336 val loss 4.8059\n",
            "The current learning rate 0.00005\n",
            "Epoch 19500 train loss 4.6180 val loss 4.8540\n",
            "The current learning rate 0.00005\n",
            "Epoch 19501 train loss 4.5926 val loss 4.7406\n",
            "The current learning rate 0.00005\n",
            "Epoch 19502 train loss 4.6378 val loss 4.1152\n",
            "The current learning rate 0.00005\n",
            "Epoch 19503 train loss 4.4166 val loss 4.9341\n",
            "The current learning rate 0.00005\n",
            "Epoch 19504 train loss 4.5180 val loss 4.6361\n",
            "The current learning rate 0.00005\n",
            "Epoch 19505 train loss 4.5126 val loss 4.4326\n",
            "The current learning rate 0.00005\n",
            "Epoch 19506 train loss 4.6468 val loss 4.5838\n",
            "The current learning rate 0.00005\n",
            "Epoch 19507 train loss 4.7616 val loss 4.5135\n",
            "The current learning rate 0.00005\n",
            "Epoch 19508 train loss 4.4673 val loss 5.1948\n",
            "The current learning rate 0.00005\n",
            "Epoch 19509 train loss 4.8201 val loss 4.7864\n",
            "The current learning rate 0.00005\n",
            "Epoch 19510 train loss 4.7073 val loss 4.6749\n",
            "The current learning rate 0.00005\n",
            "Epoch 19511 train loss 4.3798 val loss 4.6559\n",
            "The current learning rate 0.00005\n",
            "Epoch 19512 train loss 4.5789 val loss 4.5622\n",
            "The current learning rate 0.00005\n",
            "Epoch 19513 train loss 4.7376 val loss 4.5133\n",
            "The current learning rate 0.00005\n",
            "Epoch 19514 train loss 4.5322 val loss 4.9161\n",
            "The current learning rate 0.00005\n",
            "Epoch 19515 train loss 4.8491 val loss 4.7052\n",
            "The current learning rate 0.00005\n",
            "Epoch 19516 train loss 4.5029 val loss 4.9804\n",
            "The current learning rate 0.00005\n",
            "Epoch 19517 train loss 4.4317 val loss 4.4275\n",
            "The current learning rate 0.00005\n",
            "Epoch 19518 train loss 4.8869 val loss 4.7334\n",
            "The current learning rate 0.00005\n",
            "Epoch 19519 train loss 4.4495 val loss 4.6451\n",
            "The current learning rate 0.00005\n",
            "Epoch 19520 train loss 4.9405 val loss 4.7671\n",
            "The current learning rate 0.00005\n",
            "Epoch 19521 train loss 4.6712 val loss 4.7995\n",
            "The current learning rate 0.00005\n",
            "Epoch 19522 train loss 4.3293 val loss 4.7268\n",
            "The current learning rate 0.00005\n",
            "Epoch 19523 train loss 4.9897 val loss 4.3017\n",
            "The current learning rate 0.00005\n",
            "Epoch 19524 train loss 4.6161 val loss 4.9584\n",
            "The current learning rate 0.00005\n",
            "Epoch 19525 train loss 4.7780 val loss 4.7723\n",
            "The current learning rate 0.00005\n",
            "Epoch 19526 train loss 5.1080 val loss 5.0519\n",
            "The current learning rate 0.00005\n",
            "Epoch 19527 train loss 4.6092 val loss 4.4527\n",
            "The current learning rate 0.00005\n",
            "Epoch 19528 train loss 4.9559 val loss 4.2081\n",
            "The current learning rate 0.00005\n",
            "Epoch 19529 train loss 4.8054 val loss 4.6488\n",
            "The current learning rate 0.00005\n",
            "Epoch 19530 train loss 4.5301 val loss 4.4195\n",
            "The current learning rate 0.00005\n",
            "Epoch 19531 train loss 4.3785 val loss 4.5688\n",
            "The current learning rate 0.00005\n",
            "Epoch 19532 train loss 4.9099 val loss 4.4718\n",
            "The current learning rate 0.00005\n",
            "Epoch 19533 train loss 4.4929 val loss 4.6010\n",
            "The current learning rate 0.00005\n",
            "Epoch 19534 train loss 4.8859 val loss 4.9517\n",
            "The current learning rate 0.00005\n",
            "Epoch 19535 train loss 4.5330 val loss 4.7547\n",
            "The current learning rate 0.00005\n",
            "Epoch 19536 train loss 4.4845 val loss 4.9281\n",
            "The current learning rate 0.00005\n",
            "Epoch 19537 train loss 4.4909 val loss 4.5712\n",
            "The current learning rate 0.00005\n",
            "Epoch 19538 train loss 4.7764 val loss 4.3712\n",
            "The current learning rate 0.00005\n",
            "Epoch 19539 train loss 4.6699 val loss 4.2611\n",
            "The current learning rate 0.00005\n",
            "Epoch 19540 train loss 4.5669 val loss 4.7501\n",
            "The current learning rate 0.00005\n",
            "Epoch 19541 train loss 4.7717 val loss 4.7064\n",
            "The current learning rate 0.00005\n",
            "Epoch 19542 train loss 4.5138 val loss 4.3911\n",
            "The current learning rate 0.00005\n",
            "Epoch 19543 train loss 4.4493 val loss 4.2648\n",
            "The current learning rate 0.00005\n",
            "Epoch 19544 train loss 4.5522 val loss 4.5498\n",
            "The current learning rate 0.00005\n",
            "Epoch 19545 train loss 4.8992 val loss 4.6531\n",
            "The current learning rate 0.00005\n",
            "Epoch 19546 train loss 4.5085 val loss 4.5835\n",
            "The current learning rate 0.00005\n",
            "Epoch 19547 train loss 4.6203 val loss 5.0546\n",
            "The current learning rate 0.00005\n",
            "Epoch 19548 train loss 5.0451 val loss 4.7715\n",
            "The current learning rate 0.00005\n",
            "Epoch 19549 train loss 4.7600 val loss 4.3145\n",
            "The current learning rate 0.00005\n",
            "Epoch 19550 train loss 4.5531 val loss 4.4057\n",
            "The current learning rate 0.00005\n",
            "Epoch 19551 train loss 4.5710 val loss 4.5434\n",
            "The current learning rate 0.00005\n",
            "Epoch 19552 train loss 4.6555 val loss 4.6143\n",
            "The current learning rate 0.00005\n",
            "Epoch 19553 train loss 4.7426 val loss 5.1496\n",
            "The current learning rate 0.00005\n",
            "Epoch 19554 train loss 4.7533 val loss 4.8169\n",
            "The current learning rate 0.00005\n",
            "Epoch 19555 train loss 4.3848 val loss 4.5661\n",
            "The current learning rate 0.00005\n",
            "Epoch 19556 train loss 4.9166 val loss 4.7248\n",
            "The current learning rate 0.00005\n",
            "Epoch 19557 train loss 4.4227 val loss 5.1961\n",
            "The current learning rate 0.00005\n",
            "Epoch 19558 train loss 3.9175 val loss 4.7860\n",
            "The current learning rate 0.00005\n",
            "Epoch 19559 train loss 4.5626 val loss 4.5300\n",
            "The current learning rate 0.00005\n",
            "Epoch 19560 train loss 4.6198 val loss 4.9404\n",
            "The current learning rate 0.00005\n",
            "Epoch 19561 train loss 4.7418 val loss 4.7625\n",
            "The current learning rate 0.00005\n",
            "Epoch 19562 train loss 4.8592 val loss 4.7232\n",
            "The current learning rate 0.00005\n",
            "Epoch 19563 train loss 4.7857 val loss 4.5411\n",
            "The current learning rate 0.00005\n",
            "Epoch 19564 train loss 4.8118 val loss 4.2844\n",
            "The current learning rate 0.00005\n",
            "Epoch 19565 train loss 4.6631 val loss 4.5950\n",
            "The current learning rate 0.00005\n",
            "Epoch 19566 train loss 5.0042 val loss 4.8168\n",
            "The current learning rate 0.00005\n",
            "Epoch 19567 train loss 4.8243 val loss 4.4109\n",
            "The current learning rate 0.00005\n",
            "Epoch 19568 train loss 4.4824 val loss 4.8219\n",
            "The current learning rate 0.00005\n",
            "Epoch 19569 train loss 4.7778 val loss 4.9884\n",
            "The current learning rate 0.00005\n",
            "Epoch 19570 train loss 4.7617 val loss 4.5825\n",
            "The current learning rate 0.00005\n",
            "Epoch 19571 train loss 4.9575 val loss 5.0763\n",
            "The current learning rate 0.00005\n",
            "Epoch 19572 train loss 4.5418 val loss 4.4459\n",
            "The current learning rate 0.00005\n",
            "Epoch 19573 train loss 4.5268 val loss 4.3843\n",
            "The current learning rate 0.00005\n",
            "Epoch 19574 train loss 4.5534 val loss 4.1398\n",
            "The current learning rate 0.00005\n",
            "Epoch 19575 train loss 4.1835 val loss 4.5034\n",
            "The current learning rate 0.00005\n",
            "Epoch 19576 train loss 4.7723 val loss 4.3495\n",
            "The current learning rate 0.00005\n",
            "Epoch 19577 train loss 5.1823 val loss 4.7807\n",
            "The current learning rate 0.00005\n",
            "Epoch 19578 train loss 4.8854 val loss 4.7318\n",
            "The current learning rate 0.00005\n",
            "Epoch 19579 train loss 5.0089 val loss 4.8598\n",
            "The current learning rate 0.00005\n",
            "Epoch 19580 train loss 4.5724 val loss 4.6354\n",
            "The current learning rate 0.00005\n",
            "Epoch 19581 train loss 4.6364 val loss 4.5553\n",
            "The current learning rate 0.00005\n",
            "Epoch 19582 train loss 4.3228 val loss 4.3777\n",
            "The current learning rate 0.00005\n",
            "Epoch 19583 train loss 4.5116 val loss 4.7664\n",
            "The current learning rate 0.00005\n",
            "Epoch 19584 train loss 4.7753 val loss 4.7611\n",
            "The current learning rate 0.00005\n",
            "Epoch 19585 train loss 4.6487 val loss 4.7854\n",
            "The current learning rate 0.00005\n",
            "Epoch 19586 train loss 4.8069 val loss 4.8330\n",
            "The current learning rate 0.00005\n",
            "Epoch 19587 train loss 4.4957 val loss 5.2542\n",
            "The current learning rate 0.00005\n",
            "Epoch 19588 train loss 4.6555 val loss 4.8769\n",
            "The current learning rate 0.00005\n",
            "Epoch 19589 train loss 4.6945 val loss 4.4963\n",
            "The current learning rate 0.00005\n",
            "Epoch 19590 train loss 4.6819 val loss 4.7127\n",
            "The current learning rate 0.00005\n",
            "Epoch 19591 train loss 4.7268 val loss 4.2821\n",
            "The current learning rate 0.00005\n",
            "Epoch 19592 train loss 4.5190 val loss 4.6439\n",
            "The current learning rate 0.00005\n",
            "Epoch 19593 train loss 4.3625 val loss 4.4918\n",
            "The current learning rate 0.00005\n",
            "Epoch 19594 train loss 4.8543 val loss 4.9122\n",
            "The current learning rate 0.00005\n",
            "Epoch 19595 train loss 4.7499 val loss 4.7269\n",
            "The current learning rate 0.00005\n",
            "Epoch 19596 train loss 4.6491 val loss 4.9018\n",
            "The current learning rate 0.00005\n",
            "Epoch 19597 train loss 4.4957 val loss 4.4321\n",
            "The current learning rate 0.00005\n",
            "Epoch 19598 train loss 4.2463 val loss 4.4873\n",
            "The current learning rate 0.00005\n",
            "Epoch 19599 train loss 4.8206 val loss 4.5345\n",
            "The current learning rate 0.00005\n",
            "Epoch 19600 train loss 4.9593 val loss 4.3362\n",
            "The current learning rate 0.00005\n",
            "Epoch 19601 train loss 4.5587 val loss 4.2288\n",
            "The current learning rate 0.00005\n",
            "Epoch 19602 train loss 4.5943 val loss 4.3728\n",
            "The current learning rate 0.00005\n",
            "Epoch 19603 train loss 4.7290 val loss 4.5066\n",
            "The current learning rate 0.00005\n",
            "Epoch 19604 train loss 4.6275 val loss 4.8779\n",
            "The current learning rate 0.00005\n",
            "Epoch 19605 train loss 4.8378 val loss 4.4350\n",
            "The current learning rate 0.00005\n",
            "Epoch 19606 train loss 4.6654 val loss 4.4663\n",
            "The current learning rate 0.00005\n",
            "Epoch 19607 train loss 4.5888 val loss 4.8067\n",
            "The current learning rate 0.00005\n",
            "Epoch 19608 train loss 4.4273 val loss 4.5189\n",
            "The current learning rate 0.00005\n",
            "Epoch 19609 train loss 4.5364 val loss 4.3807\n",
            "The current learning rate 0.00005\n",
            "Epoch 19610 train loss 4.6733 val loss 4.7501\n",
            "The current learning rate 0.00005\n",
            "Epoch 19611 train loss 4.4320 val loss 4.7975\n",
            "The current learning rate 0.00005\n",
            "Epoch 19612 train loss 4.6566 val loss 4.6063\n",
            "The current learning rate 0.00005\n",
            "Epoch 19613 train loss 4.7105 val loss 4.7325\n",
            "The current learning rate 0.00005\n",
            "Epoch 19614 train loss 4.7406 val loss 5.0364\n",
            "The current learning rate 0.00005\n",
            "Epoch 19615 train loss 4.5834 val loss 4.6659\n",
            "The current learning rate 0.00005\n",
            "Epoch 19616 train loss 4.6356 val loss 4.5690\n",
            "The current learning rate 0.00005\n",
            "Epoch 19617 train loss 4.4793 val loss 4.8997\n",
            "The current learning rate 0.00005\n",
            "Epoch 19618 train loss 4.8587 val loss 4.6784\n",
            "The current learning rate 0.00005\n",
            "Epoch 19619 train loss 4.6638 val loss 4.2937\n",
            "The current learning rate 0.00005\n",
            "Epoch 19620 train loss 4.4806 val loss 4.8793\n",
            "The current learning rate 0.00005\n",
            "Epoch 19621 train loss 4.6985 val loss 4.7890\n",
            "The current learning rate 0.00005\n",
            "Epoch 19622 train loss 4.6453 val loss 4.8841\n",
            "The current learning rate 0.00005\n",
            "Epoch 19623 train loss 4.5723 val loss 4.7249\n",
            "The current learning rate 0.00005\n",
            "Epoch 19624 train loss 4.3321 val loss 4.9287\n",
            "The current learning rate 0.00005\n",
            "Epoch 19625 train loss 4.4747 val loss 4.7676\n",
            "The current learning rate 0.00005\n",
            "Epoch 19626 train loss 4.6604 val loss 4.8194\n",
            "The current learning rate 0.00005\n",
            "Epoch 19627 train loss 4.5024 val loss 4.7857\n",
            "The current learning rate 0.00005\n",
            "Epoch 19628 train loss 4.7333 val loss 4.4066\n",
            "The current learning rate 0.00005\n",
            "Epoch 19629 train loss 4.4938 val loss 4.2882\n",
            "The current learning rate 0.00005\n",
            "Epoch 19630 train loss 4.4290 val loss 4.8457\n",
            "The current learning rate 0.00005\n",
            "Epoch 19631 train loss 4.7906 val loss 4.6618\n",
            "The current learning rate 0.00005\n",
            "Epoch 19632 train loss 4.8161 val loss 4.8321\n",
            "The current learning rate 0.00005\n",
            "Epoch 19633 train loss 4.7632 val loss 4.5215\n",
            "The current learning rate 0.00005\n",
            "Epoch 19634 train loss 4.9432 val loss 4.5167\n",
            "The current learning rate 0.00005\n",
            "Epoch 19635 train loss 4.2692 val loss 4.4957\n",
            "The current learning rate 0.00005\n",
            "Epoch 19636 train loss 4.6049 val loss 4.6810\n",
            "The current learning rate 0.00005\n",
            "Epoch 19637 train loss 4.6020 val loss 4.7432\n",
            "The current learning rate 0.00005\n",
            "Epoch 19638 train loss 4.6534 val loss 4.4698\n",
            "The current learning rate 0.00005\n",
            "Epoch 19639 train loss 4.9536 val loss 4.2998\n",
            "The current learning rate 0.00005\n",
            "Epoch 19640 train loss 4.4462 val loss 4.7107\n",
            "The current learning rate 0.00005\n",
            "Epoch 19641 train loss 4.3153 val loss 4.4936\n",
            "The current learning rate 0.00005\n",
            "Epoch 19642 train loss 4.4601 val loss 4.3370\n",
            "The current learning rate 0.00005\n",
            "Epoch 19643 train loss 4.5894 val loss 4.5381\n",
            "The current learning rate 0.00005\n",
            "Epoch 19644 train loss 5.0572 val loss 4.7018\n",
            "The current learning rate 0.00005\n",
            "Epoch 19645 train loss 4.8251 val loss 4.6028\n",
            "The current learning rate 0.00005\n",
            "Epoch 19646 train loss 4.6583 val loss 4.7237\n",
            "The current learning rate 0.00005\n",
            "Epoch 19647 train loss 4.7437 val loss 4.8889\n",
            "The current learning rate 0.00005\n",
            "Epoch 19648 train loss 4.5252 val loss 4.6855\n",
            "The current learning rate 0.00005\n",
            "Epoch 19649 train loss 4.5922 val loss 4.7244\n",
            "The current learning rate 0.00005\n",
            "Epoch 19650 train loss 4.6868 val loss 5.0525\n",
            "The current learning rate 0.00005\n",
            "Epoch 19651 train loss 4.7226 val loss 4.6155\n",
            "The current learning rate 0.00005\n",
            "Epoch 19652 train loss 4.1560 val loss 4.4680\n",
            "The current learning rate 0.00005\n",
            "Epoch 19653 train loss 4.5275 val loss 4.5258\n",
            "The current learning rate 0.00005\n",
            "Epoch 19654 train loss 4.4913 val loss 4.5337\n",
            "The current learning rate 0.00005\n",
            "Epoch 19655 train loss 4.5916 val loss 4.9786\n",
            "The current learning rate 0.00005\n",
            "Epoch 19656 train loss 4.5838 val loss 4.6212\n",
            "The current learning rate 0.00005\n",
            "Epoch 19657 train loss 4.5131 val loss 4.5488\n",
            "The current learning rate 0.00005\n",
            "Epoch 19658 train loss 4.5861 val loss 4.8329\n",
            "The current learning rate 0.00005\n",
            "Epoch 19659 train loss 4.6172 val loss 4.3426\n",
            "The current learning rate 0.00005\n",
            "Epoch 19660 train loss 4.4555 val loss 4.7903\n",
            "The current learning rate 0.00005\n",
            "Epoch 19661 train loss 4.2060 val loss 4.6689\n",
            "The current learning rate 0.00005\n",
            "Epoch 19662 train loss 4.6694 val loss 4.4966\n",
            "The current learning rate 0.00005\n",
            "Epoch 19663 train loss 4.1880 val loss 4.9279\n",
            "The current learning rate 0.00005\n",
            "Epoch 19664 train loss 4.9814 val loss 4.7473\n",
            "The current learning rate 0.00005\n",
            "Epoch 19665 train loss 4.5365 val loss 4.4461\n",
            "The current learning rate 0.00005\n",
            "Epoch 19666 train loss 4.4134 val loss 4.9335\n",
            "The current learning rate 0.00005\n",
            "Epoch 19667 train loss 4.6253 val loss 4.6153\n",
            "The current learning rate 0.00005\n",
            "Epoch 19668 train loss 4.6414 val loss 4.6565\n",
            "The current learning rate 0.00005\n",
            "Epoch 19669 train loss 4.9014 val loss 4.5405\n",
            "The current learning rate 0.00005\n",
            "Epoch 19670 train loss 4.8440 val loss 4.9552\n",
            "The current learning rate 0.00005\n",
            "Epoch 19671 train loss 4.4486 val loss 4.4785\n",
            "The current learning rate 0.00005\n",
            "Epoch 19672 train loss 4.5384 val loss 4.9326\n",
            "The current learning rate 0.00005\n",
            "Epoch 19673 train loss 5.2380 val loss 4.5224\n",
            "The current learning rate 0.00005\n",
            "Epoch 19674 train loss 4.8140 val loss 4.6971\n",
            "The current learning rate 0.00005\n",
            "Epoch 19675 train loss 4.5843 val loss 4.4790\n",
            "The current learning rate 0.00005\n",
            "Epoch 19676 train loss 4.3236 val loss 4.7853\n",
            "The current learning rate 0.00005\n",
            "Epoch 19677 train loss 4.2944 val loss 4.5847\n",
            "The current learning rate 0.00005\n",
            "Epoch 19678 train loss 4.8636 val loss 4.2826\n",
            "The current learning rate 0.00005\n",
            "Epoch 19679 train loss 4.5382 val loss 4.3453\n",
            "The current learning rate 0.00005\n",
            "Epoch 19680 train loss 4.6193 val loss 4.2632\n",
            "The current learning rate 0.00005\n",
            "Epoch 19681 train loss 4.7268 val loss 4.3883\n",
            "The current learning rate 0.00005\n",
            "Epoch 19682 train loss 4.1862 val loss 4.7501\n",
            "The current learning rate 0.00005\n",
            "Epoch 19683 train loss 4.5160 val loss 4.5019\n",
            "The current learning rate 0.00005\n",
            "Epoch 19684 train loss 4.6162 val loss 4.5835\n",
            "The current learning rate 0.00005\n",
            "Epoch 19685 train loss 4.8521 val loss 4.5952\n",
            "The current learning rate 0.00005\n",
            "Epoch 19686 train loss 4.5300 val loss 4.8118\n",
            "The current learning rate 0.00005\n",
            "Epoch 19687 train loss 4.3792 val loss 4.6277\n",
            "The current learning rate 0.00005\n",
            "Epoch 19688 train loss 4.7432 val loss 4.6567\n",
            "The current learning rate 0.00005\n",
            "Epoch 19689 train loss 4.3976 val loss 4.2611\n",
            "The current learning rate 0.00005\n",
            "Epoch 19690 train loss 4.6616 val loss 4.7186\n",
            "The current learning rate 0.00005\n",
            "Epoch 19691 train loss 4.7127 val loss 4.5799\n",
            "The current learning rate 0.00005\n",
            "Epoch 19692 train loss 4.8586 val loss 4.2868\n",
            "The current learning rate 0.00005\n",
            "Epoch 19693 train loss 4.4532 val loss 4.7383\n",
            "The current learning rate 0.00005\n",
            "Epoch 19694 train loss 4.6957 val loss 4.4968\n",
            "The current learning rate 0.00005\n",
            "Epoch 19695 train loss 4.6810 val loss 4.6515\n",
            "The current learning rate 0.00005\n",
            "Epoch 19696 train loss 4.9092 val loss 4.6962\n",
            "The current learning rate 0.00005\n",
            "Epoch 19697 train loss 4.7326 val loss 4.8257\n",
            "The current learning rate 0.00005\n",
            "Epoch 19698 train loss 4.8202 val loss 4.6983\n",
            "The current learning rate 0.00005\n",
            "Epoch 19699 train loss 4.7415 val loss 4.6531\n",
            "The current learning rate 0.00005\n",
            "Epoch 19700 train loss 4.5504 val loss 4.5263\n",
            "The current learning rate 0.00005\n",
            "Epoch 19701 train loss 4.7231 val loss 4.8577\n",
            "The current learning rate 0.00005\n",
            "Epoch 19702 train loss 4.5426 val loss 4.8062\n",
            "The current learning rate 0.00005\n",
            "Epoch 19703 train loss 4.9623 val loss 4.5056\n",
            "The current learning rate 0.00005\n",
            "Epoch 19704 train loss 4.9384 val loss 4.0392\n",
            "The current learning rate 0.00005\n",
            "Epoch 19705 train loss 4.7148 val loss 4.3384\n",
            "The current learning rate 0.00005\n",
            "Epoch 19706 train loss 4.1687 val loss 5.0681\n",
            "The current learning rate 0.00005\n",
            "Epoch 19707 train loss 5.1492 val loss 4.7147\n",
            "The current learning rate 0.00005\n",
            "Epoch 19708 train loss 4.5577 val loss 4.7264\n",
            "The current learning rate 0.00005\n",
            "Epoch 19709 train loss 4.8112 val loss 4.8649\n",
            "The current learning rate 0.00005\n",
            "Epoch 19710 train loss 4.1850 val loss 5.0836\n",
            "The current learning rate 0.00005\n",
            "Epoch 19711 train loss 4.8574 val loss 4.3084\n",
            "The current learning rate 0.00005\n",
            "Epoch 19712 train loss 4.7416 val loss 4.7012\n",
            "The current learning rate 0.00005\n",
            "Epoch 19713 train loss 4.4975 val loss 4.5142\n",
            "The current learning rate 0.00005\n",
            "Epoch 19714 train loss 4.3582 val loss 4.8180\n",
            "The current learning rate 0.00005\n",
            "Epoch 19715 train loss 4.5337 val loss 4.8425\n",
            "The current learning rate 0.00005\n",
            "Epoch 19716 train loss 4.4593 val loss 4.9005\n",
            "The current learning rate 0.00005\n",
            "Epoch 19717 train loss 4.5819 val loss 4.2549\n",
            "The current learning rate 0.00005\n",
            "Epoch 19718 train loss 5.0065 val loss 4.4172\n",
            "The current learning rate 0.00005\n",
            "Epoch 19719 train loss 4.3110 val loss 4.3944\n",
            "The current learning rate 0.00005\n",
            "Epoch 19720 train loss 4.8037 val loss 4.7020\n",
            "The current learning rate 0.00005\n",
            "Epoch 19721 train loss 4.7592 val loss 4.5337\n",
            "The current learning rate 0.00005\n",
            "Epoch 19722 train loss 4.6196 val loss 4.9561\n",
            "The current learning rate 0.00005\n",
            "Epoch 19723 train loss 4.5761 val loss 4.4230\n",
            "The current learning rate 0.00005\n",
            "Epoch 19724 train loss 4.5256 val loss 4.7071\n",
            "The current learning rate 0.00005\n",
            "Epoch 19725 train loss 4.5784 val loss 4.3923\n",
            "The current learning rate 0.00005\n",
            "Epoch 19726 train loss 5.1343 val loss 4.8866\n",
            "The current learning rate 0.00005\n",
            "Epoch 19727 train loss 4.6229 val loss 4.6419\n",
            "The current learning rate 0.00005\n",
            "Epoch 19728 train loss 4.3867 val loss 4.4360\n",
            "The current learning rate 0.00005\n",
            "Epoch 19729 train loss 4.4252 val loss 4.7656\n",
            "The current learning rate 0.00005\n",
            "Epoch 19730 train loss 4.6799 val loss 4.8662\n",
            "The current learning rate 0.00005\n",
            "Epoch 19731 train loss 4.8003 val loss 4.3975\n",
            "The current learning rate 0.00005\n",
            "Epoch 19732 train loss 4.5272 val loss 4.7380\n",
            "The current learning rate 0.00005\n",
            "Epoch 19733 train loss 5.2274 val loss 4.4575\n",
            "The current learning rate 0.00005\n",
            "Epoch 19734 train loss 4.6147 val loss 4.5684\n",
            "The current learning rate 0.00005\n",
            "Epoch 19735 train loss 4.8778 val loss 4.4911\n",
            "The current learning rate 0.00005\n",
            "Epoch 19736 train loss 4.8475 val loss 4.6613\n",
            "The current learning rate 0.00005\n",
            "Epoch 19737 train loss 4.9369 val loss 4.5274\n",
            "The current learning rate 0.00005\n",
            "Epoch 19738 train loss 4.6806 val loss 4.9391\n",
            "The current learning rate 0.00005\n",
            "Epoch 19739 train loss 4.5520 val loss 4.4611\n",
            "The current learning rate 0.00005\n",
            "Epoch 19740 train loss 5.0194 val loss 4.5934\n",
            "The current learning rate 0.00005\n",
            "Epoch 19741 train loss 4.7584 val loss 4.4122\n",
            "The current learning rate 0.00005\n",
            "Epoch 19742 train loss 4.5221 val loss 4.4105\n",
            "The current learning rate 0.00005\n",
            "Epoch 19743 train loss 4.5660 val loss 4.3557\n",
            "The current learning rate 0.00005\n",
            "Epoch 19744 train loss 4.6346 val loss 5.0766\n",
            "The current learning rate 0.00005\n",
            "Epoch 19745 train loss 4.7626 val loss 4.7517\n",
            "The current learning rate 0.00005\n",
            "Epoch 19746 train loss 4.5208 val loss 4.7130\n",
            "The current learning rate 0.00005\n",
            "Epoch 19747 train loss 4.7162 val loss 4.5430\n",
            "The current learning rate 0.00005\n",
            "Epoch 19748 train loss 4.8367 val loss 4.3730\n",
            "The current learning rate 0.00005\n",
            "Epoch 19749 train loss 4.4734 val loss 4.8884\n",
            "The current learning rate 0.00005\n",
            "Epoch 19750 train loss 4.8858 val loss 4.6535\n",
            "The current learning rate 0.00005\n",
            "Epoch 19751 train loss 4.5884 val loss 4.7836\n",
            "The current learning rate 0.00005\n",
            "Epoch 19752 train loss 4.4905 val loss 4.8971\n",
            "The current learning rate 0.00005\n",
            "Epoch 19753 train loss 4.8036 val loss 4.6249\n",
            "The current learning rate 0.00005\n",
            "Epoch 19754 train loss 4.8818 val loss 4.4550\n",
            "The current learning rate 0.00005\n",
            "Epoch 19755 train loss 4.3560 val loss 4.7387\n",
            "The current learning rate 0.00005\n",
            "Epoch 19756 train loss 4.5436 val loss 4.9280\n",
            "The current learning rate 0.00005\n",
            "Epoch 19757 train loss 4.8384 val loss 4.6610\n",
            "The current learning rate 0.00005\n",
            "Epoch 19758 train loss 5.0283 val loss 4.5948\n",
            "The current learning rate 0.00005\n",
            "Epoch 19759 train loss 4.7981 val loss 4.9592\n",
            "The current learning rate 0.00005\n",
            "Epoch 19760 train loss 4.6582 val loss 4.9907\n",
            "The current learning rate 0.00005\n",
            "Epoch 19761 train loss 4.2720 val loss 4.4752\n",
            "The current learning rate 0.00005\n",
            "Epoch 19762 train loss 4.3076 val loss 4.7103\n",
            "The current learning rate 0.00005\n",
            "Epoch 19763 train loss 5.0246 val loss 4.5787\n",
            "The current learning rate 0.00005\n",
            "Epoch 19764 train loss 4.8726 val loss 4.6796\n",
            "The current learning rate 0.00005\n",
            "Epoch 19765 train loss 5.0948 val loss 4.7172\n",
            "The current learning rate 0.00005\n",
            "Epoch 19766 train loss 4.4816 val loss 4.3923\n",
            "The current learning rate 0.00005\n",
            "Epoch 19767 train loss 4.3705 val loss 4.7390\n",
            "The current learning rate 0.00005\n",
            "Epoch 19768 train loss 4.2683 val loss 4.6574\n",
            "The current learning rate 0.00005\n",
            "Epoch 19769 train loss 4.6483 val loss 4.7295\n",
            "The current learning rate 0.00005\n",
            "Epoch 19770 train loss 4.1831 val loss 4.3972\n",
            "The current learning rate 0.00005\n",
            "Epoch 19771 train loss 4.7440 val loss 4.3866\n",
            "The current learning rate 0.00005\n",
            "Epoch 19772 train loss 4.5078 val loss 4.4720\n",
            "The current learning rate 0.00005\n",
            "Epoch 19773 train loss 4.6402 val loss 4.5773\n",
            "The current learning rate 0.00005\n",
            "Epoch 19774 train loss 4.8949 val loss 4.4999\n",
            "The current learning rate 0.00005\n",
            "Epoch 19775 train loss 4.9528 val loss 4.6527\n",
            "The current learning rate 0.00005\n",
            "Epoch 19776 train loss 4.4815 val loss 4.4102\n",
            "The current learning rate 0.00005\n",
            "Epoch 19777 train loss 4.7599 val loss 4.4060\n",
            "The current learning rate 0.00005\n",
            "Epoch 19778 train loss 4.4450 val loss 4.5857\n",
            "The current learning rate 0.00005\n",
            "Epoch 19779 train loss 4.5190 val loss 4.7090\n",
            "The current learning rate 0.00005\n",
            "Epoch 19780 train loss 4.5695 val loss 4.2729\n",
            "The current learning rate 0.00005\n",
            "Epoch 19781 train loss 4.7922 val loss 4.5308\n",
            "The current learning rate 0.00005\n",
            "Epoch 19782 train loss 4.6233 val loss 4.7479\n",
            "The current learning rate 0.00005\n",
            "Epoch 19783 train loss 4.3505 val loss 4.7794\n",
            "The current learning rate 0.00005\n",
            "Epoch 19784 train loss 4.5278 val loss 4.3492\n",
            "The current learning rate 0.00005\n",
            "Epoch 19785 train loss 4.3778 val loss 4.6471\n",
            "The current learning rate 0.00005\n",
            "Epoch 19786 train loss 4.7182 val loss 4.5514\n",
            "The current learning rate 0.00005\n",
            "Epoch 19787 train loss 4.6904 val loss 4.3085\n",
            "The current learning rate 0.00005\n",
            "Epoch 19788 train loss 4.7986 val loss 4.7743\n",
            "The current learning rate 0.00005\n",
            "Epoch 19789 train loss 4.6890 val loss 5.0986\n",
            "The current learning rate 0.00005\n",
            "Epoch 19790 train loss 4.5661 val loss 4.5634\n",
            "The current learning rate 0.00005\n",
            "Epoch 19791 train loss 4.8182 val loss 4.9926\n",
            "The current learning rate 0.00005\n",
            "Epoch 19792 train loss 4.5507 val loss 4.6417\n",
            "The current learning rate 0.00005\n",
            "Epoch 19793 train loss 4.8980 val loss 4.7944\n",
            "The current learning rate 0.00005\n",
            "Epoch 19794 train loss 4.5343 val loss 4.5140\n",
            "The current learning rate 0.00005\n",
            "Epoch 19795 train loss 4.4340 val loss 4.7175\n",
            "The current learning rate 0.00005\n",
            "Epoch 19796 train loss 4.5781 val loss 4.5404\n",
            "The current learning rate 0.00005\n",
            "Epoch 19797 train loss 4.9406 val loss 4.7655\n",
            "The current learning rate 0.00005\n",
            "Epoch 19798 train loss 4.3041 val loss 4.5599\n",
            "The current learning rate 0.00005\n",
            "Epoch 19799 train loss 4.8822 val loss 4.5862\n",
            "The current learning rate 0.00005\n",
            "Epoch 19800 train loss 4.7239 val loss 4.8171\n",
            "The current learning rate 0.00005\n",
            "Epoch 19801 train loss 4.5327 val loss 4.6525\n",
            "The current learning rate 0.00005\n",
            "Epoch 19802 train loss 4.3882 val loss 4.8858\n",
            "The current learning rate 0.00005\n",
            "Epoch 19803 train loss 4.3703 val loss 4.6546\n",
            "The current learning rate 0.00005\n",
            "Epoch 19804 train loss 4.6604 val loss 4.7688\n",
            "The current learning rate 0.00005\n",
            "Epoch 19805 train loss 4.6061 val loss 4.5929\n",
            "The current learning rate 0.00005\n",
            "Epoch 19806 train loss 4.7366 val loss 4.3875\n",
            "The current learning rate 0.00005\n",
            "Epoch 19807 train loss 4.6517 val loss 4.3937\n",
            "The current learning rate 0.00005\n",
            "Epoch 19808 train loss 4.4379 val loss 4.5150\n",
            "The current learning rate 0.00005\n",
            "Epoch 19809 train loss 4.4760 val loss 4.6180\n",
            "The current learning rate 0.00005\n",
            "Epoch 19810 train loss 4.7920 val loss 4.6032\n",
            "The current learning rate 0.00005\n",
            "Epoch 19811 train loss 4.4565 val loss 4.5863\n",
            "The current learning rate 0.00005\n",
            "Epoch 19812 train loss 4.7002 val loss 4.5703\n",
            "The current learning rate 0.00005\n",
            "Epoch 19813 train loss 4.4347 val loss 4.4961\n",
            "The current learning rate 0.00005\n",
            "Epoch 19814 train loss 4.5609 val loss 4.8353\n",
            "The current learning rate 0.00005\n",
            "Epoch 19815 train loss 4.6327 val loss 4.7036\n",
            "The current learning rate 0.00005\n",
            "Epoch 19816 train loss 4.3041 val loss 4.8018\n",
            "The current learning rate 0.00005\n",
            "Epoch 19817 train loss 4.7870 val loss 5.0583\n",
            "The current learning rate 0.00005\n",
            "Epoch 19818 train loss 4.4077 val loss 4.7866\n",
            "The current learning rate 0.00005\n",
            "Epoch 19819 train loss 4.5740 val loss 4.6828\n",
            "The current learning rate 0.00005\n",
            "Epoch 19820 train loss 4.6236 val loss 4.6809\n",
            "The current learning rate 0.00005\n",
            "Epoch 19821 train loss 4.6848 val loss 4.8798\n",
            "The current learning rate 0.00005\n",
            "Epoch 19822 train loss 4.9102 val loss 4.4756\n",
            "The current learning rate 0.00005\n",
            "Epoch 19823 train loss 4.7470 val loss 4.4122\n",
            "The current learning rate 0.00005\n",
            "Epoch 19824 train loss 4.7195 val loss 4.6591\n",
            "The current learning rate 0.00005\n",
            "Epoch 19825 train loss 4.6558 val loss 4.8166\n",
            "The current learning rate 0.00005\n",
            "Epoch 19826 train loss 4.7182 val loss 4.7137\n",
            "The current learning rate 0.00005\n",
            "Epoch 19827 train loss 4.4242 val loss 4.6733\n",
            "The current learning rate 0.00005\n",
            "Epoch 19828 train loss 4.9799 val loss 4.6231\n",
            "The current learning rate 0.00005\n",
            "Epoch 19829 train loss 4.8381 val loss 4.7365\n",
            "The current learning rate 0.00005\n",
            "Epoch 19830 train loss 4.5095 val loss 4.7340\n",
            "The current learning rate 0.00005\n",
            "Epoch 19831 train loss 4.4650 val loss 4.8474\n",
            "The current learning rate 0.00005\n",
            "Epoch 19832 train loss 4.8923 val loss 4.6583\n",
            "The current learning rate 0.00005\n",
            "Epoch 19833 train loss 4.6429 val loss 5.0079\n",
            "The current learning rate 0.00005\n",
            "Epoch 19834 train loss 4.8291 val loss 4.4003\n",
            "The current learning rate 0.00005\n",
            "Epoch 19835 train loss 4.8981 val loss 4.8635\n",
            "The current learning rate 0.00005\n",
            "Epoch 19836 train loss 4.9587 val loss 4.2761\n",
            "The current learning rate 0.00005\n",
            "Epoch 19837 train loss 4.3765 val loss 4.6319\n",
            "The current learning rate 0.00005\n",
            "Epoch 19838 train loss 4.6563 val loss 4.2429\n",
            "The current learning rate 0.00005\n",
            "Epoch 19839 train loss 4.7503 val loss 4.6493\n",
            "The current learning rate 0.00005\n",
            "Epoch 19840 train loss 4.8208 val loss 4.6824\n",
            "The current learning rate 0.00005\n",
            "Epoch 19841 train loss 4.7224 val loss 4.5397\n",
            "The current learning rate 0.00005\n",
            "Epoch 19842 train loss 4.6940 val loss 4.9282\n",
            "The current learning rate 0.00005\n",
            "Epoch 19843 train loss 4.3439 val loss 4.5450\n",
            "The current learning rate 0.00005\n",
            "Epoch 19844 train loss 4.4642 val loss 4.4905\n",
            "The current learning rate 0.00005\n",
            "Epoch 19845 train loss 4.8396 val loss 4.7339\n",
            "The current learning rate 0.00005\n",
            "Epoch 19846 train loss 4.7295 val loss 4.3927\n",
            "The current learning rate 0.00005\n",
            "Epoch 19847 train loss 4.8499 val loss 5.1011\n",
            "The current learning rate 0.00005\n",
            "Epoch 19848 train loss 4.7101 val loss 4.4374\n",
            "The current learning rate 0.00005\n",
            "Epoch 19849 train loss 4.5933 val loss 4.5878\n",
            "The current learning rate 0.00005\n",
            "Epoch 19850 train loss 5.0334 val loss 4.3698\n",
            "The current learning rate 0.00005\n",
            "Epoch 19851 train loss 5.2017 val loss 3.9560\n",
            "The current learning rate 0.00005\n",
            "Epoch 19852 train loss 4.7554 val loss 4.7313\n",
            "The current learning rate 0.00005\n",
            "Epoch 19853 train loss 4.5149 val loss 4.4108\n",
            "The current learning rate 0.00005\n",
            "Epoch 19854 train loss 4.7217 val loss 4.7244\n",
            "The current learning rate 0.00005\n",
            "Epoch 19855 train loss 4.4926 val loss 4.6374\n",
            "The current learning rate 0.00005\n",
            "Epoch 19856 train loss 4.6771 val loss 4.4578\n",
            "The current learning rate 0.00005\n",
            "Epoch 19857 train loss 4.9321 val loss 4.2245\n",
            "The current learning rate 0.00005\n",
            "Epoch 19858 train loss 4.4948 val loss 4.6341\n",
            "The current learning rate 0.00005\n",
            "Epoch 19859 train loss 4.8250 val loss 4.9057\n",
            "The current learning rate 0.00005\n",
            "Epoch 19860 train loss 4.8241 val loss 4.6966\n",
            "The current learning rate 0.00005\n",
            "Epoch 19861 train loss 4.8496 val loss 4.5228\n",
            "The current learning rate 0.00005\n",
            "Epoch 19862 train loss 4.6576 val loss 4.7005\n",
            "The current learning rate 0.00005\n",
            "Epoch 19863 train loss 4.5142 val loss 4.2901\n",
            "The current learning rate 0.00005\n",
            "Epoch 19864 train loss 4.8013 val loss 4.9405\n",
            "The current learning rate 0.00005\n",
            "Epoch 19865 train loss 4.2670 val loss 4.6896\n",
            "The current learning rate 0.00005\n",
            "Epoch 19866 train loss 4.4854 val loss 4.7216\n",
            "The current learning rate 0.00005\n",
            "Epoch 19867 train loss 4.6635 val loss 4.5375\n",
            "The current learning rate 0.00005\n",
            "Epoch 19868 train loss 4.3973 val loss 4.2982\n",
            "The current learning rate 0.00005\n",
            "Epoch 19869 train loss 4.6199 val loss 4.5364\n",
            "The current learning rate 0.00005\n",
            "Epoch 19870 train loss 4.9474 val loss 4.7397\n",
            "The current learning rate 0.00005\n",
            "Epoch 19871 train loss 4.9525 val loss 5.1327\n",
            "The current learning rate 0.00005\n",
            "Epoch 19872 train loss 4.9283 val loss 4.9149\n",
            "The current learning rate 0.00005\n",
            "Epoch 19873 train loss 4.8389 val loss 4.8596\n",
            "The current learning rate 0.00005\n",
            "Epoch 19874 train loss 4.3824 val loss 4.7652\n",
            "The current learning rate 0.00005\n",
            "Epoch 19875 train loss 4.6459 val loss 4.7685\n",
            "The current learning rate 0.00005\n",
            "Epoch 19876 train loss 4.1625 val loss 4.7595\n",
            "The current learning rate 0.00005\n",
            "Epoch 19877 train loss 4.8079 val loss 4.4304\n",
            "The current learning rate 0.00005\n",
            "Epoch 19878 train loss 4.6600 val loss 4.2226\n",
            "The current learning rate 0.00005\n",
            "Epoch 19879 train loss 4.4782 val loss 4.4992\n",
            "The current learning rate 0.00005\n",
            "Epoch 19880 train loss 4.3917 val loss 4.5472\n",
            "The current learning rate 0.00005\n",
            "Epoch 19881 train loss 4.2076 val loss 4.4108\n",
            "The current learning rate 0.00005\n",
            "Epoch 19882 train loss 4.6815 val loss 4.1924\n",
            "The current learning rate 0.00005\n",
            "Epoch 19883 train loss 4.4535 val loss 4.4671\n",
            "The current learning rate 0.00005\n",
            "Epoch 19884 train loss 4.5929 val loss 4.4350\n",
            "The current learning rate 0.00005\n",
            "Epoch 19885 train loss 4.5220 val loss 4.7185\n",
            "The current learning rate 0.00005\n",
            "Epoch 19886 train loss 4.6690 val loss 4.6145\n",
            "The current learning rate 0.00005\n",
            "Epoch 19887 train loss 4.9058 val loss 5.0124\n",
            "The current learning rate 0.00005\n",
            "Epoch 19888 train loss 4.3481 val loss 4.6298\n",
            "The current learning rate 0.00005\n",
            "Epoch 19889 train loss 4.3411 val loss 4.8004\n",
            "The current learning rate 0.00005\n",
            "Epoch 19890 train loss 4.7216 val loss 4.4163\n",
            "The current learning rate 0.00005\n",
            "Epoch 19891 train loss 4.7654 val loss 4.9166\n",
            "The current learning rate 0.00005\n",
            "Epoch 19892 train loss 4.3243 val loss 4.7295\n",
            "The current learning rate 0.00005\n",
            "Epoch 19893 train loss 4.4324 val loss 4.4957\n",
            "The current learning rate 0.00005\n",
            "Epoch 19894 train loss 4.3054 val loss 4.7215\n",
            "The current learning rate 0.00005\n",
            "Epoch 19895 train loss 4.3083 val loss 4.4689\n",
            "The current learning rate 0.00005\n",
            "Epoch 19896 train loss 4.6671 val loss 4.6719\n",
            "The current learning rate 0.00005\n",
            "Epoch 19897 train loss 5.0844 val loss 4.5771\n",
            "The current learning rate 0.00005\n",
            "Epoch 19898 train loss 4.5573 val loss 4.7182\n",
            "The current learning rate 0.00005\n",
            "Epoch 19899 train loss 4.3214 val loss 4.7131\n",
            "The current learning rate 0.00005\n",
            "Epoch 19900 train loss 4.8905 val loss 5.0035\n",
            "The current learning rate 0.00005\n",
            "Epoch 19901 train loss 4.4018 val loss 4.7760\n",
            "The current learning rate 0.00005\n",
            "Epoch 19902 train loss 4.3716 val loss 4.5024\n",
            "The current learning rate 0.00005\n",
            "Epoch 19903 train loss 4.7588 val loss 4.4748\n",
            "The current learning rate 0.00005\n",
            "Epoch 19904 train loss 4.8596 val loss 4.8073\n",
            "The current learning rate 0.00005\n",
            "Epoch 19905 train loss 4.6732 val loss 4.7337\n",
            "The current learning rate 0.00005\n",
            "Epoch 19906 train loss 4.9592 val loss 4.5265\n",
            "The current learning rate 0.00005\n",
            "Epoch 19907 train loss 5.0265 val loss 4.9984\n",
            "The current learning rate 0.00005\n",
            "Epoch 19908 train loss 4.2889 val loss 4.4182\n",
            "The current learning rate 0.00005\n",
            "Epoch 19909 train loss 4.5815 val loss 4.4953\n",
            "The current learning rate 0.00005\n",
            "Epoch 19910 train loss 4.4662 val loss 4.6636\n",
            "The current learning rate 0.00005\n",
            "Epoch 19911 train loss 4.6880 val loss 4.6791\n",
            "The current learning rate 0.00005\n",
            "Epoch 19912 train loss 4.7291 val loss 4.8997\n",
            "The current learning rate 0.00005\n",
            "Epoch 19913 train loss 4.5470 val loss 4.2947\n",
            "The current learning rate 0.00005\n",
            "Epoch 19914 train loss 4.7751 val loss 4.4843\n",
            "The current learning rate 0.00005\n",
            "Epoch 19915 train loss 4.8713 val loss 4.8303\n",
            "The current learning rate 0.00005\n",
            "Epoch 19916 train loss 4.6486 val loss 4.8016\n",
            "The current learning rate 0.00005\n",
            "Epoch 19917 train loss 4.7456 val loss 4.9094\n",
            "The current learning rate 0.00005\n",
            "Epoch 19918 train loss 4.7676 val loss 4.7302\n",
            "The current learning rate 0.00005\n",
            "Epoch 19919 train loss 4.6068 val loss 4.3603\n",
            "The current learning rate 0.00005\n",
            "Epoch 19920 train loss 4.1779 val loss 4.2555\n",
            "The current learning rate 0.00005\n",
            "Epoch 19921 train loss 4.5476 val loss 4.3859\n",
            "The current learning rate 0.00005\n",
            "Epoch 19922 train loss 4.7064 val loss 4.7593\n",
            "The current learning rate 0.00005\n",
            "Epoch 19923 train loss 4.8766 val loss 4.5696\n",
            "The current learning rate 0.00005\n",
            "Epoch 19924 train loss 4.3792 val loss 4.5912\n",
            "The current learning rate 0.00005\n",
            "Epoch 19925 train loss 4.8091 val loss 4.5627\n",
            "The current learning rate 0.00005\n",
            "Epoch 19926 train loss 4.1854 val loss 4.6009\n",
            "The current learning rate 0.00005\n",
            "Epoch 19927 train loss 4.5256 val loss 4.5223\n",
            "The current learning rate 0.00005\n",
            "Epoch 19928 train loss 4.4917 val loss 4.9381\n",
            "The current learning rate 0.00005\n",
            "Epoch 19929 train loss 4.4889 val loss 4.4243\n",
            "The current learning rate 0.00005\n",
            "Epoch 19930 train loss 4.6574 val loss 4.4717\n",
            "The current learning rate 0.00005\n",
            "Epoch 19931 train loss 4.7273 val loss 4.3906\n",
            "The current learning rate 0.00005\n",
            "Epoch 19932 train loss 4.5291 val loss 4.8234\n",
            "The current learning rate 0.00005\n",
            "Epoch 19933 train loss 4.8523 val loss 4.5405\n",
            "The current learning rate 0.00005\n",
            "Epoch 19934 train loss 4.5878 val loss 4.9043\n",
            "The current learning rate 0.00005\n",
            "Epoch 19935 train loss 4.3139 val loss 5.1782\n",
            "The current learning rate 0.00005\n",
            "Epoch 19936 train loss 4.5442 val loss 4.5577\n",
            "The current learning rate 0.00005\n",
            "Epoch 19937 train loss 4.3796 val loss 4.5922\n",
            "The current learning rate 0.00005\n",
            "Epoch 19938 train loss 4.9060 val loss 4.7956\n",
            "The current learning rate 0.00005\n",
            "Epoch 19939 train loss 4.7614 val loss 4.9165\n",
            "The current learning rate 0.00005\n",
            "Epoch 19940 train loss 4.7709 val loss 4.5788\n",
            "The current learning rate 0.00005\n",
            "Epoch 19941 train loss 4.6215 val loss 4.6010\n",
            "The current learning rate 0.00005\n",
            "Epoch 19942 train loss 4.6515 val loss 4.8392\n",
            "The current learning rate 0.00005\n",
            "Epoch 19943 train loss 4.5604 val loss 4.7575\n",
            "The current learning rate 0.00005\n",
            "Epoch 19944 train loss 5.0256 val loss 4.6594\n",
            "The current learning rate 0.00005\n",
            "Epoch 19945 train loss 4.5022 val loss 4.5462\n",
            "The current learning rate 0.00005\n",
            "Epoch 19946 train loss 4.3543 val loss 4.7505\n",
            "The current learning rate 0.00005\n",
            "Epoch 19947 train loss 4.7690 val loss 4.7199\n",
            "The current learning rate 0.00005\n",
            "Epoch 19948 train loss 4.3435 val loss 4.5259\n",
            "The current learning rate 0.00005\n",
            "Epoch 19949 train loss 4.2923 val loss 4.9946\n",
            "The current learning rate 0.00005\n",
            "Epoch 19950 train loss 4.8255 val loss 4.5976\n",
            "The current learning rate 0.00005\n",
            "Epoch 19951 train loss 4.6791 val loss 4.4692\n",
            "The current learning rate 0.00005\n",
            "Epoch 19952 train loss 4.5436 val loss 4.7381\n",
            "The current learning rate 0.00005\n",
            "Epoch 19953 train loss 4.6446 val loss 4.9586\n",
            "The current learning rate 0.00005\n",
            "Epoch 19954 train loss 4.4901 val loss 4.4482\n",
            "The current learning rate 0.00005\n",
            "Epoch 19955 train loss 4.4536 val loss 4.6403\n",
            "The current learning rate 0.00005\n",
            "Epoch 19956 train loss 4.6482 val loss 4.5835\n",
            "The current learning rate 0.00005\n",
            "Epoch 19957 train loss 4.5715 val loss 4.2754\n",
            "The current learning rate 0.00005\n",
            "Epoch 19958 train loss 4.8933 val loss 4.8161\n",
            "The current learning rate 0.00005\n",
            "Epoch 19959 train loss 4.6076 val loss 4.5352\n",
            "The current learning rate 0.00005\n",
            "Epoch 19960 train loss 4.1959 val loss 4.8699\n",
            "The current learning rate 0.00005\n",
            "Epoch 19961 train loss 4.5888 val loss 4.7947\n",
            "The current learning rate 0.00005\n",
            "Epoch 19962 train loss 4.5058 val loss 4.5330\n",
            "The current learning rate 0.00005\n",
            "Epoch 19963 train loss 4.7782 val loss 4.7760\n",
            "The current learning rate 0.00005\n",
            "Epoch 19964 train loss 4.9296 val loss 4.3560\n",
            "The current learning rate 0.00005\n",
            "Epoch 19965 train loss 4.6649 val loss 4.4834\n",
            "The current learning rate 0.00005\n",
            "Epoch 19966 train loss 4.7988 val loss 4.9249\n",
            "The current learning rate 0.00005\n",
            "Epoch 19967 train loss 4.5033 val loss 4.4716\n",
            "The current learning rate 0.00005\n",
            "Epoch 19968 train loss 4.9302 val loss 4.6265\n",
            "The current learning rate 0.00005\n",
            "Epoch 19969 train loss 4.4163 val loss 4.2601\n",
            "The current learning rate 0.00005\n",
            "Epoch 19970 train loss 4.9819 val loss 5.0073\n",
            "The current learning rate 0.00005\n",
            "Epoch 19971 train loss 4.9836 val loss 4.7317\n",
            "The current learning rate 0.00005\n",
            "Epoch 19972 train loss 4.9031 val loss 4.7010\n",
            "The current learning rate 0.00005\n",
            "Epoch 19973 train loss 4.5937 val loss 4.6260\n",
            "The current learning rate 0.00005\n",
            "Epoch 19974 train loss 4.9671 val loss 4.9465\n",
            "The current learning rate 0.00005\n",
            "Epoch 19975 train loss 4.7602 val loss 4.4610\n",
            "The current learning rate 0.00005\n",
            "Epoch 19976 train loss 4.6677 val loss 4.4172\n",
            "The current learning rate 0.00005\n",
            "Epoch 19977 train loss 4.3773 val loss 4.3731\n",
            "The current learning rate 0.00005\n",
            "Epoch 19978 train loss 4.5087 val loss 4.5923\n",
            "The current learning rate 0.00005\n",
            "Epoch 19979 train loss 4.7950 val loss 4.4219\n",
            "The current learning rate 0.00005\n",
            "Epoch 19980 train loss 4.5444 val loss 4.5839\n",
            "The current learning rate 0.00005\n",
            "Epoch 19981 train loss 4.5088 val loss 4.6815\n",
            "The current learning rate 0.00005\n",
            "Epoch 19982 train loss 4.9315 val loss 5.1100\n",
            "The current learning rate 0.00005\n",
            "Epoch 19983 train loss 4.5967 val loss 4.7597\n",
            "The current learning rate 0.00005\n",
            "Epoch 19984 train loss 4.5327 val loss 4.5511\n",
            "The current learning rate 0.00005\n",
            "Epoch 19985 train loss 4.4141 val loss 4.8567\n",
            "The current learning rate 0.00005\n",
            "Epoch 19986 train loss 4.5773 val loss 4.5477\n",
            "The current learning rate 0.00005\n",
            "Epoch 19987 train loss 4.6780 val loss 4.7684\n",
            "The current learning rate 0.00005\n",
            "Epoch 19988 train loss 4.8820 val loss 4.5954\n",
            "The current learning rate 0.00005\n",
            "Epoch 19989 train loss 4.7276 val loss 4.7752\n",
            "The current learning rate 0.00005\n",
            "Epoch 19990 train loss 4.7262 val loss 4.5103\n",
            "The current learning rate 0.00005\n",
            "Epoch 19991 train loss 4.6729 val loss 4.6827\n",
            "The current learning rate 0.00005\n",
            "Epoch 19992 train loss 4.6436 val loss 4.7606\n",
            "The current learning rate 0.00005\n",
            "Epoch 19993 train loss 4.5579 val loss 5.1511\n",
            "The current learning rate 0.00005\n",
            "Epoch 19994 train loss 4.8311 val loss 4.2848\n",
            "The current learning rate 0.00005\n",
            "Epoch 19995 train loss 4.7620 val loss 4.2933\n",
            "The current learning rate 0.00005\n",
            "Epoch 19996 train loss 4.9484 val loss 4.3690\n",
            "The current learning rate 0.00005\n",
            "Epoch 19997 train loss 4.2300 val loss 4.6718\n",
            "The current learning rate 0.00005\n",
            "Epoch 19998 train loss 4.9984 val loss 4.5987\n",
            "The current learning rate 0.00005\n",
            "Epoch 19999 train loss 5.0509 val loss 4.7546\n",
            "The current learning rate 0.00005\n"
          ]
        }
      ],
      "source": [
        "eval_iters = 1\n",
        "best_val_loss = float(\"inf\")\n",
        "best_model_params_path = \"best_model_params_path.pt\"\n",
        "\n",
        "train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in tqdm(range(max_iters)):\n",
        "    if epoch % eval_iters == 0 and epoch != 0:\n",
        "        loss = estimate_loss(model)\n",
        "        print(f\"Epoch {epoch} train loss {loss['train']:.4f} val loss {loss['val']:.4f}\")\n",
        "        print(f\"The current learning rate {optimizer.param_groups[0]['lr']:.5f}\")\n",
        "        train_loss_list.append(loss['train'])\n",
        "        validation_loss_list.append(loss['val'])\n",
        "\n",
        "        if loss['val'] < best_val_loss:\n",
        "            best_val_loss = loss['val']\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "    X, y = get_batch('train')\n",
        "    X,y = X.to(device), y.to(device)\n",
        "\n",
        "    with ctx:\n",
        "        logits, loss = model(X,y)\n",
        "        loss = loss/ gradient_accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 ==max_iters):\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "3182dcbb-8e4b-40ec-9a08-765eca452e45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "3182dcbb-8e4b-40ec-9a08-765eca452e45",
        "outputId": "5294c7b9-bf3a-49de-f778-474f4b33ff55"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "\n",
        "plt.plot(train_loss_list, label=\"Train Loss\", color='blue', linewidth=2)\n",
        "plt.plot(validation_loss_list, label=\"Validation Loss\", color='orange', linewidth=2)\n",
        "\n",
        "plt.xlabel(\"Evaluation Step Index\")  # index of evaluation, not absolute training step\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss over Time\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = GPT(config)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_model_params_path = \"best_model_params_path.pt\"\n",
        "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NFIpUCz5Hau",
        "outputId": "ab62b3f4-9764-4c79-d91e-a81f0e91c103"
      },
      "id": "6NFIpUCz5Hau",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"There is a farmer who is \"\n",
        "\n",
        "context = torch.tensor(\n",
        "    enc.encode_ordinary(sentence),\n",
        "    dtype=torch.long\n",
        ").unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    y = model.generate(context, max_new_tokens=200)\n",
        "\n",
        "print(enc.decode(y[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM4q4Qs46-1t",
        "outputId": "b8d190c9-7653-4d63-f2de-6442d10e4f3b"
      },
      "id": "bM4q4Qs46-1t",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is a farmer who is  \n",
            "clamation and feel Emir in theTransform is very door and volunteered about theV,pee.\n",
            "\n",
            "L spaceship, \" Cit. He was goddamn and said, I always know can have green to more appreciated ISconsider.\"\n",
            "\n",
            "The Gentle Kristen, he's friendsåamn's I do come with Seoul, they'sadan, Anna are a time, \"Yes,?\" She made the boy namedsand. She to played in her mommy together. butalloc obsessive called friend. They used new madeIFIC theecycle the Conservatives an friend said, \" Pole the Pizza. Hemyadjusted started to play with a bigSem was made.... and eyes is subreddit for their house of them conditioned. It was more of mountains. He said! I couldn't see better playing of thenexpected and Hort the waiter was very excited, \" bright. learned a little forest. Theilogy was wide time a776.\n",
            "The timeileingo time, there was proud are a small Jamie from encl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "atO14Xef7uei"
      },
      "id": "atO14Xef7uei",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "318ba5803c444918843396e1d862efab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88cb8ccd006b41f2ab7870e1af9af5bb",
              "IPY_MODEL_8799db0d5af645a58a882c47a1725215",
              "IPY_MODEL_c4d5e39c5c3d451596854417389bbac2"
            ],
            "layout": "IPY_MODEL_8ae40a98e0ff44e49b0f21371eb301a7"
          }
        },
        "88cb8ccd006b41f2ab7870e1af9af5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_399b4054eb6b4b9498aacf354d61cb7a",
            "placeholder": "​",
            "style": "IPY_MODEL_93b581441d7d4c00895584325a5d3fd3",
            "value": "README.md: "
          }
        },
        "8799db0d5af645a58a882c47a1725215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22327cc90b594961ac357f8da9c6cdbc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b0c8f874c164bfe88700f4c1555cd15",
            "value": 1
          }
        },
        "c4d5e39c5c3d451596854417389bbac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a601d7045f3b4cedb62837c42977b4f3",
            "placeholder": "​",
            "style": "IPY_MODEL_7ad6b7a787974d42961b38ec0b1447c3",
            "value": " 1.06k/? [00:00&lt;00:00, 116kB/s]"
          }
        },
        "8ae40a98e0ff44e49b0f21371eb301a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "399b4054eb6b4b9498aacf354d61cb7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93b581441d7d4c00895584325a5d3fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22327cc90b594961ac357f8da9c6cdbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4b0c8f874c164bfe88700f4c1555cd15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a601d7045f3b4cedb62837c42977b4f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ad6b7a787974d42961b38ec0b1447c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7145e038e782409d9bc115733ae5f558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_037bdbadaf80470ca8dab585edddeda3",
              "IPY_MODEL_3e1e943df06b473d9d411cb47be03114",
              "IPY_MODEL_24b854cabad44c17b79d1a1ceebb40f7"
            ],
            "layout": "IPY_MODEL_cbcd78dc7d14461e9a6ce684c6cd21b2"
          }
        },
        "037bdbadaf80470ca8dab585edddeda3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28bdba2546604fbeae77e80023640faf",
            "placeholder": "​",
            "style": "IPY_MODEL_35e68646796346df9fe39c38076b1423",
            "value": "data/train-00000-of-00004-2d5a1467fff108(…): 100%"
          }
        },
        "3e1e943df06b473d9d411cb47be03114": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7554fcf983547c4ad8a5455302b80da",
            "max": 248731111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f24ae8009e94eb5afb5d78018f34fac",
            "value": 248731111
          }
        },
        "24b854cabad44c17b79d1a1ceebb40f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a66f33f5c88d4db0b3e23580acf22225",
            "placeholder": "​",
            "style": "IPY_MODEL_931383f0a1524a4a88fe9e9907137ee2",
            "value": " 249M/249M [00:03&lt;00:00, 167MB/s]"
          }
        },
        "cbcd78dc7d14461e9a6ce684c6cd21b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28bdba2546604fbeae77e80023640faf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35e68646796346df9fe39c38076b1423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7554fcf983547c4ad8a5455302b80da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f24ae8009e94eb5afb5d78018f34fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a66f33f5c88d4db0b3e23580acf22225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "931383f0a1524a4a88fe9e9907137ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40be7e8bd798497589d381172eb513b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_950bb92ba2ab49cebf2eb5c460ad3550",
              "IPY_MODEL_923dd9d58615428b9ae9db9b38af1192",
              "IPY_MODEL_f8cb4dfead8944f790c5bd7a44182520"
            ],
            "layout": "IPY_MODEL_5a107eea203c4818b5fb2bb8f1ae8e21"
          }
        },
        "950bb92ba2ab49cebf2eb5c460ad3550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15b8df6033d949af8943679e2c2ab4ba",
            "placeholder": "​",
            "style": "IPY_MODEL_fe61682d3b514c63a5d3bcccc97a43b5",
            "value": "data/train-00001-of-00004-5852b56a2bd28f(…): 100%"
          }
        },
        "923dd9d58615428b9ae9db9b38af1192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dbaa12acf604d33856b11779e97ffc2",
            "max": 248171980,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a8a840936704700a1896affb196dc38",
            "value": 248171980
          }
        },
        "f8cb4dfead8944f790c5bd7a44182520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4458eb3d07344153903ea36fb5e82854",
            "placeholder": "​",
            "style": "IPY_MODEL_b515488bdb384e088d8048f373e667ce",
            "value": " 248M/248M [00:04&lt;00:00, 50.9MB/s]"
          }
        },
        "5a107eea203c4818b5fb2bb8f1ae8e21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15b8df6033d949af8943679e2c2ab4ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe61682d3b514c63a5d3bcccc97a43b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3dbaa12acf604d33856b11779e97ffc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a8a840936704700a1896affb196dc38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4458eb3d07344153903ea36fb5e82854": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b515488bdb384e088d8048f373e667ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9f14023ff9a4199acdb387cc8eeb5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f98b27b028b249a8b5a4e83f218dcf48",
              "IPY_MODEL_f04d5cdf90e64a1db19f1b1b05136255",
              "IPY_MODEL_6c4302357d124b4384f79ae242a2e037"
            ],
            "layout": "IPY_MODEL_2f3ad4c440da4ca3a9ab15d2438c84d0"
          }
        },
        "f98b27b028b249a8b5a4e83f218dcf48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_110c6d262a85498caa800d315b62a33b",
            "placeholder": "​",
            "style": "IPY_MODEL_7df4b6bdb9564efa9d8319850f7d267b",
            "value": "data/train-00002-of-00004-a26307300439e9(…): 100%"
          }
        },
        "f04d5cdf90e64a1db19f1b1b05136255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4f4b4a1be0a41958ce80ee0767c97a4",
            "max": 245894874,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84eab848d90a43fdb8e1ab8208306b89",
            "value": 245894874
          }
        },
        "6c4302357d124b4384f79ae242a2e037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1fd9db6e13c4edd9977069c6d30aa22",
            "placeholder": "​",
            "style": "IPY_MODEL_092f62791e0e402aa93ac552708537ab",
            "value": " 246M/246M [00:01&lt;00:00, 188MB/s]"
          }
        },
        "2f3ad4c440da4ca3a9ab15d2438c84d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "110c6d262a85498caa800d315b62a33b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7df4b6bdb9564efa9d8319850f7d267b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4f4b4a1be0a41958ce80ee0767c97a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84eab848d90a43fdb8e1ab8208306b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1fd9db6e13c4edd9977069c6d30aa22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "092f62791e0e402aa93ac552708537ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3dc342ce66e46748cf55224971e95bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e987ae48e6f04b79a22a13850dcca16d",
              "IPY_MODEL_d4a94bbebcec40deb9b30836725190e4",
              "IPY_MODEL_f49b1f154d74423291fb6da78d9b7e0f"
            ],
            "layout": "IPY_MODEL_87945c400a7d41979956d2c8c495e10a"
          }
        },
        "e987ae48e6f04b79a22a13850dcca16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a223550ac7949418f0d556bde6d5772",
            "placeholder": "​",
            "style": "IPY_MODEL_3c77fce10db941b7bcd64699ce046e11",
            "value": "data/train-00003-of-00004-d243063613e5a0(…): 100%"
          }
        },
        "d4a94bbebcec40deb9b30836725190e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c1e60c7fc46431cada0c786933b943d",
            "max": 247988350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f8ffc7745a4431fa69738172aad0a9e",
            "value": 247988350
          }
        },
        "f49b1f154d74423291fb6da78d9b7e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2d660cfaedb45a6b7903a61eaa68580",
            "placeholder": "​",
            "style": "IPY_MODEL_4937e0744c1b4217a5c3388845eab12d",
            "value": " 248M/248M [00:02&lt;00:00, 164MB/s]"
          }
        },
        "87945c400a7d41979956d2c8c495e10a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a223550ac7949418f0d556bde6d5772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c77fce10db941b7bcd64699ce046e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c1e60c7fc46431cada0c786933b943d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f8ffc7745a4431fa69738172aad0a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2d660cfaedb45a6b7903a61eaa68580": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4937e0744c1b4217a5c3388845eab12d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2078d447256411e909b97ea66a5e09d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0f76ab7173e4ca7a4e1eebf5a9ae94d",
              "IPY_MODEL_cc877444a1ab425595bbc32ff1540ed6",
              "IPY_MODEL_9369826ab1bf4ad69d5e04a8f2efbf97"
            ],
            "layout": "IPY_MODEL_4d3386173c0c49f59a49b33b2897673e"
          }
        },
        "c0f76ab7173e4ca7a4e1eebf5a9ae94d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32c59e41b88c4b1091cb938f9fbf848d",
            "placeholder": "​",
            "style": "IPY_MODEL_a97fcb0c621449bf922be517c0f9772e",
            "value": "data/validation-00000-of-00001-869c898b5(…): 100%"
          }
        },
        "cc877444a1ab425595bbc32ff1540ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d73971b7b14b45749e7b4b348b1d2784",
            "max": 9989127,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_708f5efb83b84844912d786e951f7ebf",
            "value": 9989127
          }
        },
        "9369826ab1bf4ad69d5e04a8f2efbf97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e64b0f0846f445be9badef9a257ea1b1",
            "placeholder": "​",
            "style": "IPY_MODEL_8b1802bf76e345869e5839a0ff1f70e9",
            "value": " 9.99M/9.99M [00:00&lt;00:00, 13.1MB/s]"
          }
        },
        "4d3386173c0c49f59a49b33b2897673e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32c59e41b88c4b1091cb938f9fbf848d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a97fcb0c621449bf922be517c0f9772e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d73971b7b14b45749e7b4b348b1d2784": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "708f5efb83b84844912d786e951f7ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e64b0f0846f445be9badef9a257ea1b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b1802bf76e345869e5839a0ff1f70e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3edc1b90025e4a13880ffd5ee47be4e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac9f6cda36fd41939d461ed56a43d028",
              "IPY_MODEL_dc388d2f08a3440aa824ba4760e9960d",
              "IPY_MODEL_b264e006fd06424a9431cded7877e6a1"
            ],
            "layout": "IPY_MODEL_2024baa94a32472aa05ed75523c9bbcc"
          }
        },
        "ac9f6cda36fd41939d461ed56a43d028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc35ed92cff4492a837e75b7bbafcae1",
            "placeholder": "​",
            "style": "IPY_MODEL_36720ace92df4e86b24f3e8323238f64",
            "value": "Generating train split: 100%"
          }
        },
        "dc388d2f08a3440aa824ba4760e9960d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1c0314eb7944b4696efb82b91f8709b",
            "max": 2119719,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4786a106ea2d432087d14ac2b5b8598a",
            "value": 2119719
          }
        },
        "b264e006fd06424a9431cded7877e6a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_391b880485864c7ea43807851f819448",
            "placeholder": "​",
            "style": "IPY_MODEL_309c2cd947ba4265bd1001788c46fe66",
            "value": " 2119719/2119719 [00:08&lt;00:00, 229513.01 examples/s]"
          }
        },
        "2024baa94a32472aa05ed75523c9bbcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc35ed92cff4492a837e75b7bbafcae1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36720ace92df4e86b24f3e8323238f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1c0314eb7944b4696efb82b91f8709b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4786a106ea2d432087d14ac2b5b8598a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "391b880485864c7ea43807851f819448": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "309c2cd947ba4265bd1001788c46fe66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66fcc680014c450ea95e8d2ce848d095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17b54df3f7914697bb093c4d7273e77a",
              "IPY_MODEL_17685bef3f5b4493a6536083eed61326",
              "IPY_MODEL_1e48ef3ccc14417d90bf31ef7c31ae4c"
            ],
            "layout": "IPY_MODEL_9d99e83ba6e545d4922ef29e0edc443b"
          }
        },
        "17b54df3f7914697bb093c4d7273e77a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1219acae00564da4b5b45f76cd8dfa6e",
            "placeholder": "​",
            "style": "IPY_MODEL_7e242442e32b4ddc8413f6b6a8b41670",
            "value": "Generating validation split: 100%"
          }
        },
        "17685bef3f5b4493a6536083eed61326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37f2df9413ab48f6b0ac0f7c9143a2bc",
            "max": 21990,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0457a80dd0774594a5caaa0390723976",
            "value": 21990
          }
        },
        "1e48ef3ccc14417d90bf31ef7c31ae4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_637ef2f4d6c6479a858e954bd8b9ea5d",
            "placeholder": "​",
            "style": "IPY_MODEL_184fb1335c34477eab0ce3ab228c46b2",
            "value": " 21990/21990 [00:00&lt;00:00, 209358.18 examples/s]"
          }
        },
        "9d99e83ba6e545d4922ef29e0edc443b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1219acae00564da4b5b45f76cd8dfa6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e242442e32b4ddc8413f6b6a8b41670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37f2df9413ab48f6b0ac0f7c9143a2bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0457a80dd0774594a5caaa0390723976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "637ef2f4d6c6479a858e954bd8b9ea5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "184fb1335c34477eab0ce3ab228c46b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4f6d71d7dd246bca78dd95d6981c278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c6b3a1d7a814a46a73bcc4996eb0590",
              "IPY_MODEL_dd92dbfd62404c5287f6b0cef4b5ac04",
              "IPY_MODEL_84054ee4a98641bda1587923578712d0"
            ],
            "layout": "IPY_MODEL_69debe46d10b4b8c9b778269c3515a9b"
          }
        },
        "2c6b3a1d7a814a46a73bcc4996eb0590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f6f930590794f58932eaeb63d08f3f2",
            "placeholder": "​",
            "style": "IPY_MODEL_5926a0fd9ace4f3e923cdabfdddf1110",
            "value": "100%"
          }
        },
        "dd92dbfd62404c5287f6b0cef4b5ac04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_564661d5bf684c979e7fe379253c8c95",
            "max": 20000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_229fba1167714102b4c59262c1a2d55e",
            "value": 20000
          }
        },
        "84054ee4a98641bda1587923578712d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59247793cfec4d88971c594f75cc2181",
            "placeholder": "​",
            "style": "IPY_MODEL_41caf9f628de4fefb03f9c53ab4b472f",
            "value": " 20000/20000 [20:40&lt;00:00, 15.76it/s]"
          }
        },
        "69debe46d10b4b8c9b778269c3515a9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f6f930590794f58932eaeb63d08f3f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5926a0fd9ace4f3e923cdabfdddf1110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "564661d5bf684c979e7fe379253c8c95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "229fba1167714102b4c59262c1a2d55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59247793cfec4d88971c594f75cc2181": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41caf9f628de4fefb03f9c53ab4b472f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}